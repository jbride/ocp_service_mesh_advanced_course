:noaudio:
:scrollbar:
:toc2:
:linkattrs:

== Istio Security: mTLS and authorization policies

.Prerequisites

* Access to OpenShift 4.x cluster
* Istio Controlplane installed and ready.
* Emergency Response Demo installed.

.Goals

* Enable transport security (mTLS) for inter-service communication in Emergency Response Demo app
* Set authorization policies for inter-service communication in Emergency Response Demo

[NOTE]
.Revisited lab flow
====
Lab flow:

. global mTLS settings: verify/adjust
. ingress certificate and ingress wildcard gateway
* add procedure for creating self-signed certificate
. enable mTLS on Emergency response services
. enable mTLS on other services - considerations
. strict mTLS - considerations
. authorization policies

====

:numbered:

== Red Hat Service Mesh Control Plane Installation Review

NOTE: Review names of namespaces, crd's etc....

Before starting the lab, let's review the Red Hat Service Mesh installation

* The Service Mesh Controlplane is installed in the namespace `admin<x>-istio-system`.
* The Controlplane is defined by the `ServiceshMeshControlPlane` custom resource.
+
----
$ oc get servicemeshcontrolplane -n $RHSM_CONTROL_PLANE_NS
----
+
----
NAME           READY
full-install   True
----
* Review the `ServicMeshControlPlane` CR:
+
----
$ oc get servicemeshcontrolplane full-install -o yaml -n $RHSM_CONTROL_PLANE_NS
----
+
----
apiVersion: maistra.io/v1
kind: ServiceMeshControlPlane
metadata:
  creationTimestamp: '2019-10-23T21:01:01Z'
  finalizers:
    - maistra.io/istio-operator
  generation: 1
  name: full-install
  namespace: istio-system
  resourceVersion: '2394271'
  selfLink: >-
    /apis/maistra.io/v1/namespaces/istio-system/servicemeshcontrolplanes/full-install
  uid: 37f6be7f-f5d8-11e9-a89b-06c64aecfabc
spec:
  istio:
    gateways:
      istio-egressgateway:
        autoscaleEnabled: false
      istio-ingressgateway:
        autoscaleEnabled: false
    global:
      controlPlaneSecurityEnabled: false
      disablePolicyChecks: false
      mtls:
        enabled: false
      proxy:
        resources:
          limits:
            cpu: 500m
            memory: 128Mi
          requests:
            cpu: 100m
            memory: 128Mi
    grafana:
      enabled: true
    kiali:
      enabled: true
    mixer:
      policy:
        autoscaleEnabled: false
      telemetry:
        autoscaleEnabled: false
        resources:
          limits:
            cpu: 500m
            memory: 4G
          requests:
            cpu: 100m
            memory: 1G
    pilot:
      autoscaleEnabled: false
      resources:
        limits:
          cpu: 500m
          memory: 3G
      traceSampling: 100
    threescale:
      enabled: false
    tracing:
      enabled: true
      jaeger:
        template: all-in-one
status:
[...]
----
+
* Note that `mTLS` is set to `disabled`. This means that mTLs is not enforced, and pods will be able to communicate over plain HTTP.
* Setting the mTLS as globally disabled is the recommended starting point when migrating an existing set of applications to use Istio and mTLS.
. As part of the Service Mesh installation, a global _ServiceMeshPolicy_ is created in the Service Mesh ControlPlane namespace. +
The scope of the ServiceMeshPolicy is the Service Mesh. It can be overridden by namespace-scoped and service-scoped policies.
+
----
$ oc get servicemeshpolicy default -o yaml -n $RHSM_CONTROL_PLANE_NS
----
+
----
apiVersion: authentication.maistra.io/v1
kind: ServiceMeshPolicy
metadata:
  name: default
  namespace: istio-system
  ownerReferences:
    - apiVersion: maistra.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: ServiceMeshControlPlane
      name: full-install
      uid: 37f6be7f-f5d8-11e9-a89b-06c64aecfabc
  labels:
    app: security
    app.kubernetes.io/part-of: istio
    app.kubernetes.io/instance: istio-system
    release: istio
    app.kubernetes.io/version: 1.0.1-8.el8-1
    app.kubernetes.io/component: security
    app.kubernetes.io/managed-by: maistra-istio-operator
    maistra.io/owner: istio-system
    app.kubernetes.io/name: security
    chart: security
    heritage: Tiller
spec:
  peers:
    - mtls:
        mode: PERMISSIVE
----
+
* The global policy for mTLS is set to `PERMISSIVE`, which means that the proxy sidecar will accept and pass on non mTLS encrypted traffic

. _Citadel_ is the Service Mesh component responsible for the creation and rotation of the certificates that will be used for mTLS communication between services in the cluster. +
For every service account in the namespaces which are managed by the Service Mesh, Citadel creates a secret with a certificate and key pair. +
These certificates have a lifespan of 3 months and will be rotated automatically by Citadel. +
The _Subject Alternative Name_ field on the certificate corresponds to the _SPIFFE_ identity name of the service account. +
As part of client-side verification when using mTLS, this identity is verified against the _secure naming information_ maintained by the service mesh. The secure naming information contains N-to-N mappings from the server identities, which are encoded in certificates, to the service names.
. In the Emergency Response Demo project, verify that the following secrets have been created: `istio-disaster-simulator-service`, `istio.emergency-console`, `istio.incident-piority-service`, `istio.incident-service`, `istio.mission-service`, `istio.process-service`, `istio.process-viewer`, `istio.responder-service`, `istio.responder-simulator-service`
. Using `oc`, visualize the contents of the `istio.incident-service` secret.
+
----
$ oc get secret istio.incident-service -o jsonpath={.data.cert-chain\\.pem} | base64 --decode -n $ER_DEMO_NS
----
+
.Sample Output
----
-----BEGIN CERTIFICATE-----
MIIDNjCCAh6gAwIBAgIRAKjJU0K3IyoNGdLGrknYQX8wDQYJKoZIhvcNAQELBQAw
GDEWMBQGA1UEChMNY2x1c3Rlci5sb2NhbDAeFw0xOTA4MTgxNjE0MjZaFw0xOTEx
MTYxNjE0MjZaMAAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCvZvwn
0vdDFzfEXnJk4fW9/J2mZNWCyLxltDoUrJnWNI8AZZaIzzkAoj29rDbvIG3ZKPKo
UXSMq5eVv4uavWh8AYOmFeJAUab5I//XdSxCwqonWcjocoiZ4AUjCiyZQ+CwZToV
BR7lysMnbuU+Nk+eC1l92bANYEpAv8cQQ2neHpl8qLhja8w6hrUcGzYKu+brxyhB
qib9r3cueGhmRBN3gnq2XDoQfiQqFBoy3wiptaOxBOHzCyyOroXiV2lOrgdkTiqC
VzAqY52jIQMgP2v/HY30N7ot/q7F4jEWx4n9dALRIdT3z8KZOhmccyQsMWePA5Ci
Z3RydxgNwVYcONTnAgMBAAGjgZIwgY8wDgYDVR0PAQH/BAQDAgWgMB0GA1UdJQQW
MBQGCCsGAQUFBwMBBggrBgEFBQcDAjAMBgNVHRMBAf8EAjAAMFAGA1UdEQRJMEeG
RXNwaWZmZTovL2NsdXN0ZXIubG9jYWwvbnMvZW1lcmdlbmN5LXJlc3BvbnNlLWRl
bW8vc2EvaW5jaWRlbnQtc2VydmljZTANBgkqhkiG9w0BAQsFAAOCAQEAXCbWBLgW
xRcdj3oU9E7eFO+ugHhhbx7HYsj3gUrwqaZjXJxzlzSsaUmig14jIFUuYwqIr7WJ
chM/3nJUQGc3smQjtY8fMpztpMLANr4grYlB28upQ3l4rIkBigWwULeQ9qA+g6+x
Wjy17mecP6J7drgZQY2Xz7PC8S/NgDOJFueAior6QlkOp0GWOB1I8S+FvzyYXv91
wGShmD8opSwEWnmZgWx5CnTSyzUwJqp8GHbLUHTHY7OmeofGcpu8GZ/DiUSh5dEf
LhbRXBhhB2B2oMJ/4GEU15wH1gQ252c2u1l8jFU/dNvhJ5fufYTtzrvoybYmrifB
gl+CKnYHFEIp/w==
-----END CERTIFICATE-----
----
. Copy the text output of the certificate. In a browser window, navigate to https://www.sslshopper.com/certificate-decoder.html, and paste the certificate text into the text box:
+
image::images/istio-citadel-certificate.png[]
+
* The certificate subject name is set to `URI:spiffe://cluster.local/ns/<emergency-response-demo namespace>/sa/incident-service`. This is corresponds to the identity of the service as managed by Istio. 
* The certificate validity is 3 months.

NOTE: Check if Citadel certificate lifepsan is configurable.


=== Istio Ingress Gateway and Certificates

In an OpenShift environment, a Route is used to expose services outside the cluster. When using Service Mesh, this has the disadvantage that traffic is routed directly to the pods of the service, bypssing the sidecar proxy on the ingress.

The Service Mesh installs a _Ingress Gateway service_ (which is a Envoy proxy container running on its own). The idea is to route all incoming traffic into the mesh through the ingress gateway, to ensure mesh policies and routing rules are applied to incoming traffic.

_Gateway_ and _VirtualService_ objects are defined to route the traffic from the ingress gateway to the target service pods.

One way to do so is to create a _Gateway_ and a _VirtualService_ for each service exposed outside of the cluster. An alternative is to use a single wildcard Gateway and a VirtualService per service. This is the approach taken in this lab.

The Ingress Gateway also ensures end-to-end encryption using HTTPS for incoming traffic: TLS termination happens at the Istio Ingress Gateway, and is reencrypted by the Gateway using Service Mesh mTLS functionality. To achieve this, the external TLS key and certificate is mounted into the Ingress Gateway pods with a secret.

NOTE: There is a limitation when combining certificates, Gateway and VirtualService objects to route traffic into the mesh with end-to-end transport security. +
Only one certificate/key pair can be mounted into the Ingress gateway. This (wildcard) certificate will be used by the different Gateway objects. However, configuring more than one gateway using the same TLS certificate will cause browsers that leverage HTTP/2 connection reuse (i.e., most browsers) to produce 404 errors when accessing a second host after a connection to another host has already been established. +
For a description of the problem, see https://istio.io/docs/ops/traffic-management/deploy-guidelines/#browser-problem-when-multiple-gateways-configured-with-same-tls-certificate +
The workaround for this problem is to configure a single wildcard Gateway and bind the different VirtualServices to this single gateway.
An another alternative consists of having several Ingress Gateway services, each handling their own internal service and managing their own (non-wildcard) certificate. This means however that a certificate has to be provided for every service exposed outside of the mesh.

. Obtain a wildcard certificate and key for the gateway domain. All services exposed through the Istio Ingress will be part of this domain. The domain should be a subdomain of the global OpenShift application cluster domain, for example `<user>.apps.<openshift domain>`. +
In this lab we will use a self-signed certificate.
* Create a configuration file for openSSL (replace <user> with your username initials or another unique identifier, and <openshift domain> with the OpenShift cluster domain):
+
----
$ cat <<EOF | sudo tee ./cert.cfg
[ req ]
req_extensions     = req_ext
distinguished_name = req_distinguished_name
prompt             = no

[req_distinguished_name]
commonName=<user>.apps.<openshift domain>

[req_ext]
subjectAltName   = @alt_names

[alt_names]
DNS.1  = <user>.apps.<openshift domain>
DNS.2  = *.<user>.apps.<openshift domain>
EOF
----
* Create a self-signed certificate and private key:
+
----
$ openssl req -x509 -config cert.cfg -extensions req_ext -nodes -days 730 -newkey rsa:2048 -sha256 -keyout tls.key -out tls.crt
----

. Create a secret in the Service Mesh control plane namespace with the certificates. The secret must be named `istio-ingressgateway-certs`:
+
----
$ oc create secret tls istio-ingressgateway-certs --cert tls.crt --key tls.key -n $RHSM_CONTROL_PLANE_NS
----
. Restart the Istio ingress gateway pod:
+
----
$ oc rollout latest istio-ingressgateway -n $RHSM_CONTROL_PLANE_NS
----


=== Istio Ingress Wildcard Gateway

Now we can define a wildcard gateway for our services:

. Create a file called `wildcard-gateway.yml` with the following contents (replace `<user>` and `<openshift domain>` with the appropriate values for your cluster):
+
----
---
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: erd-wildcard-gateway
spec:
  selector:
    istio: ingressgateway # use istio default controller
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      privateKey: /etc/istio/ingressgateway-certs/tls.key
      serverCertificate: /etc/istio/ingressgateway-certs/tls.crt
    hosts:
    - "*.<user>.apps.<openshift domain>"
----
. Create the wildcard Gateway:
+
----
$ oc create -f wildcard-gateway.yml -n $RHSM_CONTROL_PLANE_NS
----

== Configure the Emergency Response Demo services for mTLS

In this part of the lab you will configure the different services of the Emergency Response Demo for mTLS, as well as use the Ingress gateway as external entrypoint for the services.

=== Incident Service

Enabling mTLS for the incident service involves the following tasks:

* Inject the Envoy proxy sidecar container into the Incident Service pod  - only needed if this was not yet done in a previous lab.
* Create a DestinationRule and Policy to enforce mTLS when calling the Incident Service.
* Create a VirtualService and a Route for external access to the Incident Service through the Service Mesh Ingress Gateway.

{empty} +

. Replace the health checks in the Incident Service DeploymentConfig to use a command based health check with curl rather than a HTTP based health check.
+
----
$ oc edit dc incident-service -o yaml -n $ER_DEMO_NS
----
+
Replace the existing liveness and readiness probes with command based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080/actuator/health'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080/actuator/health'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
* When enforcing strict mTLS when calling the incident service, the HTTP based healthcheck will fail, as it is executed from the kubelet, and is not able to present a suitable certificate. The command based health checks are executed in the container itself, so they are not impacted.
* The Service Mesh sidecar injector service can be configured to rewrite HTTP probes at sidecar injection time, so that the requests will be sent to Pilot, which will then redirect to the application. This global configuration is set in the `istio-sidecar-injector` configmap. However, the Service Mesh operator does not allow edits to the configmap (the operator reverts changes to the original configmap), and there is no way in the current version to configure this setting in the ServiceMeshControlPlane CR.
* The latest versions of upstream Istio also allow to have HTTP probe rewrite per service, by setting an annotation (`sidecar.istio.io/rewriteAppHTTPProbers: "true"`) on the pods. The Red Hat Service Mesh does not yet support this functionality.

. Annotate the Incident Service pods with the `sidecar.istio.io/inject: "true"` annotation.
+
----
oc edit dc incident-service -o yaml -n $ER_DEMO_NS
----
+
In the `.spec.template.metadata` section, add the annotation:
+
----
[...]]
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: "true"
      labels:
[...]
----
. This forces a redeployment of the Incident Service. Verify that the Envoy proxy sidecar has been injected sucessfully: the new pod consists of two containers, `incident-service` and `istio-proxy`.
+
image::images/incident-service-pod-sidecar.png[]

. Create a _Policy_ for the Incident Service service.
.. Create a file called `incident-service-policy.yml` with the following content:
+
----
---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: incident-service-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: incident-service
----
** Note that the policy mode is set to `PERMISSIVE`. This means that the service will accept both HTTP and mutual TLS traffic, so services that are (not yet) part of the service mesh can still call the service. Once all services in the application are part of the mesh, the different policy modes can be switched to `STRICT` mode.
+
.. Create the Policy:
+
----
$ oc create -f incident-service-policy.yml -n $ER_DEMO_NS
----

. Create a _DestinationRule_ for the Incident Service. DestinationRule defines policies that apply to traffic intended for a service after routing has occurred. In our case we configure clients of the Incident Service to use mTLS, using the certificates generated by Citadel.
.. Create a file called `incident-service-mtls-destinationrule.yml` with the following content - replace `<emergency-response-demo namespace>` with the name of the namespace of the incident service:
+
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: incident-service-client-mtls
spec:
  host: incident-service.<emergency-response-demo namespace>.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
----
.. Create the DestinationRule:
+
----
$ oc create -f incident-service-mtls-destinationrule.yml -n $ER_DEMO_NS
----

. Create a _VirtualService_ for the Incident Service. A VirtualService defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service.
.. Create a file called `incident-service-virtualservice.yml` with the following content (replace `<user>`, `<openshift domain>` and `<controlplane namespace>` with the appropriate values for your cluster):
+
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: incident-service-virtualservice
spec:
  hosts:
  - incident-service.<user>.apps.<openshift domain>
  gateways:
  - erd-wildcard-gateway.<controlplane namespace>.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /incidents
    route:
    - destination:
        port:
          number: 8080
        host: incident-service
----
.. Create the VirtualService:
+
----
$ oc create -f incident-service-virtualservice.yml -n $ER_DEMO_NS
----

. Create a route for the Incident Service which points to the Service Mesh Ingress Gateway service.
.. Create a file called `incident-service-gateway.yml` with the following content (replace `<openshift domain>` with the domain of your cluster):
+
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: "true"
  labels:
    app: incident-service
  name: incident-service-gateway
spec:
  host: incident-service.<user>.apps.<openshift domain>
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
----
.. Create the route in the `istio-system` namespace:
+
----
$ oc create -f incident-service-gateway.yml -n $RHSM_CONTROL_PLANE_NS
----

. Delete the existing Incident Service route
+
----
$ oc delete route incident-service -n $ER_DEMO_NS
----

. Verify that the Incident Service can be reached through the Istio Ingress Gateway:
+
----
$ curl -v -k https://incident-service.<user>.apps.<openshift domain>/incidents
----
+
.Sample Output
----
*   Trying 3.123.56.177:443...
* TCP_NODELAY set
* Connected to incident-service.erd.apps.7ffc.openshift.opentlc.com (3.123.56.177) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20): TLS_AES_256_GCM_SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=*.erd.apps.7ffc.openshift.opentlc.com
*  start date: Aug 18 07:09:22 2019
*  expire date: Nov 16 07:09:22 2019
*  issuer: C=US; O=Let's Encrypt; CN=Let's Encrypt Authority X3
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x55e67b400940)
> GET /incidents HTTP/2
> Host: incident-service.erd.apps.7ffc.openshift.opentlc.com
> User-Agent: curl/7.65.3
> Accept: */*
>
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
* Connection state changed (MAX_CONCURRENT_STREAMS == 4294967295)!
< HTTP/2 200
< content-type: application/json;charset=UTF-8
< date: Mon, 19 Aug 2019 21:11:40 GMT
< x-envoy-upstream-service-time: 26
< server: istio-envoy
<
[]
----

. To check that the traffic between the Istio Ingress Gateway service and the Incident service service uses mTLS, the `istioctl` tool can be used:
+
----
$ ISTIO_INGRESSGATEWAY_POD=$(oc get pod -l app=istio-ingressgateway -o jsonpath={.items.metadata.name} -n $RHSM_CONTROL_PLANE_NS)
$ istioctl -n $RHSM_CONTROL_PLANE_NS authn tls-check ${ISTIO_INGRESSGATEWAY_POD} incident-service.$ER_DEMO_NS.svc.cluster.local
----
+
.Output
----
HOST:PORT                                                           STATUS     SERVER        CLIENT     AUTHN POLICY                                             DESTINATION RULE
incident-service.emergency-response-demo.svc.cluster.local:8080     OK         HTTP/mTLS     mTLS       incident-service-strict-mtls/emergency-response-demo     incident-service-client-mtls/emergency-response-demo
----
* `SERVER` is the mode used on the server. The Incident Service mTLS policy is set to PERMISSIVE, so the status is `HTTP/mtTLS`.
* `CLIENT` is the mode used on the client - the Istio Ingress gateway. The client uses mTLS to call the Incident Service.

. Another way to verify that the traffic between the Istio Ingress Gateway and the Incident Service uses mTLS is to check the Istio Grafana dashboards. +
In a browser window, navigate to the Istio Grafana instance (https://grafana-<controlplane namespace>.apps.<openshift domain>) and log in with your admin OpenShift credentials. Locate the _Istio Workload Dashboard_,. Select the `Incident Service` workload in the `emergency-response-demo` namespace. Scroll down to the _Inbound Workloads_ section. +
Use curl to send some requests to the Incident Service. Observe the graphs and notice a spike in the incoming requests from the Ingress Gateway. Notice that the traffic is marked as mTLS.
+
image::images/istio-grafana-workload-inbound.png[]

. Perform a run of the Emergency Response Demo to validate that the application is still working  as expected.

=== Responder Service

The procedure for enabling mTLS communication for the Responder Service and other services in the Emergency Response Demo application is very similar to the Incident Service. Only some differences will be highlighted.

. Replace the health checks in the Responder Service DeploymentConfig to use a command based health check with curl rather than a HTTP based health check.
+
----
$ oc edit dc responder-service -o yaml -n $ER_DEMO_NS
----
+
Replace the existing liveness and readiness probes with command based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080/actuator/health'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080/actuator/health'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----

. Annotate the Responder Service pods with the `sidecar.istio.io/inject: "true"` annotation.
. _Policy_ for the Responder Service service.
+
----
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: responder-service-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: responder-service
----

. _DestinationRule_ for the Responder Service:
+
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: responder-service-client-mtls
spec:
  host: responder-service.<emergency-response-demo namespace>.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
----

. _VirtualService_ for the Responder Service:
+
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: responder-service-virtualservice
spec:
  hosts:
  - "responder-service.<user>.apps.<openshift domain>"
  gateways:
  - erd-wildcard-gateway.<controlplane namespace>.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /responders
    - uri:
        prefix: /responder
    - uri:
        exact: /stats
    route:
    - destination:
        port:
          number: 8080
        host: responder-service
----

. Route for the Responder Service:
+
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: responder-service
  name: responder-service-gateway
spec:
  host: "responder-service.<user>.apps.<openshift domain>"
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
----
. Delete the existing Responder Service route
+
----
$ oc delete route responder-service -n $ER_DEMO_NS
----

=== Disaster Simulator

. Replace the health checks in the Disaster Simulator DeploymentConfig to use a command based health check with curl rather than a HTTP based health check.
+
----
$ oc edit dc disaster-simulator -o yaml -n $ER_DEMO_NS
----
+
Replace the existing liveness and readiness probes with command based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----

. Annotate the Disaster Simulator pods with the `sidecar.istio.io/inject: "true"` annotation.
. _Policy_ for the Disaster Simulator service.
+
----
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: disaster-simulator-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: disaster-simulator
----

. _DestinationRule_ for the Responder Service:
+
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: disaster-simulator-client-mtls
spec:
  host: disaster-simulator.<emergency-response-demo namespace>.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
----

. _VirtualService_ for the Responder Service:
+
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: disaster-simulator-virtualservice
spec:
  hosts:
  - disaster-simulator.<user>.apps.<openshift domain>
  gateways:
  - erd-wildcard-gateway.<controlplane namespace>.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        port:
          number: 8080
        host: disaster-simulator
----

. Route for the Responder Service:
+
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: disaster-simulator
  name: disaster-simulator-gateway
spec:
  host: disaster-simulator.<user>.apps.<openshift domain>
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
----
. Delete the existing Disaster Simulator route
+
----
$ oc delete route disaster-simulator -n $ER_DEMO_NS
----

=== Incident Priority Service

. Replace the health checks in the Incident Priority Service DeploymentConfig to use a command based health check with curl rather than a HTTP based health check.
+
----
$ oc edit dc incident-priority-service -o yaml -n $ER_DEMO_NS
----
+
Replace the existing liveness and readiness probes with command based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080/health'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080/health'
          initialDelaySeconds: 10
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----

. Annotate the Incident Priority Service pods with the `sidecar.istio.io/inject: "true"` annotation.
. _Policy_ for the Incident Priority Service.
+
----
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: incident-priority-service-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: incident-priority-service
----

. _DestinationRule_ for the Incident Priority Service:
+
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: incident-priority-service-client-mtls
spec:
  host: incident-priority-service.<emergency-response-demo namespace>.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
----

. _VirtualService_ for the Incident Priority Service:
+
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: incident-priority-service-virtualservice
spec:
  hosts:
  - incident-priority-service.<user>.apps.<openshift domain>
  gateways:
  - erd-wildcard-gateway.<controlplane namespace>.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /priority
    - uri:
        exact: /reset
    route:
    - destination:
        port:
          number: 8080
        host: incident-priority-service
----

. Route for the Incident Priority Service:
+
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: incident-priority-service
  name: incident-priority-service-gateway
spec:
  host: incident-priority-service.<user>.apps.<openshift domain>
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
----
. Delete the existing Incident Priority Service route
+
----
$ oc delete route incident-priority-service -n $ER_DEMO_NS
----

=== Process Service

The process service is not exposed outside of the cluster, so there is no need for a VirtualService and route.

. Replace the health checks in the Process Service DeploymentConfig to use a command based health check with curl rather than a HTTP based health check.
+
----
$ oc edit dc process-service -o yaml -n $ER_DEMO_NS
----
+
Replace the existing liveness and readiness probes with command based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080/actuator/health'
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080/actuator/health'
          initialDelaySeconds: 45
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----

. Annotate the Process Service pods with the `sidecar.istio.io/inject: "true"` annotation.
. _Policy_ for the Responder Service service.
+
----
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: process-service-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: process-service
----

. _DestinationRule_ for the Responder Service:
+
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: process-service-client-mtls
spec:
  host: process-service.<emergency-response-demo namespace>.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
----

=== Mission Service

. Replace the health checks in the Mission Service DeploymentConfig to use a command based health check with curl rather than a HTTP based health check.
+
----
$ oc edit dc mission-service -o yaml -n $ER_DEMO_NS
----
+
Replace the existing liveness and readiness probes with command based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080'
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 1
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080'
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 1
[...]
----

. Annotate the Mission Service pods with the `sidecar.istio.io/inject: "true"` annotation.
. _Policy_ for the Mission Service service.
+
----
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: mission-service-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: mission-service
----

. _DestinationRule_ for the Mission Service:
+
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: mission-service-client-mtls
spec:
  host: mission-service.<emergency-response-demo namespace>.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
----

. _VirtualService_ for the Mission Service:
+
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: mission-service-virtualservice
spec:
  hosts:
  - mission-service.<user>.app.<openshift-domain>
  gateways:
  - erd-wildcard-gateway.<controlplane namespace>.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        port:
          number: 8080
        host: mission-service
----

. Route for the Mission Service:
+
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: mission-service
  name: mission-service-gateway
spec:
  host: mission-service.<user>.apps.<openshift domain>
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
----
. Delete the existing Mission Service route:
+
----
$ oc delete route mission-service -n $ER_DEMO_NS
----

=== Responder Simulator

. Replace the health checks in the Responder Simulator DeploymentConfig to use a command based health check with curl rather than a HTTP based health check.
+
----
$ oc edit dc responder-simulator -o yaml -n $ER_DEMO_NS
----
+
Replace the existing liveness and readiness probes with command based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080'
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 1
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080'
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 1
[...]
----

. Annotate the Responder Simulator pods with the `sidecar.istio.io/inject: "true"` annotation.
. _Policy_ for the Responder Simulator service.
+
----
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: responder-simulator-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: responder-simulator
----

. _DestinationRule_ for the Responder Simulator:
+
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: responder-simulator-client-mtls
spec:
  host: responder-simulator.<emergency-response-demo namespace>.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
----

. _VirtualService_ for the Responder Simulator:
+
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: responder-simulator-virtualservice
spec:
  hosts:
  - responder-simulator.<user>.apps.<openshift-domain>
  gateways:
  - erd-wildcard-gateway.<controlplane namespace>.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        port:
          number: 8080
        host: responder-simulator
----

. Route for the Responder Simulator:
+
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: responder-simulator
  name: responder-simulator-gateway
spec:
  host: responder-simulator.<user>.apps.<openshit domain>
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
----
. Delete the existing Responder Simulator route
+
----
$ oc delete route responder-simulator -n $ER_DEMO_NS
----

=== Process Viewer

. Replace the health checks in the Process Viewer DeploymentConfig to use a command based health check with curl rather than a HTTP based health check.
+
----
$ oc edit dc process-viewer -o yaml -n $ER_DEMO_NS
----
+
Replace the existing liveness and readiness probes with command based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080/health'
          initialDelaySeconds: 15
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080/health'
          initialDelaySeconds: 5
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----

. Annotate the Process Viewer pods with the `sidecar.istio.io/inject: "true"` annotation.
. _Policy_ for the Process Viewer service.
+
----
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: process-viewer-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: process-viewer
----

. _DestinationRule_ for Process Viewer:
+
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: process-viewer-client-mtls
spec:
  host: process-viewer.<emergency-response-demo namespace>.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
----

. _VirtualService_ for Process Viewer:
+
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: process-viewer-virtualservice
spec:
  hosts:
  - process-viewer.<user>.apps.<openshift domain>
  gateways:
  - erd-wildcard-gateway.<controlplane namespace>.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /image
    - uri:
        prefix: /data
    route:
    - destination:
        port:
          number: 8080
        host: process-viewer
----

. Route for Process Viewer:
+
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: process-viewer
  name: process-viewer-gateway
spec:
  host: process-viewer.<user>.apps.<openshift-domain>
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
----
. Delete the existing Process Viewer route
+
----
$ oc delete route process-viewer -n $ER_DEMO_NS
----

=== Emergency Console

. Replace the health checks in the Emergency Console DeploymentConfig to use a command based health check with curl rather than a HTTP based health check.
+
----
$ oc edit dc emergency-console -o yaml -n $ER_DEMO_NS
----
+
Replace the existing liveness and readiness probes with command based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----

. Annotate the Emergency Console pods with the `sidecar.istio.io/inject: "true"` annotation.
. _Policy_ for the Emergency Console service.
+
----
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: emergency-console-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: emergency-console
----

. _DestinationRule_ for Emergency Console:
+
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: emergency-console-client-mtls
spec:
  host: emergency-console.<emergency-response-demo namespace>.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
----

. _VirtualService_ for Emergency Console:
+
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: emergency-console-virtualservice
spec:
  hosts:
  - emergency-console.<user>.apps.<openshift domain>
  gateways:
  - erd-wildcard-gateway.<controlplane namespace>.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        port:
          number: 8080
        host: emergency-console
----

. Route for Emergency Console:
+
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: emergency-console
  name: emergency-console-gateway
spec:
  host: emergency-console.<user>.apps.<openshift domain>
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
----
. Delete the existing Emergency Console route
+
----
$ oc delete route emergency-console -n $ER_DEMO_NS
----

. The redirect URL in the `emergency-realm` in the SSO service needs to be changed. +
Log in into the Red Hat SSO instance administration console, and navigate to the `emergency realm` realm, and then to the `js` client. In the _Valid Redirect URIs_ section, remove the existing redirect URL for Emergency Console, and add the new URL for the Emergency Console:
+
----
https://emergency-console.<user>.apps.<openshift domain>/*
----
+
NOTE: check realm name

=== PostgreSQL

Service Mesh mTLS can also be enabled for TCP traffic. The Incident Service, Responder Service and Process Service use a PostgreSQL database. With Service Mesh we can secure the communication beween the service and the database.

. Scale down the Incident Service and Responder Service pods to 0 pods
. Annotate the PostgreSQL pod with the `sidecar.istio.io/inject: "true"` annotation.

. _Policy_ for the PostgresQL service.
+
----
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: postgresql-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: postgresql
----

. _DestinationRule_ for PostgreSQL:
+
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: postgresql-client-mtls
spec:
  host: postgresql.<emergency-response-demo namespace>.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
----

. After successful redeployment of the PostgreSQL pods, scale up the Incident Service and Responder Service.

=== Process Service PostgreSQL

. Scale down the Process Service to 0 pods
. Annotate the Process Service PostgreSQL pod with the `sidecar.istio.io/inject: "true"` annotation.

. _Policy_ for the PostgresQL service.
+
----
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: process-postgresql-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: postgresql
----

. _DestinationRule_ for PostgreSQL:
+
----
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: process-service-postgresql-client-mtls
spec:
  host: process-service-postgresql.<emergency-response-demo namespace>.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
----

. After successful redeployment of the PostgreSQL pod, scale up the Process Service.

=== Strict mTLS Policy

. Test the Emergency Response Demo application. Everything should work as expected.
. Edit the different Policy objects, set the mTLS mode to `STRICT`:
+
----
$ oc patch policy incident-service-mtls --type='json' -p '[{"op":"replace","path":"/spec/peers/0/mtls/mode", "value": "STRICT"}]' -n -n $ER_DEMO_NS
----

. Repeat for the other policy objects.
. Test the application again. Everything should still be working as expected.

With strict mTLS enforced, the Prometheus pod in the `emergency-response-monitoring` namespace cannot longer scrape metric data from the pods in the `emergency-response-demo` namespace. +
Adding the `emergency-response-monitoring` namespace to the service mesh and injecting the Envoy proxy in the prometheus pod does not work: scraping targets for Prometheus are pod IPs, not service names. The Envoy proxy does not process the traffic properly when directed to a pod IP instead of a service URL. +
Possible solutions: +
- Applications expose Prometheus metrics on a separate port, and TLS is disabled for that port in the DestinationRule. +
- Applications expose Prometheus metrics on a separate port,  and that port is configured for pass-through in the Servie Mesh proxy init container (which sets up the networking in the pod so that a;l tyraffic is intercepted by the proxy side-car). This is a mesh-wide setting, so all appication services must use the same port. Also it seems not possible to configure the init-container settings using the Service Mesh CR.

NOTE: When using mTLS to the Postgresql database, the Process Service does not always start correctly, and frequently shows database errors in the logs: +
`org.postgresql.util.PSQLException: An I/O error occurred while sending to the backend.` +
`org.postgresql.util.PSQLException: Connection has been closed automatically because a new connection was opened for the same PooledConnection or the PooledConnection has been closed.` +
This error require further investigation.

NOTE: The communication beween the Mission Service and datagrid is not managed by the service mesh. There are several bug reports about Istio mTLS not working with StatefulSets. Also, the communication between the mission service and the datagrid uses the HotRod protocol, with support for transport level security.

NOTE: The communication beween the services and Kafka is not managed by the service mesh. Strimzi/kafka has its own security mechanisms, including transport security and authentication/authorization. 

== Authorization Policy for Services

With Service Mesh Authorization Policy, you can define service-to-service authorization policies to control which services can communicate with other services.

Service Mesh Authorization is enabled using a _ServiceMeshRbacConfig_ object. The Authorization Policies are defined using a combination of _ServiceRole_ and _ServiceRoleBinding_ objects. The ServiceRole defines list of permissions, which are then bound to a one or more _Subjects_ using a ServiceRoleBinding.

As a use-case for this lab, we can define and enable the following policy:

* All services within the Emergency Response Demo are allowed to call the Incident Service REST APIs, using all HTTP methods.
* When accessed from outside of the cluster, only those APIs who use HTTP GET are allowed.

{nbsp}

. The Service Mesh RBAC Policy requires mTLS strict mode. +
To enable mesh-wide strict mTLS, the default _ServiceMeshPolicy_ object needs to be patched to change the mTLS mode from PERMISSIVE to STRICT:
+
----
$ oc patch servicemeshpolicy default --type='json' -p '[{"op":"replace","path":"/spec/peers/0/mtls/mode","value":"STRICT"}]' -n $RHSM_CONTROL_PLANE_NS
----

. Create a ServiceMeshRbacConfig object.
* On the local file system, create a file called `servicemesh-rbac.yml` with the following contents (replace `<emergency-response-demo namespace>` with the name of the namespace of the Emergency Response Demo):
+
----
apiVersion: rbac.maistra.io/v1
kind: ServiceMeshRbacConfig
metadata:
  name: default
spec:
  mode: ON_WITH_INCLUSION
  inclusion:
    services:
      - incident-service.<emergency-response-demo namespace>.svc.cluster.local
----
+
** The name of the ServiceMeshRbacConfig must be `default`.
** The mode is set to `ON_WITH_INCLUSION`. This means that RBAC is only enabled for the services listed in the `inclusion` whitelist. +
Other possible values are `ON`, `OFF`, `ON_WITH_EXCLUSION`.
** In our case, Service Mesh authorization is only enabled for the Incident Service in the Emergency Response Demo workspace. 
* Create the object in the Service Mesh controlplane namespace:
+
----
$ oc create -f servicemesh-rbac.yml -n $RHSM_CONTROL_PLANE_NS
----

. At this point, no service can call the Incident Service, not from within the mesh nor from the outside. You can test this as follows:
* Call the `/incidents` REST endpoint of the incident service with `curl`:
+
----
$ INCIDENT_SERVICE_URL=$(oc get route incident-service-gateway -n $RHSM_CONTROL_PLANE_NS -o template --template={{.spec.host}})
$ curl -v -k https://$INCIDENT_SERVICE_URL/incidents
----
+
----
*   Trying 35.156.140.248:443...
* TCP_NODELAY set
* Connected to incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com (35.156.140.248) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20):
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=*.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
*  start date: Oct 25 15:12:23 2019 GMT
*  expire date: Jan 23 15:12:23 2020 GMT
*  issuer: C=US; O=Let's Encrypt; CN=Let's Encrypt Authority X3
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x562314295a40)
> GET /incidents HTTP/2
> Host: incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
> User-Agent: curl/7.65.3
> Accept: */*
> 
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
* Connection state changed (MAX_CONCURRENT_STREAMS == 4294967295)!
< HTTP/2 403 
< content-length: 19
< content-type: text/plain
< date: Wed, 30 Oct 2019 20:09:18 GMT
< server: istio-envoy
< x-envoy-upstream-service-time: 7
< 
* Connection #0 to host incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com left intact
RBAC: access denied
----
** The HTTP return code is 403, with message "RBAC: acces denied".
* To test a call to the Incident Service from within the cluster, open the Disaster Simulator home page in a browser, and click on the _Clear Incidents_ button. Check the logs of the Disaster Simulator pod. Expect to see something like:
+
----
Oct 30, 2019 2:19:52 PM com.redhat.cajun.navy.datagenerate.RestClientVerticle lambda$resetIncidents$4
INFO: Reset incidents: HTTP response status 403
----

. Create a _ServiceRole_ object for the services in the Emergency Response Demo namespace.
* On the local filesystem, create a file called `sr-incident-service-internal.yml` with the following contents (replace `<emergency-response-demo namespace>` with the name of the namespace of the Emergency Response Demo):
+
----
apiVersion: rbac.istio.io/v1alpha1
kind: ServiceRole
metadata:
  name: incident-service-internal
spec:
  rules:
    - services: ["incident-service.<emergency-response-demo namespace>.svc.cluster.local"]
      methods: ["*"]
----
** This policy allows to call the Incident Service service using all HTTP methods.
* Create the ServiceRole object in the Emergency Response Demo namespace:
+
----
$ oc create -f sr-incident-service-internal.yml -n $ER_DEMO_NS 
----

. Create a _ServiceRoleBinding_ referencing the ServiceRole you just created and bind it to all services in the Emergency Response Demo namespace.
* On the local filesystem, create a file called `srb-incident-service-internal.yml` with the following contents (replace `<emergency-response-demo namespace>` with the name of the namespace of the Emergency Response Demo):
+
----
apiVersion: rbac.istio.io/v1alpha1
kind: ServiceRoleBinding
metadata:
  name: incident-service-internal
spec:
  subjects:
    - properties:
        source.namespace: "<emergency-response-demo namespace>"
  roleRef:
    kind: ServiceRole
    name: "incident-service-internal"
----
* Create the ServiceRoleBinding object in the Emergency Response Demo namespace:
+
----
$ oc create -f srb-incident-service-internal.yml -n $ER_DEMO_NS 
----
. Check that the Incident Service can now be called from other services in the Emergency Response Demo namespace, but not from the outside.
* Click on the _Clear Incidents_ button of the Disaster Simulator home page. Check the logs of the Disaster Simulator pod. Expect to see something like:
+
----
Oct 30, 2019 3:02:31 PM com.redhat.cajun.navy.datagenerate.RestClientVerticle lambda$resetIncidents$4
INFO: Reset incidents: HTTP response status 200
----
* A curl request to the `/incidents` endpoint of the Incident Service still errors out with a HTTP 403 return code.

. Create a _ServiceRole_ object for the external access to the Incident Service. This Service Role grants only HTTP GET access to the Incident Service REST APIs.
* On the local filesystem, create a file called `srb-incident-service-ingress.yml` with the following contents (replace `<emergency-response-demo namespace>` with the name of the namespace of the Emergency Response Demo):
+
----
apiVersion: rbac.istio.io/v1alpha1
kind: ServiceRole
metadata:
  name: incident-service-ingress
spec:
  rules:
    - services: ["incident-service.<emergency-response-demo namespace>.svc.cluster.local"]
      methods: ["GET"]
----
** This policy allows to call the Incident Service service using HTTP GET, but disallows other HTTP methods.
* Create the ServiceRole object in the Emergency Response Demo namespace:
+
----
$ oc create -f sr-incident-service-ingress.yml -n $ER_DEMO_NS 
----

. Create a _ServiceRoleBinding_ referencing the ServiceRole you just created and bind it to the SPIFFE identity of the Service Mesh Ingress Gateway service.
* On the local filesystem, create a file called `srb-incident-service-ingress.yml` with the following contents (replace `<controlplane namespace>` with the name of the Service Mesh controlplane namespace):
+
----
apiVersion: rbac.istio.io/v1alpha1
kind: ServiceRoleBinding
metadata:
  name: incident-service-ingress
spec:
  subjects:
    - user: "cluster.local/ns/<controlplane namespace>/sa/istio-ingressgateway-service-account"
  roleRef:
    kind: ServiceRole
    name: "incident-service-internal"
----
* Create the ServiceRoleBinding object in the Emergency Response Demo namespace:
+
----
$ oc create -f srb-incident-service-ingress.yml -n $ER_DEMO_NS 
----
. Check that the Incident Service can now be called from outside of the cluster, but only for HTTP GET endpoints.
* A curl request to the `/incidents` endpoint of the Incident Service returns a HTTP 200 code.
* A curl POST request to the `/incidents` endpoint returns a HTTP 403 error:
+
----
$ curl -k -v -X POST -H "Content-type: application/json" -d '{"lat": 34.14338, "lon": -77.86569, "numberOfPeople": 3, "medicalNeeded": true, "victimName": "victim",  "victimPhoneNumber": "111-111-111" }' https://$INCIDENT_SERVICE_URL/incidents
----
+
----
Note: Unnecessary use of -X or --request, POST is already inferred.
*   Trying 35.158.14.219:443...
* TCP_NODELAY set
* Connected to incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com (35.158.14.219) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20):
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=*.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
*  start date: Oct 25 15:12:23 2019 GMT
*  expire date: Jan 23 15:12:23 2020 GMT
*  issuer: C=US; O=Let's Encrypt; CN=Let's Encrypt Authority X3
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x55e4c4578a40)
> POST /incidents HTTP/2
> Host: incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
> User-Agent: curl/7.65.3
> Accept: */*
> Content-type: application/json
> Content-Length: 141
> 
* We are completely uploaded and fine
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
* Connection state changed (MAX_CONCURRENT_STREAMS == 4294967295)!
< HTTP/2 403 
< content-length: 19
< content-type: text/plain
< date: Wed, 30 Oct 2019 21:52:32 GMT
< server: istio-envoy
< x-envoy-upstream-service-time: 1
< 
* Connection #0 to host incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com left intact
RBAC: access denied
----

== Origin Authentiction

Istio supports end-user authentication using JWT (JSON Web Token). End-user authentication verifies the original client making the request as an end-user (or device). The end-user request requires to pass in a JWT token which will be verified by the target service proxy. End-user authentication is configured in the `origins` section of a Policy object.

In this lab you will secure the Ingress Gateway service with JWT. You will use Red Hat SSO as JWT Token provider.

IMPORTANT: verify the RHSSO setup in the lab environment - realm name, admin access, ...

. Configure the Red Hat SSO instance to be able to obtain a JWT token.
* Log in as administrator into the Administration console of the Red Hat SSO instance deployed as part of the Emergency Response Demo. 
* In the SSO Administration console, navigate to the realm associated with your instance of the Emergency Response Demo.
+
IMPORTANT: Check realm name
* Open the _Clients_ tab and click _Create_ to create a new client.
* Enter `curl` as the Client ID and click _Save_:
+
image::images/sso-client-create.png[]  
* In the _Settings_ tab, disable _Standard Flow_ and enable _Direct Access Grants_, then click _Save_:
+
image::images/sso-client-settings.png[]
* Check that you have at least one user defined. On the left pane, click on _Users_, and then click on _View all users_. Expect to see the user with which you registered into the Emergency Response Demo console:
+
image::images/sso-users.png[]
. Obtain a JWT token.
* In a terminal, use the `curl` utility to obtain a token based on your SSO username and password.
+
----
$ export RHSSO_URL=<url of the RHSSO server>
$ export REALM=<name of the emergency response demo realm>
$ export USER=<realm username>
$ export PASSWD=<real password>
$ TKN=$(curl -X POST "$RHSSO_URL/auth/realms/$REALM/protocol/openid-connect/token" \
 -H "Content-Type: application/x-www-form-urlencoded" \
 -d "username=$USER" \
 -d "password=$PASSWD" \
 -d "grant_type=password" \
 -d "client_id=curl" \
  --insecure \
 | sed 's/.*access_token":"//g' | sed 's/".*//g')
$ echo $TKN
----
+
.Sample output
----
eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJrLVZtbjU2SG9keWZLSjBBLTFab01ZWUxvbDJXQlNsTjNRYVBjTzFzSTNJIn0.eyJqdGkiOiJjODgwM2M1NC1kMzRjLTRlNmYtOTM1OC0zYjhmNDQyZGE2MWMiLCJleHAiOjE1NzI1NjE1MjIsIm5iZiI6MCwiaWF0IjoxNTcyNTYxMjIyLCJpc3MiOiJodHRwczovL3Nzby11c2VyLXNzby5hcHBzLmNsdXN0ZXItYTNjNi5hM2M2LmV4YW1wbGUub3BlbnRsYy5jb20vYXV0aC9yZWFsbXMvZW1lcmdlbmN5LXJlYWxtIiwiYXVkIjoiYWNjb3VudCIsInN1YiI6ImVlNjBkNTBhLTllNGEtNDNjZi1iOWM2LWFlYzQwYjM3OWU4ZCIsInR5cCI6IkJlYXJlciIsImF6cCI6ImN1cmwiLCJhdXRoX3RpbWUiOjAsInNlc3Npb25fc3RhdGUiOiI4MDkxNTA5MC1jNzE1LTQxZDQtYTc1Mi02NjFmNmFhYjJlODQiLCJhY3IiOiIxIiwicmVhbG1fYWNjZXNzIjp7InJvbGVzIjpbIm9mZmxpbmVfYWNjZXNzIiwidW1hX2F1dGhvcml6YXRpb24iXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJzY29wZSI6ImVtYWlsIHByb2ZpbGUiLCJlbWFpbF92ZXJpZmllZCI6ZmFsc2UsIm5hbWUiOiJCZXJuYXJkIFRpc29uIiwicHJlZmVycmVkX3VzZXJuYW1lIjoiYnRpc29uIiwiZ2l2ZW5fbmFtZSI6IkJlcm5hcmQiLCJmYW1pbHlfbmFtZSI6IlRpc29uIiwiZW1haWwiOiJidGlzb25AcmVkaGF0LmNvbSJ9.dUQZM_tEcmyw4MMzkv4h6pV7yFwEyPtVkkXIbNYNgSrHU5-1sfIZlTjgtj22cOkTV6Eg6-3TN_xgduvJXJ7dcgyz-rQJ0RiYtt_U8JPl2lvAP1krFepZQzUoTZ1-Yk1ybnRamvsu3IS4gOr2e5c6H1Levy-h3ifirkO0MoAWS96tYHAYLMDedZGgxp9eNqrcF1XoyK9e-YFLufrdhIE2NyAi32mFhsGNKJMyy9Lb1DLMLuaiu6y3wVsKn9JROv_yyo9cN3y8DezDdd8KZ710m7kBCNdDU0UNq735L7aSuWzwTctiAOCaO-E5I_HH4UBWh3I2Vh2lZnBlTopA_hD7tQ
----
* Open a web browser, navigate to https://jwt.io and paste the contents of the $TKN environment variable into the left debugger window to view the actual contents of the JSON Web Token (JWT):
+
image::images/jwt-token.png[]

. Create a _Policy_ object to configure end-user authentication for the Ingress Gateway service.
* On the local filesystem, create a file called `ingressgateway-origin.yml` with the following contents:
+
----
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: ingressgateway-origin
spec:
  targets:
    - name: istio-ingressgateway
  origins:
    - jwt:
        issuer: <token issuer>
        jwksUri: <JWT Web Key url>
  principalBinding: USE_ORIGIN
----
+
** Replace <token issuer> with the value of the _iss_ field in the token. It's value should be `<url of the RHSSO server>/auth/realms/<realm name>`.
** Replace <JWT Web Key url> with the value of `<url of the RHSSO server>/auth/realms/<realm name>/protocol/openid-connect/certs`
* Create the Policy object in the Control Plane namespace:
+
----
$ oc create -f ingressgateway-origin.yml -n $RHSM_CONTROL_PLANE_NS 
----

. With the Policy in place, all requests through the Ingress Gateway will require a valid token.
* Try to access the `/incidents` endpoint of the Incident Service:
+
----
$ INCIDENT_SERVICE_URL=$(oc get route incident-service-gateway -n $RHSM_CONTROL_PLANE_NS -o template --template={{.spec.host}})
$ curl -v -k https://$INCIDENT_SERVICE_URL/incidents
----
+
----
*   Trying 35.158.14.219:443...
* TCP_NODELAY set
* Connected to incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com (35.158.14.219) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20):
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=*.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
*  start date: Oct 25 15:12:23 2019 GMT
*  expire date: Jan 23 15:12:23 2020 GMT
*  issuer: C=US; O=Let's Encrypt; CN=Let's Encrypt Authority X3
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x561b88ce3a40)
> GET /incidents HTTP/2
> Host: incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
> User-Agent: curl/7.65.3
> Accept: */*
> 
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
* Connection state changed (MAX_CONCURRENT_STREAMS == 4294967295)!
< HTTP/2 401 
< content-length: 29
< content-type: text/plain
< date: Thu, 31 Oct 2019 22:58:37 GMT
< server: istio-envoy
< 
* Connection #0 to host incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com left intact
Origin authentication failed.
----
+
** The HTTP return code is 401, with message "Origin authentication failed".
* Obtain a new token, and add the token as a `Authorization` header to the request:
+
----
$ TKN=$(curl -X POST "$RHSSO_URL/auth/realms/$REALM/protocol/openid-connect/token" \
 -H "Content-Type: application/x-www-form-urlencoded" \
 -d "username=$USER" \
 -d "password=$PASSWD" \
 -d "grant_type=password" \
 -d "client_id=curl" \
  --insecure \
 | sed 's/.*access_token":"//g' | sed 's/".*//g')
$ curl -v -k -H "Authorization: Bearer $TKN" https://$INCIDENT_SERVICE_URL/incidents
----
+
----
*   Trying 35.156.140.248:443...
* TCP_NODELAY set
* Connected to incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com (35.156.140.248) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20):
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=*.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
*  start date: Oct 25 15:12:23 2019 GMT
*  expire date: Jan 23 15:12:23 2020 GMT
*  issuer: C=US; O=Let's Encrypt; CN=Let's Encrypt Authority X3
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x558e40afda40)
> GET /incidents HTTP/2
> Host: incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
> User-Agent: curl/7.65.3
> Accept: */*
> Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJrLVZtbjU2SG9keWZLSjBBLTFab01ZWUxvbDJXQlNsTjNRYVBjTzFzSTNJIn0.eyJqdGkiOiI2Y2IwYjlmMC1jMzFiLTQyOTQtYWJkMi1hZWVlMTEwNWQ2ZDEiLCJleHAiOjE1NzI1NjM0NTQsIm5iZiI6MCwiaWF0IjoxNTcyNTYzMTU0LCJpc3MiOiJodHRwczovL3Nzby11c2
VyLXNzby5hcHBzLmNsdXN0ZXItYTNjNi5hM2M2LmV4YW1wbGUub3BlbnRsYy5jb20vYXV0aC9yZWFsbXMvZW1lcmdlbmN5LXJlYWxtIiwiYXVkIjoiYWNjb3VudCIsInN1YiI6ImVlNjBkNTBhLTllNGEtNDNjZi1iOWM2LWFlYzQwYjM3OWU4ZCIsInR5cCI6IkJlYXJlciIsImF6cCI6ImN1cmwiLCJhdXRoX3RpbWUiOjAsInNlc3Npb25fc3RhdGUiOiJhY2MxMzQyOC0yOGFi
LTRiOWItYWEyNi02OWNlY2UxNmFiNWQiLCJhY3IiOiIxIiwicmVhbG1fYWNjZXNzIjp7InJvbGVzIjpbIm9mZmxpbmVfYWNjZXNzIiwidW1hX2F1dGhvcml6YXRpb24iXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJzY29wZSI6ImVtYWlsIH
Byb2ZpbGUiLCJlbWFpbF92ZXJpZmllZCI6ZmFsc2UsIm5hbWUiOiJCZXJuYXJkIFRpc29uIiwicHJlZmVycmVkX3VzZXJuYW1lIjoiYnRpc29uIiwiZ2l2ZW5fbmFtZSI6IkJlcm5hcmQiLCJmYW1pbHlfbmFtZSI6IlRpc29uIiwiZW1haWwiOiJidGlzb25AcmVkaGF0LmNvbSJ9.NvT75pNsT3PdQMbYFtbsPeNx9_bdZwDYdfBOO4BU5GPvgB6z3RTIBfKJCnGpBZmpZw46PgJ
8eLljOZ3eDmZzPGStSMSBvgDndNv7AhdscmXVisGGR-WCLzDYtkYrZXjGVLIzhpLP_lTfapFflXlQWNZk1DkPsY_xuJ7dIX-vMQPurn0fyBhFcNtl0F6uHd7s02bCMqsX5EoOa0mV5lkI-RIqNjb94h12lBRVza9Go7aQ7q6NWP5LRr3RkKmdaLtz4E--L3pK4fVGCuhFAJfmQk7LndIQESs4DZvdtYYvQuT6RQEaLvm_0dh77xsKkIuPI7jXCugyAl_8ODeXEqsQcg
> 
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
* Connection state changed (MAX_CONCURRENT_STREAMS == 4294967295)!
< HTTP/2 200 
< content-type: application/json;charset=UTF-8
< date: Thu, 31 Oct 2019 23:06:09 GMT
< x-envoy-upstream-service-time: 17
< server: istio-envoy
< 
* Connection #0 to host incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com left intact
[]
----
** The request succeeds with a HTTP return code 200.

NOTE: All external traffic to the Emergence Response application goes through the Ingress Gateway, and all requests require a valid JWT token. This means that the Disaster Simulator UI and the Emergency Console won't work as expected anymore. To fix this would require to have a separate Ingress Gateway for these services, which does not require origin authentication.

NOTE: The Service Mesh Policy only checks for a valid token, based on the issuer, the validy of the signature on the token and the token expiry time. It is possible to configure more fine-grained authorization based on groups or roles, however, the way that Red Hat SSO encodes role information in a JWT Token is incompatible with how the Service Mesh expects this information.

NOTE: The Service Mesh will only propagate the JWT token one hop. It will take the body of the JWT token and pass it along to the application in a separate header. The JWT body will be sent in the `sec-istio-auth-userinfo` header. It will be the responsibility of the application to propagate the token to downstream services or resubmit for a new token based on the end user’s identity and the service’s identity.

Before continuing with the remainder of the labs, delete the end-user authentication policy:
+
----
oc delete policy ingressgateway-origin -n $RHSM_CONTROL_PLANE_NS 
----
