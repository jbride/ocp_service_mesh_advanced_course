:noaudio:
:scrollbar:
:toc2:
:linkattrs:

== Istio Security: mTLS and authorization policies

.Prerequisites
* Access to OpenShift 4.x cluster
* Istio Controlplane installed and ready.
* Emergency Response Demo installed.

.Goals
* Enable transport security (mTLS) for inter-service communication in Emergency Response Demo app
* Set authorization policies for inter-service communication in Emergency Response Demo

:numbered:

== Red Hat Service Mesh Control Plane Installation Review

Before starting the lab, let's review the Red Hat Service Mesh installation.

. Ensure you're logged into OpenShift as the control plane admin user.
+
----
$ oc login $LAB_MASTER_API -u $SM_CP_ADMIN -p $OCP_PASSWD
----
. The Service Mesh control plane is installed in the namespace `$SM_CP_ADMIN-istio-system`. +
The control plane is defined by the `ServiceshMeshControlPlane` custom resource.
+
----
$ oc get servicemeshcontrolplane -n $SM_CP_NS
----
+
.Output
----
NAME           READY
full-install   True
----

. Review the `ServiceMeshControlPlane` Custom Resource:
+
----
$ oc get servicemeshcontrolplane full-install -o yaml -n $SM_CP_NS
----
+
----
apiVersion: maistra.io/v1
kind: ServiceMeshControlPlane
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"maistra.io/v1","kind":"ServiceMeshControlPlane","metadata":{"annotations":{},"name":"full-install","namespace":"admin50-istio-system"},"spec":{"istio":{"gateways":{"istio-egressgateway":{"autoscaleEnabled":false},"istio-ingressgateway":{"autoscaleEnabled":false
,"ior_enabled":false}},"global":{"disablePolicyChecks":false,"mtls":false,"proxy":{"resources":{"limits":{"cpu":"500m","memory":"128Mi"},"requests":{"cpu":"100m","memory":"128Mi"}}}},"kiali":{"dashboard":{"passphrase":"redhat","user":"admin"}},"mixer":{"policy":{"autoscaleEnabled":
false},"telemetry":{"autoscaleEnabled":false,"resources":{"limits":{"cpu":"500m","memory":"4G"},"requests":{"cpu":"100m","memory":"1G"}}}},"pilot":{"autoscaleEnabled":false,"traceSampling":100},"tracing":{"enabled":true}},"threeScale":{"enabled":false}}}
  creationTimestamp: "2019-11-12T15:08:30Z"
  finalizers:
  - maistra.io/istio-operator
  generation: 1
  name: full-install
  namespace: admin50-istio-system
  resourceVersion: "704679"
  selfLink: /apis/maistra.io/v1/namespaces/admin50-istio-system/servicemeshcontrolplanes/full-install
  uid: 4998df80-055e-11ea-8412-0aaef3f8697a
spec:
  istio:
    gateways:
      istio-egressgateway:
        autoscaleEnabled: false
      istio-ingressgateway:
        autoscaleEnabled: false
        ior_enabled: false
    global:
      disablePolicyChecks: false
      mtls: false
      proxy:
        resources:
          limits:
            cpu: 500m
            memory: 128Mi
          requests:
            cpu: 100m
            memory: 128Mi
    kiali:
      dashboard:
        passphrase: redhat
        user: admin
    mixer:
      policy:
        autoscaleEnabled: false
      telemetry:
        autoscaleEnabled: false
        resources:
          limits:
            cpu: 500m
            memory: 4G
          requests:
            cpu: 100m
            memory: 1G
    pilot:
      autoscaleEnabled: false
      traceSampling: 100
    tracing:
      enabled: true
  threeScale:
    enabled: false
status:
[...]
----
+
* Note that `mTLS` is set to `disabled`. This means that mTLs is not enforced, and services will be able to communicate over plain HTTP.
* Setting the mTLS as globally disabled is the recommended starting point when migrating an existing set of applications to use Istio and mTLS.
. As part of the Service Mesh installation, a global _ServiceMeshPolicy_ is created in the Service Mesh control plane namespace. +
The scope of the ServiceMeshPolicy is the Service Mesh. It can be overridden by namespace-scoped and service-scoped policies.
+
----
$ oc get servicemeshpolicy default -o yaml -n $SM_CP_NS
----
+
.Output
----
apiVersion: authentication.maistra.io/v1
kind: ServiceMeshPolicy
metadata:
  name: default
  namespace: istio-system
  ownerReferences:
    - apiVersion: maistra.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: ServiceMeshControlPlane
      name: full-install
      uid: 37f6be7f-f5d8-11e9-a89b-06c64aecfabc
  labels:
    app: security
    app.kubernetes.io/part-of: istio
    app.kubernetes.io/instance: istio-system
    release: istio
    app.kubernetes.io/version: 1.0.1-8.el8-1
    app.kubernetes.io/component: security
    app.kubernetes.io/managed-by: maistra-istio-operator
    maistra.io/owner: istio-system
    app.kubernetes.io/name: security
    chart: security
    heritage: Tiller
spec:
  peers:
    - mtls:
        mode: PERMISSIVE
----
+
* The global policy for mTLS is set to `PERMISSIVE`, which means that the proxy sidecar will accept non mTLS encrypted traffic

. _Citadel_ is the Service Mesh component responsible for the creation and rotation of the certificates that will be used for mTLS communication between services in the cluster. +
For every service account in the namespaces which are managed by the Service Mesh, Citadel creates a secret with a certificate and key pair. +
These certificates have a lifespan of 3 months and will be rotated automatically by Citadel. +
The _Subject Alternative Name_ field on the certificate corresponds to the _SPIFFE_ identity name of the service account. +
As part of client-side verification when using mTLS, this identity is verified against the _secure naming information_ maintained by the service mesh. The secure naming information contains N-to-N mappings from the server identities, which are encoded in certificates, to the service names.
. In the Emergency Response Demo project, verify that the following secrets have been created: `istio-disaster-simulator-service`, `istio.emergency-console`, `istio.incident-piority-service`, `istio.incident-service`, `istio.mission-service`, `istio.process-service`, `istio.process-viewer`, `istio.responder-service`, `istio.responder-simulator-service`
. Visualize the contents of the `istio.incident-service` secret.
+
----
$ oc get secret istio.incident-service -o jsonpath={.data.cert-chain\\.pem} -n $ERDEMO_NS | base64 --decode
----
+
.Sample Output
----
-----BEGIN CERTIFICATE-----
MIIDNjCCAh6gAwIBAgIRAKjJU0K3IyoNGdLGrknYQX8wDQYJKoZIhvcNAQELBQAw
GDEWMBQGA1UEChMNY2x1c3Rlci5sb2NhbDAeFw0xOTA4MTgxNjE0MjZaFw0xOTEx
MTYxNjE0MjZaMAAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCvZvwn
0vdDFzfEXnJk4fW9/J2mZNWCyLxltDoUrJnWNI8AZZaIzzkAoj29rDbvIG3ZKPKo
UXSMq5eVv4uavWh8AYOmFeJAUab5I//XdSxCwqonWcjocoiZ4AUjCiyZQ+CwZToV
BR7lysMnbuU+Nk+eC1l92bANYEpAv8cQQ2neHpl8qLhja8w6hrUcGzYKu+brxyhB
qib9r3cueGhmRBN3gnq2XDoQfiQqFBoy3wiptaOxBOHzCyyOroXiV2lOrgdkTiqC
VzAqY52jIQMgP2v/HY30N7ot/q7F4jEWx4n9dALRIdT3z8KZOhmccyQsMWePA5Ci
Z3RydxgNwVYcONTnAgMBAAGjgZIwgY8wDgYDVR0PAQH/BAQDAgWgMB0GA1UdJQQW
MBQGCCsGAQUFBwMBBggrBgEFBQcDAjAMBgNVHRMBAf8EAjAAMFAGA1UdEQRJMEeG
RXNwaWZmZTovL2NsdXN0ZXIubG9jYWwvbnMvZW1lcmdlbmN5LXJlc3BvbnNlLWRl
bW8vc2EvaW5jaWRlbnQtc2VydmljZTANBgkqhkiG9w0BAQsFAAOCAQEAXCbWBLgW
xRcdj3oU9E7eFO+ugHhhbx7HYsj3gUrwqaZjXJxzlzSsaUmig14jIFUuYwqIr7WJ
chM/3nJUQGc3smQjtY8fMpztpMLANr4grYlB28upQ3l4rIkBigWwULeQ9qA+g6+x
Wjy17mecP6J7drgZQY2Xz7PC8S/NgDOJFueAior6QlkOp0GWOB1I8S+FvzyYXv91
wGShmD8opSwEWnmZgWx5CnTSyzUwJqp8GHbLUHTHY7OmeofGcpu8GZ/DiUSh5dEf
LhbRXBhhB2B2oMJ/4GEU15wH1gQ252c2u1l8jFU/dNvhJ5fufYTtzrvoybYmrifB
gl+CKnYHFEIp/w==
-----END CERTIFICATE-----
----
. Copy the text output of the certificate. In a browser window, navigate to https://www.sslshopper.com/certificate-decoder.html, and paste the certificate text into the text box:
+
image::images/istio-citadel-certificate.png[]
+
* The certificate subject name is set to `URI:spiffe://cluster.local/ns/<emergency-response-demo namespace>/sa/incident-service`. This corresponds to the identity of the service as managed by Istio.
* The certificate validity is 3 months.

=== Istio Ingress Gateway and Certificates

In an OpenShift environment, a Route is used to expose services outside the cluster. Through the route, traffic is directed to the service pods.

In a Service Mesh, a better approach is to use a gateway for traffic coming into the service mesh. This allows service mesh policies and routing rules to be applied to traffic entering the service mesh.

The Service Mesh installs an _Istio Ingress Gateway service_ (which is a Envoy proxy container running on its own). All incoming traffic into the service mesh should be routed through the Istio Ingress Gateway, to ensure mesh policies and routing rules are applied to incoming traffic.

To route the traffic from the Ingress Gateway to the target services, _Gateway_ and _VirtualService_ resources are defined .

One way to do so is to create a _Gateway_ and a _VirtualService_ for each service exposed outside of the cluster. An alternative is to use a single wildcard Gateway and a VirtualService per service. The latter is the approach taken in this lab.

The Ingress Gateway also ensures end-to-end encryption for incoming traffic: TLS termination happens at the Ingress Gateway, and traffic is re-encrypted by the Gateway using Service Mesh mTLS functionality before routing to the service. To achieve this, the public TLS key and certificate are mounted into the Ingress Gateway pods using a Secret.

NOTE: There is a limitation when combining certificates, Gateway and VirtualService resources to route traffic into the mesh with end-to-end transport security. +
Only one certificate/key pair can be mounted into the Istio Ingress Gateway. This (wildcard) certificate will be used by the different Gateway resources. However, configuring more than one Gateway using the same TLS certificate will cause browsers that leverage HTTP/2 connection reuse (i.e., most browsers) to produce 404 errors when accessing a second host after a connection to another host has already been established. +
For a description of the problem, see https://istio.io/docs/ops/common-problems/network-issues/#404-errors-occur-when-multiple-gateways-configured-with-same-tls-certificate +
The workaround for this problem is to configure a single wildcard Gateway and bind the different VirtualServices to this single Gateway.
An another alternative consists of having several Istio Ingress Gateway services, each handling their own internal service and managing their own (non-wildcard) certificate. This means however that a public certificate has to be provided for every service exposed outside of the mesh.

. Obtain a wildcard certificate and key for the gateway domain. All services exposed through the Service Mesh Ingress will be part of this domain. The domain should be a subdomain of the global OpenShift cluster application domain, for example `<user>.apps.<openshift domain>`. +
In this lab we will use a self-signed certificate.
* Create a configuration file for openSSL:
+
----
$ cat <<EOF | sudo tee ./cert.cfg
[ req ]
req_extensions     = req_ext
distinguished_name = req_distinguished_name
prompt             = no

[req_distinguished_name]
commonName=$ERDEMO_USER.apps.$SUBDOMAIN_BASE

[req_ext]
subjectAltName   = @alt_names

[alt_names]
DNS.1  = $ERDEMO_USER.apps.$SUBDOMAIN_BASE
DNS.2  = *.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
EOF
----
* Create a self-signed certificate and private key:
+
----
$ openssl req -x509 -config cert.cfg -extensions req_ext -nodes -days 730 -newkey rsa:2048 -sha256 -keyout tls.key -out tls.crt
----

. Create a secret in the Service Mesh control plane namespace with the certificates. The secret must be named `istio-ingressgateway-certs`:
+
----
$ oc create secret tls istio-ingressgateway-certs --cert tls.crt --key tls.key -n $SM_CP_NS
----
. Restart the Istio Ingress Gateway pod:
+
----
$ oc patch deployment istio-ingressgateway -p '{"spec":{"template":{"metadata":{"annotations":{"kubectl.kubernetes.io/restartedAt": "'`date -Iseconds`'"}}}}}' -n $SM_CP_NS
----


=== Wildcard Gateway

Now we can define a wildcard Gateway for our services:

. Execute the following command to create a file called `wildcard-gateway.yml` with the definition of the wildcard Gateway:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: erd-wildcard-gateway
spec:
  selector:
    istio: ingressgateway # use istio default controller
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      privateKey: /etc/istio/ingressgateway-certs/tls.key
      serverCertificate: /etc/istio/ingressgateway-certs/tls.crt
    hosts:
    - \"*.$ERDEMO_USER.apps.$SUBDOMAIN_BASE\"
" > wildcard-gateway.yml
----

. Create the wildcard Gateway:
+
----
$ oc create -f wildcard-gateway.yml -n $SM_CP_NS
----

== Configure the Emergency Response Demo services for mTLS

In this part of the lab you will configure the different services of the Emergency Response Demo for mTLS, as well as use the Istio Ingress Gateway as external entrypoint for the services.

=== Incident Service

Enabling mTLS for the Incident Service involves the following tasks:

* Inject the Envoy proxy sidecar container into the Incident Service pod  - only needed if this was not yet done in a previous lab.
* Create a _DestinationRule_ and _Policy_ to enforce mTLS when calling the Incident Service.
* Create a _VirtualService_ and a _Route_ for external access to the Incident Service through the Istio Ingress Gateway.

{empty} +

. Replace the health checks in the Incident Service DeploymentConfig to use a command based health check with curl rather than a HTTP based health check.
+
----
$ oc edit dc $ERDEMO_USER-incident-service -o yaml -n $ERDEMO_NS
----
+
Replace the existing liveness and readiness probes with command based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080/actuator/health'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080/actuator/health'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
Alternatively you can use the following `oc patch` command to patch the DeploymentConfig:
+
----
$ oc patch dc $ERDEMO_USER-incident-service --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/actuator/health"]}, "initialDelaySeconds": 30, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/actuator/health"]}, "initialDelaySeconds": 30, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
* When enforcing strict mTLS when calling the incident service, the HTTP based healthcheck will fail, as it is executed from the kubelet, and is not able to present a suitable certificate. The command based health checks are executed in the container itself, so they are not impacted.
* The Service Mesh sidecar injector service can be configured to rewrite HTTP probes at sidecar injection time, so that the requests will be sent to Pilot, which will then redirect to the application. This global configuration is set in the `istio-sidecar-injector` configmap. However, the Service Mesh operator does not allow edits to the configmap (the operator reverts changes to the original configmap), and there is no way in the current version to configure this setting in the ServiceMeshControlPlane CR.
* The latest versions of upstream Istio also allow to have HTTP probe rewrite per service, by setting an annotation (`sidecar.istio.io/rewriteAppHTTPProbers: "true"`) on the pods. The Red Hat Service Mesh does not yet support this functionality.

. Annotate the Incident Service pods with the `sidecar.istio.io/inject: "true"` annotation. Skip this step if the service has been annotated in a previous lab.
+
----
oc edit dc incident-service -o yaml -n $ERDEMO_NS
----
+
In the `.spec.template.metadata` section, add the annotation:
+
----
[...]]
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: "true"
      labels:
[...]
----
+
Alternatively, run the following `oc patch` command:
+
----
$ oc patch dc $ERDEMO_USER-incident-service --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-incident-service\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----
+
This forces a redeployment of the Incident Service. Verify that the Envoy proxy sidecar has been injected sucessfully: the new pod consists of two containers, `incident-service` and `istio-proxy`.
+
image::images/incident-service-pod-sidecar.png[]

. Create a _Policy_ for the Incident Service service.
.. Create a file called `incident-service-policy.yml` with the following command:
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: incident-service-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: $ERDEMO_USER-incident-service
" > incident-service-policy.yml
----
** Note that the policy mTLS mode is set to `PERMISSIVE`. This means that the service proxy will accept both HTTP and mutual TLS traffic, so services that are not yet part of the service mesh can still call the Incident Service. Once all services in the application are part of the mesh, the different policy mTLS modes can be switched to `STRICT` mode.
+
.. Create the Policy:
+
----
$ oc create -f incident-service-policy.yml -n $ERDEMO_NS
----

. Create a _DestinationRule_ for the Incident Service. A DestinationRule defines policies that apply to traffic intended for a service after routing has occurred. In our case we configure clients of the Incident Service to use mTLS, using the certificates generated by Citadel.
.. Create a file called `incident-service-mtls-destinationrule.yml` with the following command:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: incident-service-client-mtls
spec:
  host: $ERDEMO_USER-incident-service.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" > incident-service-mtls-destinationrule.yml
----
.. Create the DestinationRule:
+
----
$ oc create -f incident-service-mtls-destinationrule.yml -n $ERDEMO_NS
----

. Create a _VirtualService_ for the Incident Service. A VirtualService defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service.
.. Create a file called `incident-service-virtualservice.yml` with the following command:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: incident-service-virtualservice
spec:
  hosts:
  - incident-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
  - erd-wildcard-gateway.$SM_CP_NS.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /incidents
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-incident-service.$ERDEMO_NS.svc.cluster.local
" > incident-service-virtualservice.yml
----
.. Create the VirtualService:
+
----
$ oc create -f incident-service-virtualservice.yml -n $ERDEMO_NS
----

. Create a route for the Incident Service which points to the Istio Ingress Gateway service.
.. Create a file called `incident-service-gateway.yml` with the following command:
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: \"true\"
  labels:
    app: incident-service
  name: incident-service-gateway
spec:
  host: incident-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" > incident-service-gateway.yml
----
.. Create the route in the control plane namespace:
+
----
$ oc create -f incident-service-gateway.yml -n $SM_CP_NS
----

. Delete the existing Incident Service route
+
----
$ oc delete route $ERDEMO_USER-incident-service -n $ERDEMO_NS
----

. Verify that the Incident Service can be reached through the Istio Ingress Gateway:
+
----
$ curl -v -k https://incident-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/incidents
----
+
.Sample Output
----
*   Trying 3.123.56.177:443...
* TCP_NODELAY set
* Connected to incident-service.erd.apps.7ffc.openshift.opentlc.com (3.123.56.177) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20): TLS_AES_256_GCM_SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=*.erd.apps.7ffc.openshift.opentlc.com
*  start date: Aug 18 07:09:22 2019
*  expire date: Nov 16 07:09:22 2019
*  issuer: C=US; O=Let's Encrypt; CN=Let's Encrypt Authority X3
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x55e67b400940)
> GET /incidents HTTP/2
> Host: incident-service.erd.apps.7ffc.openshift.opentlc.com
> User-Agent: curl/7.65.3
> Accept: */*
>
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
* Connection state changed (MAX_CONCURRENT_STREAMS == 4294967295)!
< HTTP/2 200
< content-type: application/json;charset=UTF-8
< date: Mon, 19 Aug 2019 21:11:40 GMT
< x-envoy-upstream-service-time: 26
< server: istio-envoy
<
[]
----

. To check that the traffic between the Istio Ingress Gateway service and the Incident Service service uses mTLS, the `istioctl` tool can be used:
+
----
$ ISTIO_INGRESSGATEWAY_POD=$(oc get pod -l app=istio-ingressgateway -o jsonpath={.items[0].metadata.name} -n $SM_CP_NS)
$ istioctl -n $SM_CP_NS -i $SM_CP_NS authn tls-check ${ISTIO_INGRESSGATEWAY_POD} $ERDEMO_USER-incident-service.$ERDEMO_NS.svc.cluster.local
----
+
.Sample Output
----
HOST:PORT                                                           STATUS     SERVER        CLIENT     AUTHN POLICY                                             DESTINATION RULE
incident-service.emergency-response-demo.svc.cluster.local:8080     OK         HTTP/mTLS     mTLS       incident-service-strict-mtls/emergency-response-demo     incident-service-client-mtls/emergency-response-demo
----
* `SERVER` is the mode used on the server. The Incident Service mTLS policy is set to PERMISSIVE, so the status is `HTTP/mtTLS`.
* `CLIENT` is the mode used on the client - the Istio Ingress gateway. The client uses mTLS to call the Incident Service.

. Another way to verify that the traffic between the Istio Ingress Gateway and the Incident Service uses mTLS is to check the Istio Grafana dashboards. +
In a browser window, navigate to the Istio Grafana instance (https://grafana-$SM_CP_NS.apps.$SUBDOMAIN_BASE) and log in with your admin OpenShift credentials. Locate the _Istio Workload Dashboard_. Select the `Incident Service` workload in the `emergency-response-demo` namespace. Scroll down to the _Inbound Workloads_ section. +
Use curl to send some requests to the Incident Service. Observe the graphs and notice a spike in the incoming requests from the Ingress Gateway. Notice that the traffic is marked as mTLS.
+
image::images/istio-grafana-workload-inbound.png[]

. Perform a run of the Emergency Response Demo to validate that the application is still working as expected.

=== Responder Service

The procedure for enabling mTLS communication for the Responder Service and other services in the Emergency Response Demo application is very similar to the Incident Service. Only some differences will be highlighted in the next section.

. Replace the health checks in the Responder Service DeploymentConfig to use a command based health check with curl rather than a HTTP based health check.
+
----
$ oc edit dc responder-service -o yaml -n $ERDEMO_NS
----
+
Replace the existing liveness and readiness probes with command based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080/actuator/health'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080/actuator/health'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
Alternatively, run the following `oc patch` command:
+
----
$ oc patch dc $ERDEMO_USER-responder-service --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/actuator/health"]},"initialDelaySeconds": 30, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/actuator/health"]},"initialDelaySeconds": 30, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----

. Annotate the Responder Service pods with the `sidecar.istio.io/inject: "true"` annotation.
+
----
$ oc patch dc $ERDEMO_USER-responder-service --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-responder-service\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----
. _Policy_ for the Responder Service service.
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: responder-service-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: $ERDEMO_USER-responder-service
" > responder-service-policy.yml
----
+
----
$ oc create -f responder-service-policy.yml -n $ERDEMO_NS
----

. _DestinationRule_ for the Responder Service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: responder-service-client-mtls
spec:
  host: $ERDEMO_USER-responder-service.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" > responder-service-mtls-destinationrule.yml
----
+
----
$ oc create -f responder-service-mtls-destinationrule.yml -n $ERDEMO_NS
----
. _VirtualService_ for the Responder Service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: responder-service-virtualservice
spec:
  hosts:
  - \"responder-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE\"
  gateways:
  - erd-wildcard-gateway.$SM_CP_NS.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /responders
    - uri:
        prefix: /responder
    - uri:
        exact: /stats
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-responder-service.$ERDEMO_NS.svc.cluster.local
" > responder-service-virtualservice.yml
----
+
----
$ oc create -f responder-service-virtualservice.yml -n $ERDEMO_NS
----
. Route for the Responder Service:
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: responder-service
  name: responder-service-gateway
spec:
  host: "responder-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE"
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" > responder-service-gateway.yml
----
+
----
$ oc create -f responder-service-gateway.yml -n $SM_CP_NS
----
. Delete the existing Responder Service route
+
----
$ oc delete route $ERDEMO_USER-responder-service -n $ERDEMO_NS
----
. To test external access to the Responder Service:
+
----
$ curl -v -k https://responder-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/responders/available
----

=== Disaster Simulator

. Replace the health checks in the Disaster Simulator DeploymentConfig to use a command based health check with curl rather than a HTTP based health check.
+
----
$ oc edit dc disaster-simulator -o yaml -n $ERDEMO_NS
----
+
Replace the existing liveness and readiness probes with command based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
Alternatively, run the following `oc patch` command:
+
----
$ oc patch dc $ERDEMO_USER-disaster-simulator --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080"]}, "initialDelaySeconds": 10, "timeoutSeconds": 1, "periodSeconds": 10, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080"]},"initialDelaySeconds": 10, "timeoutSeconds": 1, "periodSeconds": 10, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
. Annotate the Disaster Simulator pods with the `sidecar.istio.io/inject: "true"` annotation.
+
----
$ oc patch dc $ERDEMO_USER-disaster-simulator --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-disaster-simulator\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----
. _Policy_ for the Disaster Simulator service.
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: disaster-simulator-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: $ERDEMO_USER-disaster-simulator
" > disaster-simulator-policy.yml
----
+
----
$ oc create -f disaster-simulator-policy.yml -n $ERDEMO_NS
----

. _DestinationRule_ for the Disaster Simulator Service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: disaster-simulator-client-mtls
spec:
  host: $ERDEMO_USER-disaster-simulator.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" > disaster-simulator-mtls-destinationrule.yml
----
+
----
$ oc create -f disaster-simulator-mtls-destinationrule.yml -n $ERDEMO_NS
----

. _VirtualService_ for the Responder Service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: disaster-simulator-virtualservice
spec:
  hosts:
  - disaster-simulator.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
  - erd-wildcard-gateway.$SM_CP_NS.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-disaster-simulator.$ERDEMO_NS.svc.cluster.local
" > disaster-simulator-virtualservice.yml
----
+
----
$ oc create -f disaster-simulator-virtualservice.yml -n $ERDEMO_NS
----

. Route for the Disaster Simulator:
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: disaster-simulator
  name: disaster-simulator-gateway
spec:
  host: disaster-simulator.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" > disaster-simulator-gateway.yml
----
+
----
$ oc create -f disaster-simulator-gateway.yml -n $SM_CP_NS
----

. Delete the existing Disaster Simulator route
+
----
$ oc delete route $ERDEMO_USER-disaster-simulator -n $ERDEMO_NS
----

. To test external access to the Disaster Simulator:
+
----
$ curl -v -k https://disaster-simulator.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
----

=== Incident Priority Service

. Replace the health checks in the Incident Priority Service DeploymentConfig to use a command based health check with curl rather than a HTTP based health check.
+
----
$ oc edit dc $ERDEMO_USER-incident-priority-service -o yaml -n $ERDEMO_NS
----
+
Replace the existing liveness and readiness probes with command based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080/health'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080/health'
          initialDelaySeconds: 10
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
Alternatively, run the following `oc patch` command:
+
----
$ oc patch dc $ERDEMO_USER-incident-priority-service --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/health"]}, "initialDelaySeconds": 30, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/health"]},"initialDelaySeconds": 10, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
. Annotate the Incident Priority Service pods with the `sidecar.istio.io/inject: "true"` annotation.
+
----
$ oc patch dc $ERDEMO_USER-incident-priority-service --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-incident-priority-service\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----
. _Policy_ for the Incident Priority Service.
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: incident-priority-service-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: $ERDEMO_USER-incident-priority-service
" > incident-priority-service-policy.yml
----
+
----
$ oc create -f incident-priority-service-policy.yml -n $ERDEMO_NS
----
. _DestinationRule_ for the Incident Priority Service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: incident-priority-service-client-mtls
spec:
  host: $ERDEMO_USER-incident-priority-service.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" > incident-priority-service-mtls-destinationrule.yml
----
+
----
$ oc create -f incident-priority-service-mtls-destinationrule.yml -n $ERDEMO_NS
----

. _VirtualService_ for the Incident Priority Service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: incident-priority-service-virtualservice
spec:
  hosts:
  - incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
  - erd-wildcard-gateway.$SM_CP_NS.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /priority
    - uri:
        exact: /reset
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-incident-priority-service.$ERDEMO_NS.svc.cluster.local
" > incident-priority-service-virtualservice.yml
----
+
----
$ oc create -f incident-priority-service-virtualservice.yml -n $ERDEMO_NS
----

. Route for the Incident Priority Service:
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: incident-priority-service
  name: incident-priority-service-gateway
spec:
  host: incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" > incident-priority-service-gateway.yml
----
+
----
$ oc create -f incident-priority-service-gateway.yml -n $SM_CP_NS
----

. Delete the existing Incident Priority Service route
+
----
$ oc delete route $ERDEMO_USER-incident-priority-service -n $ERDEMO_NS
----

. To test external access to the Incident Priority Service:
+
----
$ curl -v -k https://incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/priority/qwerty
----

=== Process Service

The process service is not exposed outside of the cluster, so there is no need for a VirtualService and Route.

. Replace the health checks in the Process Service DeploymentConfig to use a command based health check with curl rather than a HTTP based health check.
+
----
$ oc edit dc process-service -o yaml -n $ERDEMO_NS
----
+
Replace the existing liveness and readiness probes with command based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080/actuator/health'
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080/actuator/health'
          initialDelaySeconds: 45
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
Alternatively, run the following `oc patch` command:
+
----
$ oc patch dc $ERDEMO_USER-process-service --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/acuator/health"]},"initialDelaySeconds": 60, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/actuator/health"]},"initialDelaySeconds": 45, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
. Annotate the Process Service pods with the `sidecar.istio.io/inject: "true"` annotation.
+
----
$ oc patch dc $ERDEMO_USER-process-service --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-process-service\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----
. _Policy_ for the Process Service service.
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: process-service-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: $ERDEMO_USER-process-service
" > process-service-policy.yml
----
+
----
$ oc create -f process-service-policy.yml -n $ERDEMO_NS
----

. _DestinationRule_ for the Process Service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: process-service-client-mtls
spec:
  host: $ERDEMO_USER-process-service.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" > process-service-mtls-destinationrule.yml
----
+
----
$ oc create -f process-service-mtls-destinationrule.yml -n $ERDEMO_NS
----

=== Mission Service

. Replace the health checks in the Mission Service DeploymentConfig to use a command based health check with curl rather than a HTTP based health check.
+
----
$ oc edit dc mission-service -o yaml -n $ERDEMO_NS
----
+
Replace the existing liveness and readiness probes with command based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080'
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 1
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080'
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 1
[...]
----
+
Alternatively, run the following `oc patch` command:
+
----
$ oc patch dc $ERDEMO_USER-mission-service --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080"]},"initialDelaySeconds": 10, "timeoutSeconds": 1, "periodSeconds": 10, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080"]},"initialDelaySeconds": 10, "timeoutSeconds": 1, "periodSeconds": 10, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
. Annotate the Mission Service pods with the `sidecar.istio.io/inject: "true"` annotation.
+
----
$ oc patch dc $ERDEMO_USER-mission-service --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-mission-service\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----
. _Policy_ for the Mission Service service.
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: mission-service-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: $ERDEMO_USER-mission-service
" > mission-service-policy.yml
----
+
----
$ oc create -f mission-service-policy.yml -n $ERDEMO_NS
----

. _DestinationRule_ for the Mission Service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: mission-service-client-mtls
spec:
  host: $ERDEMO_USER-mission-service.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" > mission-service-mtls-destinationrule.yml
----
+
----
$ oc create -f mission-service-mtls-destinationrule.yml -n $ERDEMO_NS
----

. _VirtualService_ for the Mission Service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: mission-service-virtualservice
spec:
  hosts:
  - mission-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
  - erd-wildcard-gateway.$SM_CP_NS.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-mission-service.$ERDEMO_NS.svc.cluster.local
" > mission-service-virtualservice.yml
----
+
----
$ oc create -f mission-service-virtualservice.yml -n $ERDEMO_NS
----

. Route for the Mission Service:
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: mission-service
  name: mission-service-gateway
spec:
  host: mission-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" > mission-service-gateway.yml
----
+
----
$ oc create -f mission-service-gateway.yml -n $SM_CP_NS
----

. Delete the existing Mission Service route:
+
----
$ oc delete route $ERDEMO_USER-mission-service -n $ERDEMO_NS
----

. To test external access to the Incident Priority Service:
+
----
$ curl -v -k https://mission-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/api/missions
----

=== Responder Simulator

. Replace the health checks in the Responder Simulator DeploymentConfig to use a command based health check with curl rather than a HTTP based health check.
+
----
$ oc edit dc responder-simulator -o yaml -n $ERDEMO_NS
----
+
Replace the existing liveness and readiness probes with command based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080'
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 1
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080'
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 1
[...]
----
+
Alternatively, run the following `oc patch` command:
+
----
$ oc patch dc $ERDEMO_USER-responder-simulator --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/metrics"]},"initialDelaySeconds": 10, "timeoutSeconds": 1, "periodSeconds": 10, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/metrics"]},"initialDelaySeconds": 10, "timeoutSeconds": 1, "periodSeconds": 10, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----

. Annotate the Responder Simulator pods with the `sidecar.istio.io/inject: "true"` annotation.
+
----
$ oc patch dc $ERDEMO_USER-responder-simulator --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-responder-simulator\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----

. _Policy_ for the Responder Simulator service.
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: responder-simulator-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: $ERDEMO_USER-responder-simulator
" > responder-simulator-policy.yml
----
+
----
$ oc create -f responder-simulator-policy.yml -n $ERDEMO_NS
----

. _DestinationRule_ for the Responder Simulator:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: responder-simulator-client-mtls
spec:
  host: $ERDEMO_USER-responder-simulator.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" > responder-simulator-mtls-destinationrule.yml
----
+
----
$ oc create -f responder-simulator-mtls-destinationrule.yml -n $ERDEMO_NS
----

. _VirtualService_ for the Responder Simulator:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: responder-simulator-virtualservice
spec:
  hosts:
  - responder-simulator.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
  - erd-wildcard-gateway.$SM_CP_NS.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-responder-simulator.$ERDEMO_NS.svc.cluster.local
" > responder-simulator-virtualservice.yml
----
+
----
$ oc create -f responder-simulator-virtualservice.yml -n $ERDEMO_NS
----

. Route for the Responder Simulator:
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: responder-simulator
  name: responder-simulator-gateway
spec:
  host: responder-simulator.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" > responder-simulator-gateway.yml
----
+
----
$ oc create -f responder-simulator-gateway.yml -n $SM_CP_NS
----

. Delete the existing Responder Simulator route
+
----
$ oc delete route $ERDEMO_USER-responder-simulator -n $ERDEMO_NS
----

. To test external access to the Responder Simulator:
+
----
$ curl -v -k https://responder-simulator.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/stats/mc
----

=== Process Viewer

. Replace the health checks in the Process Viewer DeploymentConfig to use a command based health check with curl rather than a HTTP based health check.
+
----
$ oc edit dc process-viewer -o yaml -n $ERDEMO_NS
----
+
Replace the existing liveness and readiness probes with command based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080/health'
          initialDelaySeconds: 15
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080/health'
          initialDelaySeconds: 5
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
Alternatively, run the following `oc patch` command:
+
----
$ oc patch dc $ERDEMO_USER-process-viewer --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/health"]},"initialDelaySeconds": 15, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/health"]},"initialDelaySeconds": 5, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----

. Annotate the Process Viewer pods with the `sidecar.istio.io/inject: "true"` annotation.
+
----
$ oc patch dc $ERDEMO_USER-process-viewer --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-process-viewer\"}}}]" -n $ERDEMO_NS
----

. _Policy_ for the Process Viewer service.
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: process-viewer-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: $ERDEMO_USER-process-viewer
" > process-viewer-policy.yml
----
+
----
$ oc create -f process-viewer-policy.yml -n $ERDEMO_NS
----

. _DestinationRule_ for Process Viewer:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: process-viewer-client-mtls
spec:
  host: $ERDEMO_USER-process-viewer.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" > process-viewer-mtls-destinationrule.yml
----
+
----
$ oc create -f process-viewer-mtls-destinationrule.yml -n $ERDEMO_NS
----

. _VirtualService_ for Process Viewer:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: process-viewer-virtualservice
spec:
  hosts:
  - process-viewer.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
  - erd-wildcard-gateway.$SM_CP_NS.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /image
    - uri:
        prefix: /data
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-process-viewer.$ERDEMO_NS.svc.cluster.local
" > process-viewer-virtualservice.yml
----
+
----
$ oc create -f process-viewer-virtualservice.yml -n $ERDEMO_NS
----

. Route for Process Viewer:
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: process-viewer
  name: process-viewer-gateway
spec:
  host: process-viewer.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" > process-viewer-gateway.yml
----
+
----
$ oc create -f process-viewer-gateway.yml -n $SM_CP_NS
----

. Delete the existing Process Viewer route
+
----
$ oc delete route $ERDEMO_USER-process-viewer -n $ERDEMO_NS
----

. To test external access to the Incident Priority Service:
+
----
$ curl -v -k https://process-viewer.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/image/process/incident-process
----

=== Emergency Console

. Replace the health checks in the Emergency Console DeploymentConfig to use a command based health check with curl rather than a HTTP based health check.
+
----
$ oc edit dc emergency-console -o yaml -n $ERDEMO_NS
----
+
Replace the existing liveness and readiness probes with command based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://localhost:8080'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
Alternatively, run the following `oc patch` command:
+
----
$ oc patch dc $ERDEMO_USER-emergency-console --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080"]}, "initialDelaySeconds": 30, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080"]},"initialDelaySeconds": 30, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----

. Annotate the Emergency Console pods with the `sidecar.istio.io/inject: "true"` annotation.
+
----
$ oc patch dc $ERDEMO_USER-emergency-console --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-emergency-console\"}}}]" -n $ERDEMO_NS
----

. _Policy_ for the Emergency Console service.
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: emergency-console-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: $ERDEMO_USER-emergency-console
" > emergency-console-policy.yml
----
+
----
$ oc create -f emergency-console-policy.yml -n $ERDEMO_NS
----

. _DestinationRule_ for Emergency Console:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: emergency-console-client-mtls
spec:
  host: $ERDEMO_USER-emergency-console.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" > emergency-console-mtls-destinationrule.yml
----
+
----
$ oc create -f emergency-console-mtls-destinationrule.yml -n $ERDEMO_NS
----

. _VirtualService_ for Emergency Console:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: emergency-console-virtualservice
spec:
  hosts:
  - emergency-console.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
  - erd-wildcard-gateway.$SM_CP_NS.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-emergency-console.$ERDEMO_NS.svc.cluster.local
" > emergency-console-virtualservice.yml
----
+
----
$ oc create -f emergency-console-virtualservice.yml -n $ERDEMO_NS
----

. Route for Emergency Console:
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: emergency-console
  name: emergency-console-gateway
spec:
  host: emergency-console.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" > emergency-console-gateway.yml
----
+
----
$ oc create -f emergency-console-gateway.yml -n $SM_CP_NS
----

. Delete the existing Emergency Console route
+
----
$ oc delete route $ERDEMO_USER-emergency-console -n $ERDEMO_NS
----

. The redirect URL in the `emergency-realm` in the SSO ionstance needs to be changed.
* Obtain the URL for the administration console of the SSO instance:
+
----
$ echo -en "\n\nhttps://sso-user-sso.apps.$SUBDOMAIN_BASE/auth/admin/$ERDEMO_USER-emergency-realm/console\n\n"
----
* In a browser window, navigate to the SSO console. Use the username and password you used to register in the Emergency Response application. +
You are directed to the administration section of your realm.
* Open the _Clients_ tab and open the settings for the `js` client.
* In the _Valid Redirect URIs_ section, remove the existing redirect URL for Emergency Console, and add the new URL for the Emergency Console:
+
----
https://emergency-console.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/*
----

. Test access to the Emergency Console in a browser window. Use the URL `https:emergency-console.$ERDEMO_USER.apps.$SUBDOMAIN_BASE`.

=== PostgreSQL

Service Mesh mTLS can also be enabled for TCP traffic. The Incident Service, Responder Service and Process Service use a PostgreSQL database. With Service Mesh we can secure the communication beween the service and the database.

. Scale down the Incident Service and Responder Service pods to 0 pods
. Annotate the PostgreSQL pod with the `sidecar.istio.io/inject: "true"` annotation.
+
----
$ oc patch dc postgresql --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"postgresql\", \"name\":\"postgresql\"}}}]" -n $ERDEMO_NS
----

. _Policy_ for the PostgresQL service.
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: postgresql-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: postgresql
" > postgresql-policy.yml
----
+
----
$ oc create -f postgresql-policy.yml -n $ERDEMO_NS
----

. _DestinationRule_ for PostgreSQL:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: postgresql-client-mtls
spec:
  host: postgresql.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" > postgresql-mtls-destinationrule.yml
----
+
----
$ oc create -f postgresql-mtls-destinationrule.yml -n $ERDEMO_NS
----

. After successful redeployment of the PostgreSQL pods, scale up the Incident Service and Responder Service.

=== Process Service PostgreSQL

. Scale down the Process Service to 0 pods
. Annotate the Process Service PostgreSQL pod with the `sidecar.istio.io/inject: "true"` annotation.
+
----
$ oc patch dc $ERDEMO_USER-process-service-postgresql --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-process-service\", \"name\":\"$ERDEMO_USER-process-service-postgresql\"}}}]" -n $ERDEMO_NS
----

. _Policy_ for the Process Service PostgresQL service.
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: process-postgresql-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: postgresql
" > process-postgresql-policy.yml
----

. _DestinationRule_ for PostgreSQL:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: process-service-postgresql-client-mtls
spec:
  host: $ERDEMO_USER-process-service-postgresql.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" > process-postgresql-mtls-destinationrule.yml
----

. After successful redeployment of the PostgreSQL pod, scale up the Process Service.

=== Strict mTLS Policy

. Test the Emergency Response Demo application. Everything should work as expected.
. Edit the different Policy resources, set the mTLS mode to `STRICT`:
+
----
$ oc patch policy incident-service-mtls --type='json' -p '[{"op":"replace","path":"/spec/peers/0/mtls/mode", "value": "STRICT"}]' -n $ERDEMO_NS
----

. Repeat for the other policy resources.
. Test the application again. Everything should still be working as expected.

With strict mTLS enforced, the Prometheus pod in the `emergency-response-monitoring` namespace cannot longer scrape metric data from the pods in the `emergency-response-demo` namespace. +
Adding the `emergency-response-monitoring` namespace to the service mesh and injecting the Envoy proxy in the prometheus pod does not work: scraping targets for Prometheus are pod IPs, not service names. The Envoy proxy does not process the traffic properly when directed to a pod IP instead of a service URL. +
Possible solutions: +
- Applications expose Prometheus metrics on a separate port, and TLS is disabled for that port in the DestinationRule. +
- Applications expose Prometheus metrics on a separate port,  and that port is configured for pass-through in the Service Mesh proxy init container (which sets up the networking in the pod so that all traffic is intercepted by the proxy side-car). This is a mesh-wide setting, so all application services must use the same port. Also it seems not possible to configure the init-container settings using the Service Mesh CR.

NOTE: When using mTLS to the Postgresql database, the Process Service does not always start correctly, and frequently shows database errors in the logs: +
`org.postgresql.util.PSQLException: An I/O error occurred while sending to the backend.` +
`org.postgresql.util.PSQLException: Connection has been closed automatically because a new connection was opened for the same PooledConnection or the PooledConnection has been closed.` +
This error requires further investigation.

NOTE: The communication beween the Mission Service and the Datagrid is not managed by the service mesh. There are several bug reports about Istio mTLS not working with StatefulSets. Also, the communication between the mission service and the datagrid uses the HotRod protocol, with support for transport level security.

NOTE: The communication beween the services and Kafka is not managed by the service mesh. Strimzi/Kafka has its own security mechanisms, including transport security and authentication/authorization.

== Authorization Policy for Services

With Service Mesh Authorization Policy, you can define service-to-service authorization policies to control which services can communicate with other services.

Service Mesh Authorization is enabled using a _ServiceMeshRbacConfig_ resource. The Authorization Policies are defined using a combination of _ServiceRole_ and _ServiceRoleBinding_ resources. The ServiceRole defines list of permissions, which are then bound to a one or more _Subjects_ using a ServiceRoleBinding.

As a use-case for this lab, we can define and enable the following policy:

* All services within the Emergency Response Demo are allowed to call the Incident Service REST APIs, using all HTTP methods.
* When accessed from outside of the cluster, only those APIs who use HTTP GET are allowed.

{nbsp}

. The Service Mesh RBAC Policy requires mTLS strict mode. +
To enable mesh-wide strict mTLS, the default _ServiceMeshPolicy_ resource needs to be patched to change the mTLS mode from PERMISSIVE to STRICT:
+
----
$ oc patch servicemeshpolicy default --type='json' -p '[{"op":"replace","path":"/spec/peers/0/mtls/mode","value":"STRICT"}]' -n $SM_CP_NS
----

. Create a ServiceMeshRbacConfig resource.
* On the local file system, create a file called `servicemesh-rbac.yml` with the following command:
+
----
$ echo "---
apiVersion: rbac.maistra.io/v1
kind: ServiceMeshRbacConfig
metadata:
  name: default
spec:
  mode: ON_WITH_INCLUSION
  inclusion:
    services:
      - $ERDEMO_USER-incident-service.$ERDEMO_NS.svc.cluster.local
" > servicemesh-rbac.yml
----
+
** The name of the ServiceMeshRbacConfig must be `default`.
** The mode is set to `ON_WITH_INCLUSION`. This means that RBAC is only enabled for the services listed in the `inclusion` whitelist. +
Other possible values are `ON`, `OFF`, `ON_WITH_EXCLUSION`.
** In our case, Service Mesh authorization is only enabled for the Incident Service in the Emergency Response Demo namespace.
* Create the resource in the Service Mesh controlplane namespace:
+
----
$ oc create -f servicemesh-rbac.yml -n $SM_CP_NS
----

. At this point, no service can call the Incident Service, not from within the mesh nor from the outside. You can test this as follows:
* Call the `/incidents` REST endpoint of the incident service with `curl`:
+
----
$ INCIDENT_SERVICE_URL=$(oc get route incident-service-gateway -n $SM_CP_NS -o template --template={{.spec.host}})
$ curl -v -k https://$INCIDENT_SERVICE_URL/incidents
----
+
.Output
----
*   Trying 35.156.140.248:443...
* TCP_NODELAY set
* Connected to incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com (35.156.140.248) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20):
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=*.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
*  start date: Oct 25 15:12:23 2019 GMT
*  expire date: Jan 23 15:12:23 2020 GMT
*  issuer: C=US; O=Let's Encrypt; CN=Let's Encrypt Authority X3
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x562314295a40)
> GET /incidents HTTP/2
> Host: incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
> User-Agent: curl/7.65.3
> Accept: */*
>
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
* Connection state changed (MAX_CONCURRENT_STREAMS == 4294967295)!
< HTTP/2 403
< content-length: 19
< content-type: text/plain
< date: Wed, 30 Oct 2019 20:09:18 GMT
< server: istio-envoy
< x-envoy-upstream-service-time: 7
<
* Connection #0 to host incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com left intact
RBAC: access denied
----
** The HTTP return code is 403, with message "RBAC: access denied".
* To test a call to the Incident Service from within the cluster, open the Disaster Simulator home page in a browser, and click on the _Clear Incidents_ button. Check the logs of the Disaster Simulator pod. Expect to see something like:
+
----
Oct 30, 2019 2:19:52 PM com.redhat.cajun.navy.datagenerate.RestClientVerticle lambda$resetIncidents$4
INFO: Reset incidents: HTTP response status 403
----

. Create a _ServiceRole_ resource for the services in the Emergency Response Demo namespace.
* On the local filesystem, create a file called `sr-incident-service-internal.yml` with the following command:
+
----
$ echo "---
apiVersion: rbac.istio.io/v1alpha1
kind: ServiceRole
metadata:
  name: incident-service-internal
spec:
  rules:
    - services: [\"$ERDEMO_USER-incident-service.$ERDEMO_NS.svc.cluster.local\"]
      methods: [\"*\"]
" > sr-incident-service-internal.yml
----
** This policy allows to call the Incident Service service using all HTTP methods.
* Create the ServiceRole resource in the Emergency Response Demo namespace:
+
----
$ oc create -f sr-incident-service-internal.yml -n $ERDEMO_NS
----

. Create a _ServiceRoleBinding_ referencing the ServiceRole you just created and bind it to all services in the Emergency Response Demo namespace.
* On the local filesystem, create a file called `srb-incident-service-internal.yml` with the following command:
+
----
$ echo "---
apiVersion: rbac.istio.io/v1alpha1
kind: ServiceRoleBinding
metadata:
  name: incident-service-internal
spec:
  subjects:
    - properties:
        source.namespace: "$ERDEMO_NS"
  roleRef:
    kind: ServiceRole
    name: "incident-service-internal"
" > rb-incident-service-internal.yml
----
* Create the ServiceRoleBinding resource in the Emergency Response Demo namespace:
+
----
$ oc create -f srb-incident-service-internal.yml -n $ERDEMO_NS
----
. Check that the Incident Service can now be called from other services in the Emergency Response Demo namespace, but not from the outside.
* Click on the _Clear Incidents_ button of the Disaster Simulator home page. Check the logs of the Disaster Simulator pod. Expect to see something like:
+
----
Oct 30, 2019 3:02:31 PM com.redhat.cajun.navy.datagenerate.RestClientVerticle lambda$resetIncidents$4
INFO: Reset incidents: HTTP response status 200
----
* A curl request to the `/incidents` endpoint of the Incident Service still errors out with a HTTP 403 return code.

. Create a _ServiceRole_ resource for the external access to the Incident Service. This Service Role grants only HTTP GET access to the Incident Service REST APIs.
* On the local filesystem, create a file called `srb-incident-service-ingress.yml` with the following command:
+
----
$ echo "---
apiVersion: rbac.istio.io/v1alpha1
kind: ServiceRole
metadata:
  name: incident-service-ingress
spec:
  rules:
    - services: ["$ERDEMO_USER-incident-service.$ERDEMO_NS.svc.cluster.local"]
      methods: ["GET"]
" > srb-incident-service-ingress.yml
----
** This policy allows to call the Incident Service service using HTTP GET, but disallows other HTTP methods.
* Create the ServiceRole resource in the Emergency Response Demo namespace:
+
----
$ oc create -f sr-incident-service-ingress.yml -n $ERDEMO_NS
----

. Create a _ServiceRoleBinding_ referencing the ServiceRole you just created and bind it to the SPIFFE identity of the Istio Ingress Gateway service.
* On the local filesystem, create a file called `srb-incident-service-ingress.yml` with the following command:
+
----
$ echo "---
apiVersion: rbac.istio.io/v1alpha1
kind: ServiceRoleBinding
metadata:
  name: incident-service-ingress
spec:
  subjects:
    - user: "cluster.local/ns/$ERDEMO_NS/sa/istio-ingressgateway-service-account"
  roleRef:
    kind: ServiceRole
    name: "incident-service-internal"
" > srb-incident-service-ingress.yml
----
* Create the ServiceRoleBinding resource in the Emergency Response Demo namespace:
+
----
$ oc create -f srb-incident-service-ingress.yml -n $ERDEMO_NS
----
. Check that the Incident Service can now be called from outside of the cluster, but only for HTTP GET endpoints.
* A curl request to the `/incidents` endpoint of the Incident Service returns a HTTP 200 code.
* A curl POST request to the `/incidents` endpoint returns a HTTP 403 error:
+
----
$ curl -k -v -X POST -H "Content-type: application/json" -d '{"lat": 34.14338, "lon": -77.86569, "numberOfPeople": 3, "medicalNeeded": true, "victimName": "victim",  "victimPhoneNumber": "111-111-111" }' https://$INCIDENT_SERVICE_URL/incidents
----
+
.Output
----
Note: Unnecessary use of -X or --request, POST is already inferred.
*   Trying 35.158.14.219:443...
* TCP_NODELAY set
* Connected to incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com (35.158.14.219) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20):
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=*.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
*  start date: Oct 25 15:12:23 2019 GMT
*  expire date: Jan 23 15:12:23 2020 GMT
*  issuer: C=US; O=Let's Encrypt; CN=Let's Encrypt Authority X3
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x55e4c4578a40)
> POST /incidents HTTP/2
> Host: incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
> User-Agent: curl/7.65.3
> Accept: */*
> Content-type: application/json
> Content-Length: 141
>
* We are completely uploaded and fine
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
* Connection state changed (MAX_CONCURRENT_STREAMS == 4294967295)!
< HTTP/2 403
< content-length: 19
< content-type: text/plain
< date: Wed, 30 Oct 2019 21:52:32 GMT
< server: istio-envoy
< x-envoy-upstream-service-time: 1
<
* Connection #0 to host incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com left intact
RBAC: access denied
----

== Origin Authentiction

Istio supports end-user authentication using JWT (JSON Web Token). End-user authentication verifies the original client making the request as an end-user (or device). The end-user request requires to pass in a JWT token which will be verified by the target service proxy. End-user authentication is configured in the `origins` section of a Policy resource.

In this lab you will secure the Ingress Gateway service with JWT. You will use Red Hat SSO as JWT Token provider.

. Configure the Red Hat SSO instance to be able to obtain a JWT token.
* Log in as realm administrator into the Administration console of the Red Hat SSO instance deployed as part of the Emergency Response Demo.
** Obtain the URL for the administration console of the SSO instance:
+
----
$ $ echo -en "\n\nhttps://sso-user-sso.apps.$SUBDOMAIN_BASE/auth/admin/$ERDEMO_USER-emergency-realm/console\n\n"
----
** In a browser window, navigate to the SSO console. Use the username and password you used to register in the Emergency Response application. +
You are directed to the administration section of your realm.
* Open the _Clients_ tab and click _Create_ to create a new client.
* Enter `curl` as the Client ID and click _Save_:
+
image::images/sso-client-create.png[]
* In the _Settings_ tab, disable _Standard Flow_ and enable _Direct Access Grants_, then click _Save_:
+
image::images/sso-client-settings.png[]
. Obtain a JWT token.
* In a terminal, use the `curl` utility to obtain a token based on your SSO username and password.
+
----
$ export RHSSO_URL=sso-user-sso.$SUBDOMAIN_BASE
$ export REALM=$ERDEMO_USER-emergency-realm
$ export USER=<realm username>
$ export PASSWD=<realm password>
$ TKN=$(curl -X POST "$RHSSO_URL/auth/realms/$REALM/protocol/openid-connect/token" \
 -H "Content-Type: application/x-www-form-urlencoded" \
 -d "username=$USER" \
 -d "password=$PASSWD" \
 -d "grant_type=password" \
 -d "client_id=curl" \
  --insecure \
 | sed 's/.*access_token":"//g' | sed 's/".*//g')
$ echo $TKN
----
+
.Sample output
----
eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJrLVZtbjU2SG9keWZLSjBBLTFab01ZWUxvbDJXQlNsTjNRYVBjTzFzSTNJIn0.eyJqdGkiOiJjODgwM2M1NC1kMzRjLTRlNmYtOTM1OC0zYjhmNDQyZGE2MWMiLCJleHAiOjE1NzI1NjE1MjIsIm5iZiI6MCwiaWF0IjoxNTcyNTYxMjIyLCJpc3MiOiJodHRwczovL3Nzby11c2VyLXNzby5hcHBzLmNsdXN0ZXItYTNjNi5hM2M2LmV4YW1wbGUub3BlbnRsYy5jb20vYXV0aC9yZWFsbXMvZW1lcmdlbmN5LXJlYWxtIiwiYXVkIjoiYWNjb3VudCIsInN1YiI6ImVlNjBkNTBhLTllNGEtNDNjZi1iOWM2LWFlYzQwYjM3OWU4ZCIsInR5cCI6IkJlYXJlciIsImF6cCI6ImN1cmwiLCJhdXRoX3RpbWUiOjAsInNlc3Npb25fc3RhdGUiOiI4MDkxNTA5MC1jNzE1LTQxZDQtYTc1Mi02NjFmNmFhYjJlODQiLCJhY3IiOiIxIiwicmVhbG1fYWNjZXNzIjp7InJvbGVzIjpbIm9mZmxpbmVfYWNjZXNzIiwidW1hX2F1dGhvcml6YXRpb24iXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJzY29wZSI6ImVtYWlsIHByb2ZpbGUiLCJlbWFpbF92ZXJpZmllZCI6ZmFsc2UsIm5hbWUiOiJCZXJuYXJkIFRpc29uIiwicHJlZmVycmVkX3VzZXJuYW1lIjoiYnRpc29uIiwiZ2l2ZW5fbmFtZSI6IkJlcm5hcmQiLCJmYW1pbHlfbmFtZSI6IlRpc29uIiwiZW1haWwiOiJidGlzb25AcmVkaGF0LmNvbSJ9.dUQZM_tEcmyw4MMzkv4h6pV7yFwEyPtVkkXIbNYNgSrHU5-1sfIZlTjgtj22cOkTV6Eg6-3TN_xgduvJXJ7dcgyz-rQJ0RiYtt_U8JPl2lvAP1krFepZQzUoTZ1-Yk1ybnRamvsu3IS4gOr2e5c6H1Levy-h3ifirkO0MoAWS96tYHAYLMDedZGgxp9eNqrcF1XoyK9e-YFLufrdhIE2NyAi32mFhsGNKJMyy9Lb1DLMLuaiu6y3wVsKn9JROv_yyo9cN3y8DezDdd8KZ710m7kBCNdDU0UNq735L7aSuWzwTctiAOCaO-E5I_HH4UBWh3I2Vh2lZnBlTopA_hD7tQ
----
* Open a web browser, navigate to https://jwt.io and paste the contents of the $TKN environment variable into the left debugger window to view the actual contents of the JSON Web Token (JWT):
+
image::images/jwt-token.png[]

. Create a _Policy_ resource to configure end-user authentication for the Istio Ingress Gateway service.
* On the local filesystem, create a file called `ingressgateway-origin.yml` with the following command:
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: ingressgateway-origin
spec:
  targets:
    - name: istio-ingressgateway
  origins:
    - jwt:
        issuer: https://sso-user-sso.$SUBDOMAIN_BASE/auth/realms/$ERDEMO_USER-emergency-realm
        jwksUri: https://sso-user-sso.$SUBDOMAIN_BASE/auth/realms/$ERDEMO_USER-emergency-realm/protocol/openid-connect/certs
  principalBinding: USE_ORIGIN
" > ingressgateway-origin.yml
----
+
** Make sure that the value of the `issuer` element corresponds to the value of the _iss_ field in the token.
** Make sure that the value of the `jwksUri` element corresponds to the `issuer` url followed by `/protocol/openid-connect/certs`
* Create the Policy resource in the Control Plane namespace:
+
----
$ oc create -f ingressgateway-origin.yml -n $SM_CP_NS
----

. With the Policy in place, all requests through the Ingress Gateway will require a valid token.
* Try to access the `/incidents` endpoint of the Incident Service:
+
----
$ INCIDENT_SERVICE_URL=$(oc get route incident-service-gateway -n $SM_CP_NS -o template --template={{.spec.host}})
$ curl -v -k https://$INCIDENT_SERVICE_URL/incidents
----
+
.Output
----
*   Trying 35.158.14.219:443...
* TCP_NODELAY set
* Connected to incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com (35.158.14.219) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20):
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=*.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
*  start date: Oct 25 15:12:23 2019 GMT
*  expire date: Jan 23 15:12:23 2020 GMT
*  issuer: C=US; O=Let's Encrypt; CN=Let's Encrypt Authority X3
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x561b88ce3a40)
> GET /incidents HTTP/2
> Host: incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
> User-Agent: curl/7.65.3
> Accept: */*
>
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
* Connection state changed (MAX_CONCURRENT_STREAMS == 4294967295)!
< HTTP/2 401
< content-length: 29
< content-type: text/plain
< date: Thu, 31 Oct 2019 22:58:37 GMT
< server: istio-envoy
<
* Connection #0 to host incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com left intact
Origin authentication failed.
----
+
The HTTP return code is 401, with message "Origin authentication failed".
* Obtain a new token, and add the token as a `Authorization` header to the request:
+
----
$ TKN=$(curl -X POST "$RHSSO_URL/auth/realms/$REALM/protocol/openid-connect/token" \
 -H "Content-Type: application/x-www-form-urlencoded" \
 -d "username=$USER" \
 -d "password=$PASSWD" \
 -d "grant_type=password" \
 -d "client_id=curl" \
  --insecure \
 | sed 's/.*access_token":"//g' | sed 's/".*//g')
$ curl -v -k -H "Authorization: Bearer $TKN" https://$INCIDENT_SERVICE_URL/incidents
----
+
.Output
----
*   Trying 35.156.140.248:443...
* TCP_NODELAY set
* Connected to incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com (35.156.140.248) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20):
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=*.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
*  start date: Oct 25 15:12:23 2019 GMT
*  expire date: Jan 23 15:12:23 2020 GMT
*  issuer: C=US; O=Let's Encrypt; CN=Let's Encrypt Authority X3
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x558e40afda40)
> GET /incidents HTTP/2
> Host: incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
> User-Agent: curl/7.65.3
> Accept: */*
> Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJrLVZtbjU2SG9keWZLSjBBLTFab01ZWUxvbDJXQlNsTjNRYVBjTzFzSTNJIn0.eyJqdGkiOiI2Y2IwYjlmMC1jMzFiLTQyOTQtYWJkMi1hZWVlMTEwNWQ2ZDEiLCJleHAiOjE1NzI1NjM0NTQsIm5iZiI6MCwiaWF0IjoxNTcyNTYzMTU0LCJpc3MiOiJodHRwczovL3Nzby11c2
VyLXNzby5hcHBzLmNsdXN0ZXItYTNjNi5hM2M2LmV4YW1wbGUub3BlbnRsYy5jb20vYXV0aC9yZWFsbXMvZW1lcmdlbmN5LXJlYWxtIiwiYXVkIjoiYWNjb3VudCIsInN1YiI6ImVlNjBkNTBhLTllNGEtNDNjZi1iOWM2LWFlYzQwYjM3OWU4ZCIsInR5cCI6IkJlYXJlciIsImF6cCI6ImN1cmwiLCJhdXRoX3RpbWUiOjAsInNlc3Npb25fc3RhdGUiOiJhY2MxMzQyOC0yOGFi
LTRiOWItYWEyNi02OWNlY2UxNmFiNWQiLCJhY3IiOiIxIiwicmVhbG1fYWNjZXNzIjp7InJvbGVzIjpbIm9mZmxpbmVfYWNjZXNzIiwidW1hX2F1dGhvcml6YXRpb24iXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJzY29wZSI6ImVtYWlsIH
Byb2ZpbGUiLCJlbWFpbF92ZXJpZmllZCI6ZmFsc2UsIm5hbWUiOiJCZXJuYXJkIFRpc29uIiwicHJlZmVycmVkX3VzZXJuYW1lIjoiYnRpc29uIiwiZ2l2ZW5fbmFtZSI6IkJlcm5hcmQiLCJmYW1pbHlfbmFtZSI6IlRpc29uIiwiZW1haWwiOiJidGlzb25AcmVkaGF0LmNvbSJ9.NvT75pNsT3PdQMbYFtbsPeNx9_bdZwDYdfBOO4BU5GPvgB6z3RTIBfKJCnGpBZmpZw46PgJ
8eLljOZ3eDmZzPGStSMSBvgDndNv7AhdscmXVisGGR-WCLzDYtkYrZXjGVLIzhpLP_lTfapFflXlQWNZk1DkPsY_xuJ7dIX-vMQPurn0fyBhFcNtl0F6uHd7s02bCMqsX5EoOa0mV5lkI-RIqNjb94h12lBRVza9Go7aQ7q6NWP5LRr3RkKmdaLtz4E--L3pK4fVGCuhFAJfmQk7LndIQESs4DZvdtYYvQuT6RQEaLvm_0dh77xsKkIuPI7jXCugyAl_8ODeXEqsQcg
>
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
* Connection state changed (MAX_CONCURRENT_STREAMS == 4294967295)!
< HTTP/2 200
< content-type: application/json;charset=UTF-8
< date: Thu, 31 Oct 2019 23:06:09 GMT
< x-envoy-upstream-service-time: 17
< server: istio-envoy
<
* Connection #0 to host incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com left intact
[]
----
+
The request succeeds with a HTTP return code 200.

NOTE: All external traffic to the Emergence Response application goes through the Ingress Gateway, and all requests require a valid JWT token. This means that the Disaster Simulator UI and the Emergency Console won't work as expected anymore. To fix this would require to have a separate Ingress Gateway for these services, which does not require origin authentication.

NOTE: The Service Mesh Policy only checks for a valid token, based on the issuer, the validy of the signature on the token and the token expiry time. It is possible to configure more fine-grained authorization based on groups or roles, however, the way Red Hat SSO encodes role information in a JWT Token is incompatible with how the Service Mesh expects this information.

NOTE: The Service Mesh only propagates the JWT token one hop. The service proxy takes the body of the JWT token and sends it along to the application in a separate HTTP header. The JWT body is sent in the `sec-istio-auth-userinfo` header. It is the responsibility of the application to propagate the token to downstream services or resubmit for a new token based on the end users identity and the services identity.

Before continuing with the remainder of the labs, delete the end-user authentication policy:

----
$ oc delete policy ingressgateway-origin -n $SM_CP_NS
----
