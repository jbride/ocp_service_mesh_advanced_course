:noaudio:
:scrollbar:
:toc2:
:linkattrs:
:data-uri:

== Istio Security: mTLS and Authorization Policies Lab

.Prerequisites
* Access to the Red Hat^(R)^ OpenShift^(R)^ Container Platform 4.x cluster
* Istio control plane installed and ready
* Emergency Response Demo application installed

.Goals
* Enable transport security (mTLS) for inter-service communication in Emergency Response Demo
* Set authorization policies for inter-service communication in Emergency Response Demo
* Configure service mesh origin authentication
* Configure Citadel to use certificates issued by an external CA

:numbered:

== Review OpenShift Service Mesh Control Plane Installation

Before starting the lab, you review the Red Hat OpenShift Service Mesh installation.

. Make sure you are logged in to OpenShift Container Platform as the control plane admin user:
+
----
$ oc login $LAB_MASTER_API -u $SM_CP_ADMIN -p $OCP_PASSWD
----
. The service mesh control plane is installed in the `$SM_CP_ADMIN-istio-system` namespace and defined by the `ServiceMeshControlPlane` custom resource. View the custom resource:
+
----
$ oc get servicemeshcontrolplane -n $SM_CP_NS
----
+
.Sample Output
----
NAME           READY   STATUS             TEMPLATE   VERSION   AGE
full-install   9/9     UpdateSuccessful   default    v1.1      2d1h
----

. Review the `ServiceMeshControlPlane` custom resource:
+
----
$ oc get servicemeshcontrolplane full-install -o yaml -n $SM_CP_NS
----
+
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"maistra.io/v1","kind":"ServiceMeshControlPlane","metadata":{"annotations":{},"name":"full-install","namespace":"admin50-istio-system"},"spec":{"istio":{"gateways":{"istio-egressgateway":{"autoscaleEnabled":false},"istio-ingressgateway":{"autoscaleEnabled":false
,"ior_enabled":false}},"global":{"disablePolicyChecks":false,"mtls":false,"proxy":{"resources":{"limits":{"cpu":"500m","memory":"128Mi"},"requests":{"cpu":"100m","memory":"128Mi"}}}},"kiali":{"dashboard":{"passphrase":"redhat","user":"admin"}},"mixer":{"policy":{"autoscaleEnabled":
false},"telemetry":{"autoscaleEnabled":false,"resources":{"limits":{"cpu":"500m","memory":"4G"},"requests":{"cpu":"100m","memory":"1G"}}}},"pilot":{"autoscaleEnabled":false,"traceSampling":100},"tracing":{"enabled":true}},"threeScale":{"enabled":false}}}
  creationTimestamp: "2019-11-12T15:08:30Z"
  finalizers:
  - maistra.io/istio-operator
  generation: 1
  name: full-install
  namespace: admin50-istio-system
  resourceVersion: "704679"
  selfLink: /apis/maistra.io/v1/namespaces/admin50-istio-system/servicemeshcontrolplanes/full-install
  uid: 4998df80-055e-11ea-8412-0aaef3f8697a
spec:
  istio:
    gateways:
      istio-egressgateway:
        autoscaleEnabled: false
      istio-ingressgateway:
        autoscaleEnabled: false
        ior_enabled: false
    global:
      disablePolicyChecks: false
      mtls:
        auto: false
        enabled: false
      proxy:
        resources:
          limits:
            cpu: 500m
            memory: 128Mi
          requests:
            cpu: 100m
            memory: 128Mi
    kiali:
      dashboard:
        passphrase: redhat
        user: admin
    mixer:
      policy:
        autoscaleEnabled: false
      telemetry:
        autoscaleEnabled: false
        resources:
          limits:
            cpu: 500m
            memory: 4G
          requests:
            cpu: 100m
            memory: 1G
    pilot:
      autoscaleEnabled: false
      traceSampling: 100
    tracing:
      enabled: true
  threeScale:
    enabled: false
status:
[...]
----
+
* Note that `mTLS.enabled` is set to `false`. This means that mTLS is not enforced, and services are able to communicate over plain HTTP.
* Setting `mTLS` as globally disabled is the recommended starting point when migrating an existing set of applications to use Istio and mTLS.
. As part of the OpenShift Service Mesh installation, a global `ServiceMeshPolicy` policy is created in the service mesh control plane namespace.
The scope of `ServiceMeshPolicy` is the service mesh. It can be overridden by namespace-scoped and service-scoped policies. Review the policy:
+
----
$ oc get servicemeshpolicy default -o yaml -n $SM_CP_NS
----
+
.Sample Output
----
apiVersion: authentication.maistra.io/v1
kind: ServiceMeshPolicy
metadata:
  name: default
  namespace: istio-system
  ownerReferences:
    - apiVersion: maistra.io/v2
      blockOwnerDeletion: true
      controller: true
      kind: ServiceMeshControlPlane
      name: full-install
      uid: 37f6be7f-f5d8-11e9-a89b-06c64aecfabc
  labels:
    app: security
    app.kubernetes.io/component: security
    app.kubernetes.io/instance: admin27-istio-system
    app.kubernetes.io/managed-by: maistra-istio-operator
    app.kubernetes.io/name: security
    app.kubernetes.io/part-of: istio
    app.kubernetes.io/version: 2.0.0-11.el8-1
    chart: security
    heritage: Tiller
    maistra-version: 1.1.10
    maistra.io/owner: admin27-istio-system
    release: istio
spec:
  peers:
    - mtls:
        mode: PERMISSIVE
----

* The global policy for mTLS is set to `PERMISSIVE`, which means that the proxy sidecar accepts non-mTLS encrypted traffic.

=== Review Certificates
_Citadel_ is the OpenShift Service Mesh component responsible for the creation and rotation of the certificates used for mTLS communication between services in the cluster.
For every service account in the namespaces that is managed by the service mesh, Citadel creates a secret with a certificate and key pair.
These certificates have a lifespan of three months and are rotated automatically by Citadel.

The _Subject Alternative Names_ field on the certificate corresponds to the _SPIFFE_ identity name of the service account.
As part of client-side verification when using mTLS, this identity is verified against the _secure naming information_ maintained by the service mesh. The secure naming information contains N-to-N mappings from the server identities (encoded in certificates) to the service names.

. In the Emergency Response Demo project, verify that the `istio` secrets for `key-and-cert` were created:
+
----
oc get secrets --field-selector=type=istio.io/key-and-cert -n $ERDEMO_NS
----
+
.Sample Output
----
NAME                                  TYPE                    DATA   AGE
istio.builder                         istio.io/key-and-cert   3      29h
istio.default                         istio.io/key-and-cert   3      29h
istio.deployer                        istio.io/key-and-cert   3      29h
istio.disaster-simulator-service      istio.io/key-and-cert   3      29h
istio.emergency-console               istio.io/key-and-cert   3      29h
istio.incident-priority-service       istio.io/key-and-cert   3      29h
istio.incident-service                istio.io/key-and-cert   3      29h
istio.kafka-cluster-entity-operator   istio.io/key-and-cert   3      29h
istio.kafka-cluster-kafka             istio.io/key-and-cert   3      29h
istio.kafka-cluster-zookeeper         istio.io/key-and-cert   3      29h
istio.mission-service                 istio.io/key-and-cert   3      29h
istio.postgresql                      istio.io/key-and-cert   3      29h
istio.process-service                 istio.io/key-and-cert   3      29h
istio.process-viewer                  istio.io/key-and-cert   3      29h
istio.responder-client-app            istio.io/key-and-cert   3      29h
istio.responder-service               istio.io/key-and-cert   3      29h
istio.responder-simulator-service     istio.io/key-and-cert   3      29h
----

. Review the contents of the `istio.incident-service` secret:
+
----
$ oc get secret istio.incident-service -o jsonpath={.data.cert-chain\\.pem} -n $ERDEMO_NS | base64 --decode
----
+
.Sample Output
----
-----BEGIN CERTIFICATE-----
MIIDNjCCAh6gAwIBAgIRAKjJU0K3IyoNGdLGrknYQX8wDQYJKoZIhvcNAQELBQAw
GDEWMBQGA1UEChMNY2x1c3Rlci5sb2NhbDAeFw0xOTA4MTgxNjE0MjZaFw0xOTEx
MTYxNjE0MjZaMAAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCvZvwn
0vdDFzfEXnJk4fW9/J2mZNWCyLxltDoUrJnWNI8AZZaIzzkAoj29rDbvIG3ZKPKo
UXSMq5eVv4uavWh8AYOmFeJAUab5I//XdSxCwqonWcjocoiZ4AUjCiyZQ+CwZToV
BR7lysMnbuU+Nk+eC1l92bANYEpAv8cQQ2neHpl8qLhja8w6hrUcGzYKu+brxyhB
qib9r3cueGhmRBN3gnq2XDoQfiQqFBoy3wiptaOxBOHzCyyOroXiV2lOrgdkTiqC
VzAqY52jIQMgP2v/HY30N7ot/q7F4jEWx4n9dALRIdT3z8KZOhmccyQsMWePA5Ci
Z3RydxgNwVYcONTnAgMBAAGjgZIwgY8wDgYDVR0PAQH/BAQDAgWgMB0GA1UdJQQW
MBQGCCsGAQUFBwMBBggrBgEFBQcDAjAMBgNVHRMBAf8EAjAAMFAGA1UdEQRJMEeG
RXNwaWZmZTovL2NsdXN0ZXIubG9jYWwvbnMvZW1lcmdlbmN5LXJlc3BvbnNlLWRl
bW8vc2EvaW5jaWRlbnQtc2VydmljZTANBgkqhkiG9w0BAQsFAAOCAQEAXCbWBLgW
xRcdj3oU9E7eFO+ugHhhbx7HYsj3gUrwqaZjXJxzlzSsaUmig14jIFUuYwqIr7WJ
chM/3nJUQGc3smQjtY8fMpztpMLANr4grYlB28upQ3l4rIkBigWwULeQ9qA+g6+x
Wjy17mecP6J7drgZQY2Xz7PC8S/NgDOJFueAior6QlkOp0GWOB1I8S+FvzyYXv91
wGShmD8opSwEWnmZgWx5CnTSyzUwJqp8GHbLUHTHY7OmeofGcpu8GZ/DiUSh5dEf
LhbRXBhhB2B2oMJ/4GEU15wH1gQ252c2u1l8jFU/dNvhJ5fufYTtzrvoybYmrifB
gl+CKnYHFEIp/w==
-----END CERTIFICATE-----
----
. Copy the text output of the certificate.
. In a browser window, navigate to `https://www.sslshopper.com/certificate-decoder.html` and paste the certificate text into the text box there:
+
image::images/istio-citadel-certificate.png[]

* Note that the certificate subject name is set to `URI:spiffe://cluster.local/ns/<emergency-response-demo namespace>/sa/incident-service`. This corresponds to the identity of the service as managed by Istio.
* Note also that the certificate validity timeframe is three months.

=== Explore Istio Ingress Gateways and Certificates

In an OpenShift environment, a route is used to expose services outside the cluster. Through the route, traffic is directed to the service pods.

In a service mesh, a better approach is to use a gateway for incoming traffic. This allows service mesh policies and routing rules to be applied to traffic entering the service mesh.

The service mesh installs an _Istio ingress gateway service_, which is an Envoy proxy container running on its own. All incoming traffic into the service mesh should be routed through the Istio ingress gateway to ensure that mesh policies and routing rules are applied to incoming traffic.

To route the traffic from the ingress gateway to the target services, `Gateway` and `VirtualService` resources are defined. One way to do this is to create `Gateway` and `VirtualService` resources for each service exposed outside of the cluster. An alternative is to use a single wildcard `Gateway` resource along with the `VirtualService` resource for each service. The latter is the approach taken in this lab.

The ingress gateway also ensures end-to-end encryption for incoming traffic: TLS termination happens at the ingress gateway, and traffic is re-encrypted by the gateway using OpenShift Service Mesh mTLS functionality before routing to the service. To achieve this, the public TLS key and certificate are mounted into the ingress gateway pods using a secret.

[NOTE]
====
There is a limitation when combining certificates with `Gateway` and `VirtualService` resources to route traffic into the mesh with end-to-end transport security.
Only one certificate/key pair can be mounted into the Istio ingress gateway. This (wildcard) certificate is used by the different `Gateway` resources. However, configuring more than one `Gateway` resource using the same TLS certificate causes browsers that leverage HTTP/2 connection reuse--that is, most browsers--to produce 404 errors when accessing a second host after a connection to another host has already been established.
For a description of the problem, see the link:https://istio.io/docs/ops/common-problems/network-issues/#404-errors-occur-when-multiple-gateways-configured-with-same-tls-certificate[Istio documentation].

The workaround for this problem is to configure a single wildcard `Gateway` resource and bind the different `VirtualService` resources to it.
An alternative is to have several Istio ingress gateway services, each handling their own internal service and managing their own non-wildcard certificate. This means however, that a public certificate must be provided for every service exposed outside of the mesh.
====
. Obtain a wildcard certificate and key for the gateway domain:
* All services exposed through the service mesh ingress gateway are part of this domain.
* The domain needs to be a subdomain of the global OpenShift cluster application domain--for example, `<user>.apps.<openshift domain>`.
* In this lab, you use a self-signed certificate.

.. Create a configuration file for OpenSSL:
+
----
$ echo "
[ req ]
req_extensions     = req_ext
distinguished_name = req_distinguished_name
prompt             = no

[req_distinguished_name]
commonName=$ERDEMO_USER.apps.$SUBDOMAIN_BASE

[req_ext]
subjectAltName   = @alt_names

[alt_names]
DNS.1  = $ERDEMO_USER.apps.$SUBDOMAIN_BASE
DNS.2  = *.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
" > cert.cfg
----
.. Create a self-signed certificate and private key:
+
----
$ openssl req -x509 -config cert.cfg -extensions req_ext -nodes -days 730 -newkey rsa:2048 -sha256 -keyout tls.key -out tls.crt
----

. Create a secret named `istio-ingressgateway-certs` in the service mesh control plane namespace with the certificates:
+
----
$ oc create secret tls istio-ingressgateway-certs --cert tls.crt --key tls.key -n $SM_CP_NS
----
. Restart the Istio ingress gateway pod:
+
----
$ oc patch deployment istio-ingressgateway -p '{"spec":{"template":{"metadata":{"annotations":{"kubectl.kubernetes.io/restartedAt": "'`date +%FT%T%z`'"}}}}}' -n $SM_CP_NS
----


=== Define Wildcard Gateway

In this section, you define a wildcard gateway for your services.

. Create a file called `wildcard-gateway.yml` with the definition of the wildcard gateway:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: erd-wildcard-gateway
spec:
  selector:
    istio: ingressgateway # use istio default controller
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      privateKey: /etc/istio/ingressgateway-certs/tls.key
      serverCertificate: /etc/istio/ingressgateway-certs/tls.crt
    hosts:
    - \"*.$ERDEMO_USER.apps.$SUBDOMAIN_BASE\"
" > wildcard-gateway.yml
----

. Create the wildcard gateway:
+
----
$ oc create -f wildcard-gateway.yml -n $SM_CP_NS
----

== Configure Emergency Response Demo Services for mTLS

In this part of the lab, you configure the different services of the Emergency Response Demo application for mTLS, as well as configuring the use of the Istio ingress gateway as an external entry point for the services.

=== Configure Incident Service

Enabling mTLS for the incident service involves the following tasks:

* Inject the Envoy proxy sidecar container into the incident service pod if you have not done this in a previous lab.
* Create a DestinationRule and policy to enforce mTLS when calling the incident service.
* Create a VirtualService and a route for external access to the incident service through the Istio ingress gateway.

* Ensure that health checks are still working when using strict mTLS.
** When enforcing strict mTLS when calling the incident service, the HTTP-based health check fails because it is executed from the kubelet and is not able to present a suitable certificate. The command-based health checks are executed in the container itself, so they are not impacted.
** The OpenShift Service Mesh sidecar injector service can be configured to rewrite HTTP probes at sidecar injection time, so that the requests are sent to the _Pilot agent_ in the Envoy sidecar, which then redirects to the application. This global configuration is set in the `ServiceMeshControlPlane` CR and is applied to all pods in the service mesh data plane that have the Envoy sidecar injected.

==== Inject Envoy Proxy Sidecar

In this section, you annotate the incident service pods with the `sidecar.istio.io/inject: "true"` annotation. You can skip this section if the service has been annotated in a previous lab.

. Open the DeploymentConfig of the incident service for editing:
+
----
oc edit dc $ERDEMO_USER-incident-service -o yaml -n $ERDEMO_NS
----

. In the `.spec.template.metadata` section, add the annotation:
+
----
[...]
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: "true"
      labels:
[...]
----
+
[TIP]
====
Alternatively, you can run the following `oc patch` command:

----
$ oc patch dc $ERDEMO_USER-incident-service --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-incident-service\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----

This forces a redeployment of the incident service.
====

. Verify that the Envoy proxy sidecar has been injected successfully and that the new pod consists of two containers, `incident-service` and `istio-proxy`:
+
image::images/incident-service-pod-sidecar.png[]

==== Configure HTTP-Based Health Checks

In this section, you configure the HTTP-based health checks to be rewritten and handled by the Pilot agent in the sidecar container.

. Open the `Deployment Config` resource for editing:
+
----
$ oc edit dc/$ERDEMO_USER-incident-service -n $ERDEMO_NS
----
. Add the configuration to allow rewriting of HTTP probes:
+
----
spec:
    [...]
    annotations:
      sidecar.istio.io/rewriteAppHTTPProbers: "true"
  [...]
----
. Save the `DeploymentConfig` resource.
. Redeploy the incident service:
+
----
$ oc rollout latest dc/$ERDEMO_USER-incident-service -n $ERDEMO_NS
----
. Once the new pod is up and running, examine the liveness and readiness probes:
+
----
$ oc get pod <incident service pod name> -o yaml -n $ERDEMO_NS
----
+
.Sample Output
----
apiVersion: v1
kind: Pod
metadata:
  [..]
spec:
  containers:
  - env:
    - name: KUBERNETES_NAMESPACE
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.namespace
    image: quay.io/emergencyresponsedemo/incident-service@sha256:6a25a3eb7ede1fb5ec23f506d3e5e3fb8b8aef783353bff63f438be784ecc6f4
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 3
      httpGet:
        path: /app-health/user1-incident-service/livez
        port: 15020
        scheme: HTTP
      initialDelaySeconds: 30
      periodSeconds: 30
      successThreshold: 1
      timeoutSeconds: 3
    name: user1-incident-service
    ports:
    - containerPort: 8080
      name: http
      protocol: TCP
    readinessProbe:
      failureThreshold: 3
      httpGet:
        path: /app-health/user1-incident-service/readyz
        port: 15020
        scheme: HTTP
      initialDelaySeconds: 30
      periodSeconds: 30
      successThreshold: 1
      timeoutSeconds: 3
    resources:
      limits:
        cpu: 500m
        memory: 500Mi
      requests:
        cpu: 100m
        memory: 200Mi
    securityContext:
      capabilities:
        drop:
        - KILL
        - MKNOD
        - SETGID
        - SETUID
      privileged: false
      procMount: Default
      runAsUser: 1000670000
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /app/logging
      name: logging
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: incident-service-token-qfx22
      readOnly: true
  - [...]
  volumes:
  [...]
status:
  [...]
----

* Note that the HTTP probes point to port 15020 instead of 8080.
* In the sidecar container, the Pilot agent is listening on port 15020. This is the process that bootstraps the Envoy proxy.
* The Pilot agent redirects the call to the application, strips the response body and returns the response code to the kubelet.

==== Configure Command-Based Health Checks

In this section, you replace the health checks in the DeploymentConfig to use a command-based health check with `curl` rather than an HTTP-based health check.

. Open the DeploymentConfig of the incident service for editing:
+
----
$ oc edit dc $ERDEMO_USER-incident-service -o yaml -n $ERDEMO_NS
----

. Replace the existing liveness and readiness probes with command-based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/actuator/health'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/actuator/health'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
[TIP]
====
Alternatively you can use the following `oc patch` command to patch the DeploymentConfig:

----
$ oc patch dc $ERDEMO_USER-incident-service --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/actuator/health"]}, "initialDelaySeconds": 30, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/actuator/health"]}, "initialDelaySeconds": 30, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
====

==== Create `Policy`
In this section, you create a `Policy` for the incident service.

. Create a file called `incident-service-policy.yml`:
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: incident-service-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: $ERDEMO_USER-incident-service
" > incident-service-policy.yml
----
* Note that the policy's mTLS mode is set to `PERMISSIVE`. This means that the service proxy accepts both HTTP and mutual TLS traffic, so services that are not yet part of the service mesh can still call the incident service. Once all services in the application are part of the mesh, the different policy mTLS modes can be switched to `STRICT` mode.
+
. Create the policy:
+
----
$ oc create -f incident-service-policy.yml -n $ERDEMO_NS
----

==== Create `DestinationRule`
In this section, you create a `DestinationRule` resource for the incident service. A `DestinationRule` resource defines policies that apply to traffic intended for a service after routing has occurred. In this case, you configure clients of the incident service to use mTLS using the certificates generated by Citadel.

. Create a file called `incident-service-mtls-destinationrule.yml`:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: incident-service-client-mtls
spec:
  host: $ERDEMO_USER-incident-service.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" > incident-service-mtls-destinationrule.yml
----
. Create the `DestinationRule` resource:
+
----
$ oc create -f incident-service-mtls-destinationrule.yml -n $ERDEMO_NS
----

==== Create `VirtualService`

In this section, you create a `VirtualService` resource for the incident service. A `VirtualService` resource defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service.

. Create a file called `incident-service-virtualservice.yml`:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: incident-service-virtualservice
spec:
  hosts:
  - incident-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
  - erd-wildcard-gateway.$SM_CP_NS.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /incidents
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-incident-service.$ERDEMO_NS.svc.cluster.local
" > incident-service-virtualservice.yml
----
. Create the `VirtualService` resource:
+
----
$ oc create -f incident-service-virtualservice.yml -n $ERDEMO_NS
----

==== Create Route
Next, you create a route for the incident service which points to the Istio ingress gateway service.

. Create a file called `incident-service-gateway.yml`:
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: \"true\"
  labels:
    app: incident-service
  name: incident-service-gateway
spec:
  host: incident-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" > incident-service-gateway.yml
----
. Create the route in the control plane namespace:
+
----
$ oc create -f incident-service-gateway.yml -n $SM_CP_NS
----

. Delete the existing incident service route:
+
----
$ oc delete route $ERDEMO_USER-incident-service -n $ERDEMO_NS
----

. Verify that the incident service can be reached through the Istio ingress gateway:
+
----
$ curl -v -k https://incident-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/incidents
----
+
.Sample Output
----
*   Trying 3.123.56.177:443...
* TCP_NODELAY set
* Connected to incident-service.erd.apps.7ffc.openshift.opentlc.com (3.123.56.177) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20): TLS_AES_256_GCM_SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=*.erd.apps.7ffc.openshift.opentlc.com
*  start date: Aug 18 07:09:22 2019
*  expire date: Nov 16 07:09:22 2019
*  issuer: C=US; O=Let's Encrypt; CN=Let's Encrypt Authority X3
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x55e67b400940)
> GET /incidents HTTP/2
> Host: incident-service.erd.apps.7ffc.openshift.opentlc.com
> User-Agent: curl/7.65.3
> Accept: */*
>
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
* Connection state changed (MAX_CONCURRENT_STREAMS == 4294967295)!
< HTTP/2 200
< content-type: application/json;charset=UTF-8
< date: Mon, 19 Aug 2019 21:11:40 GMT
< x-envoy-upstream-service-time: 26
< server: istio-envoy
<
[]
----

==== Verify Use of mTLS

. Use the `istioctl` tool to check that the traffic between the Istio ingress gateway service and the incident service uses mTLS:
+
----
$ ISTIO_INGRESSGATEWAY_POD=$(oc get pod -l app=istio-ingressgateway -o jsonpath={.items[0].metadata.name} -n $SM_CP_NS)
$ istioctl -n $SM_CP_NS -i $SM_CP_NS authn tls-check ${ISTIO_INGRESSGATEWAY_POD} $ERDEMO_USER-incident-service.$ERDEMO_NS.svc.cluster.local
----
+
.Sample Output
[options="nowrap"]
----
HOST:PORT                                                           STATUS     SERVER        CLIENT             AUTHN POLICY                                             DESTINATION RULE
incident-service.emergency-response-demo.svc.cluster.local:8080     OK         PERMISSIVE    ISTIO_MUTUAL       incident-service-strict-mtls/emergency-response-demo     incident-service-client-mtls/emergency-response-demo
----
* `SERVER` is the mode used on the server. The incident service mTLS policy is set to `PERMISSIVE`.
* `CLIENT` is the mode used on the client--the Istio ingress gateway. The client uses mTLS to call the incident service.

. Use the Istio Grafana dashboards to verify that the traffic between the Istio ingress gateway and the incident service uses mTLS:
.. In a browser window, navigate to the Istio Grafana instance at `https://grafana-$SM_CP_NS.apps.$SUBDOMAIN_BASE` and log in with your OpenShift admin credentials.
.. On the left hand side, select the Square Icon. Then select *Dashboard > Manage*
+
image::images/dashboard-manage.png[]

.. Locate the *Istio Workload Dashboard*.
.. Select the `emergency-response-demo` namespace and the `Incident Service` workload.
+
image::images/select-namespace-and-workload.png[]

.. Scroll down to the *Inbound Workloads* section and use `curl` to send some requests to the incident service.
.. Observe the graphs and expect to see a spike in the incoming requests from the ingress gateway.
** Note that the traffic is marked as mTLS.
+
image::images/istio-grafana-workload-inbound.png[]

. Perform a run of the Emergency Response Demo application to validate that the application is still working as expected.

=== Configure Responder Service

The procedure for enabling mTLS communication for the responder service and other services in the Emergency Response Demo application is very similar to that of the incident service. Only some differences are highlighted in the sections that follow.

. Replace the health checks in the responder service DeploymentConfig to use a command-based health check with `curl` rather than an HTTP-based health check. You can skip this step if you enabled HTTP probe rewriting in the service mesh.
.. Open the DeploymentConfig of the responder service for editing:
+
----
$ oc edit dc $ERDEMO_USER-responder-service -o yaml -n $ERDEMO_NS
----

.. Replace the existing liveness and readiness probes with command-based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/actuator/health'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/actuator/health'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
[TIP]
====
Alternatively, you can run the following `oc patch` command:

----
$ oc patch dc $ERDEMO_USER-responder-service --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/actuator/health"]},"initialDelaySeconds": 30, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/actuator/health"]},"initialDelaySeconds": 30, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
====
. Annotate the responder service pods with the `sidecar.istio.io/inject: "true"` annotation:
+
----
$ oc patch dc $ERDEMO_USER-responder-service --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-responder-service\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----
. Create a `Policy` resource for the responder service:
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: responder-service-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: $ERDEMO_USER-responder-service
" > responder-service-policy.yml
----
+
----
$ oc create -f responder-service-policy.yml -n $ERDEMO_NS
----

. Create a `DestinationRule` resource for the responder service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: responder-service-client-mtls
spec:
  host: $ERDEMO_USER-responder-service.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" > responder-service-mtls-destinationrule.yml
----
+
----
$ oc create -f responder-service-mtls-destinationrule.yml -n $ERDEMO_NS
----
. Create a `VirtualService` resource for the responder service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: responder-service-virtualservice
spec:
  hosts:
  - \"responder-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE\"
  gateways:
  - erd-wildcard-gateway.$SM_CP_NS.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /responders
    - uri:
        prefix: /responder
    - uri:
        exact: /stats
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-responder-service.$ERDEMO_NS.svc.cluster.local
" > responder-service-virtualservice.yml
----
+
----
$ oc create -f responder-service-virtualservice.yml -n $ERDEMO_NS
----
. Create a route for the responder service:
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: responder-service
  name: responder-service-gateway
spec:
  host: "responder-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE"
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" > responder-service-gateway.yml
----
+
----
$ oc create -f responder-service-gateway.yml -n $SM_CP_NS
----
. Delete the existing responder service route:
+
----
$ oc delete route $ERDEMO_USER-responder-service -n $ERDEMO_NS
----
. Test external access to the responder service:
+
----
$ curl -v -k https://responder-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/responders/available
----

=== Configure Disaster Simulator

. Replace the health checks in the disaster simulator DeploymentConfig to use a command-based health check with `curl` rather than an HTTP-based health check. You can skip this step if you enabled HTTP probe rewriting in the service mesh.
.. Open the DeploymentConfig of the disaster simulator for editing:
+
----
$ oc edit dc $ERDEMO_USER-disaster-simulator -o yaml -n $ERDEMO_NS
----

.. Replace the existing liveness and readiness probes with command-based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
[TIP]
====
Alternatively, run the following `oc patch` command:

----
$ oc patch dc $ERDEMO_USER-disaster-simulator --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080"]}, "initialDelaySeconds": 10, "timeoutSeconds": 1, "periodSeconds": 10, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080"]},"initialDelaySeconds": 10, "timeoutSeconds": 1, "periodSeconds": 10, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
====
. Annotate the disaster simulator pods with the `sidecar.istio.io/inject: "true"` annotation.
+
----
$ oc patch dc $ERDEMO_USER-disaster-simulator --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-disaster-simulator\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----
. Create a `Policy` resource for the disaster simulator service:
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: disaster-simulator-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: $ERDEMO_USER-disaster-simulator
" > disaster-simulator-policy.yml
----
+
----
$ oc create -f disaster-simulator-policy.yml -n $ERDEMO_NS
----

. Create a `DestinationRule` resource for the disaster simulator service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: disaster-simulator-client-mtls
spec:
  host: $ERDEMO_USER-disaster-simulator.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" > disaster-simulator-mtls-destinationrule.yml
----
+
----
$ oc create -f disaster-simulator-mtls-destinationrule.yml -n $ERDEMO_NS
----

. Create a `VirtualService` resource for the disaster simulator service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: disaster-simulator-virtualservice
spec:
  hosts:
  - disaster-simulator.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
  - erd-wildcard-gateway.$SM_CP_NS.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-disaster-simulator.$ERDEMO_NS.svc.cluster.local
" > disaster-simulator-virtualservice.yml
----
+
----
$ oc create -f disaster-simulator-virtualservice.yml -n $ERDEMO_NS
----

. Create a route for the disaster simulator service:
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: disaster-simulator
  name: disaster-simulator-gateway
spec:
  host: disaster-simulator.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" > disaster-simulator-gateway.yml
----
+
----
$ oc create -f disaster-simulator-gateway.yml -n $SM_CP_NS
----

. Delete the existing disaster simulator route:
+
----
$ oc delete route $ERDEMO_USER-disaster-simulator -n $ERDEMO_NS
----

. Test external access to the disaster simulator service:
+
----
$ curl -v -k https://disaster-simulator.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
----

=== Configure Incident Priority Service

. Replace the health checks in the incident priority service DeploymentConfig to use a command-based health check with `curl` rather than an HTTP-based health check. You can skip this step if you enabled HTTP probe rewriting in the service mesh.
.. Open the DeploymentConfig of the incident priority service for editing:
+
----
$ oc edit dc $ERDEMO_USER-incident-priority-service -o yaml -n $ERDEMO_NS
----

.. Replace the existing liveness and readiness probes with command-based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/health'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/health'
          initialDelaySeconds: 10
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
[TIP]
====
Alternatively, run the following `oc patch` command:

----
$ oc patch dc $ERDEMO_USER-incident-priority-service --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/health"]}, "initialDelaySeconds": 30, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/health"]},"initialDelaySeconds": 10, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
====
. Annotate the incident priority service pods with the `sidecar.istio.io/inject: "true"` annotation:
+
----
$ oc patch dc $ERDEMO_USER-incident-priority-service --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-incident-priority-service\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----
. Create a `Policy` resource for the incident priority service:
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: incident-priority-service-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: $ERDEMO_USER-incident-priority-service
" > incident-priority-service-policy.yml
----
+
----
$ oc create -f incident-priority-service-policy.yml -n $ERDEMO_NS
----
. Create a `DestinationRule` resource for the incident priority service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: incident-priority-service-client-mtls
spec:
  host: $ERDEMO_USER-incident-priority-service.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" > incident-priority-service-mtls-destinationrule.yml
----
+
----
$ oc create -f incident-priority-service-mtls-destinationrule.yml -n $ERDEMO_NS
----

. Create a `VirtualService` resource for the incident priority service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: incident-priority-service-virtualservice
spec:
  hosts:
  - incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
  - erd-wildcard-gateway.$SM_CP_NS.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /priority
    - uri:
        exact: /reset
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-incident-priority-service.$ERDEMO_NS.svc.cluster.local
" > incident-priority-service-virtualservice.yml
----
+
----
$ oc create -f incident-priority-service-virtualservice.yml -n $ERDEMO_NS
----

. Create a route for the incident priority service:
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: incident-priority-service
  name: incident-priority-service-gateway
spec:
  host: incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" > incident-priority-service-gateway.yml
----
+
----
$ oc create -f incident-priority-service-gateway.yml -n $SM_CP_NS
----

. Delete the existing incident priority service route:
+
----
$ oc delete route $ERDEMO_USER-incident-priority-service -n $ERDEMO_NS
----

. Test external access to the incident priority service:
+
----
$ curl -v -k https://incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/priority/qwerty
----

=== Configure Process Service

The process service is not exposed outside of the cluster, so there is no need for a `VirtualService` resource or a route.

. Replace the health checks in the process service DeploymentConfig to use a command-based health check with `curl` rather than an HTTP-based health check. You can skip this step if you enabled HTTP probe rewriting in the service mesh.
.. Open the DeploymentConfig of the process service for editing:
+
----
$ oc edit dc $ERDEMO_USER-process-service -o yaml -n $ERDEMO_NS
----

.. Replace the existing liveness and readiness probes with command-based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/actuator/health'
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/actuator/health'
          initialDelaySeconds: 45
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
[TIP]
====
Alternatively, run the following `oc patch` command:

----
$ oc patch dc $ERDEMO_USER-process-service --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/actuator/health"]},"initialDelaySeconds": 60, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/actuator/health"]},"initialDelaySeconds": 45, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
====

. Annotate the process service pods with the `sidecar.istio.io/inject: "true"` annotation:
+
----
$ oc patch dc $ERDEMO_USER-process-service --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-process-service\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----
. Create a `Policy` resource for the process service:
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: process-service-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: $ERDEMO_USER-process-service
" > process-service-policy.yml
----
+
----
$ oc create -f process-service-policy.yml -n $ERDEMO_NS
----

. Create a `DestinationRule` resource for the process service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: process-service-client-mtls
spec:
  host: $ERDEMO_USER-process-service.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" > process-service-mtls-destinationrule.yml
----
+
----
$ oc create -f process-service-mtls-destinationrule.yml -n $ERDEMO_NS
----

=== Configure Mission Service

. Replace the health checks in the mission service DeploymentConfig to use a command-based health check with `curl` rather than an HTTP-based health check. You can skip this step if you enabled HTTP probe rewriting in the service mesh.
.. Open the DeploymentConfig of the mission service for editing:
+
----
$ oc edit dc $ERDEMO_USER-mission-service -o yaml -n $ERDEMO_NS
----

.. Replace the existing liveness and readiness probes with command-based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080'
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 1
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080'
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 1
[...]
----
+
[TIP]
====
Alternatively, run the following `oc patch` command:

----
$ oc patch dc $ERDEMO_USER-mission-service --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080"]},"initialDelaySeconds": 10, "timeoutSeconds": 1, "periodSeconds": 10, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080"]},"initialDelaySeconds": 10, "timeoutSeconds": 1, "periodSeconds": 10, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
====
. Annotate the mission service pods with the `sidecar.istio.io/inject: "true"` annotation:
+
----
$ oc patch dc $ERDEMO_USER-mission-service --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-mission-service\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----
. Create a `Policy` resource for the mission service:
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: mission-service-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: $ERDEMO_USER-mission-service
" > mission-service-policy.yml
----
+
----
$ oc create -f mission-service-policy.yml -n $ERDEMO_NS
----

. Create a `DestinationRule` resource for the mission service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: mission-service-client-mtls
spec:
  host: $ERDEMO_USER-mission-service.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" > mission-service-mtls-destinationrule.yml
----
+
----
$ oc create -f mission-service-mtls-destinationrule.yml -n $ERDEMO_NS
----

. Create a `VirtualService` resource for the mission service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: mission-service-virtualservice
spec:
  hosts:
  - mission-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
  - erd-wildcard-gateway.$SM_CP_NS.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-mission-service.$ERDEMO_NS.svc.cluster.local
" > mission-service-virtualservice.yml
----
+
----
$ oc create -f mission-service-virtualservice.yml -n $ERDEMO_NS
----

. Create a route for the mission service:
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: mission-service
  name: mission-service-gateway
spec:
  host: mission-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" > mission-service-gateway.yml
----
+
----
$ oc create -f mission-service-gateway.yml -n $SM_CP_NS
----

. Delete the existing mission service route:
+
----
$ oc delete route $ERDEMO_USER-mission-service -n $ERDEMO_NS
----

. Test external access to the mission service:
+
----
$ curl -v -k https://mission-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/api/missions
----

=== Configure Responder Simulator

. Replace the health checks in the responder simulator DeploymentConfig to use a command-based health check with `curl` rather than an HTTP-based health check. You can skip this step if you enabled HTTP probe rewriting in the service mesh.
.. Open the DeploymentConfig of the responder simulator for editing:
+
----
$ oc edit dc $ERDEMO_USER-responder-simulator -o yaml -n $ERDEMO_NS
----

.. Replace the existing liveness and readiness probes with command-based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080'
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 1
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080'
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 1
[...]
----
+
[TIP]
====
Alternatively, run the following `oc patch` command:

----
$ oc patch dc $ERDEMO_USER-responder-simulator --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/metrics"]},"initialDelaySeconds": 10, "timeoutSeconds": 1, "periodSeconds": 10, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/metrics"]},"initialDelaySeconds": 10, "timeoutSeconds": 1, "periodSeconds": 10, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
====

. Annotate the responder simulator pods with the `sidecar.istio.io/inject: "true"` annotation:
+
----
$ oc patch dc $ERDEMO_USER-responder-simulator --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-responder-simulator\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----

. Create a `Policy` resource for the responder simulator service:
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: responder-simulator-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: $ERDEMO_USER-responder-simulator
" > responder-simulator-policy.yml
----
+
----
$ oc create -f responder-simulator-policy.yml -n $ERDEMO_NS
----

. Create a `DestinationRule` resource for the responder simulator:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: responder-simulator-client-mtls
spec:
  host: $ERDEMO_USER-responder-simulator.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" > responder-simulator-mtls-destinationrule.yml
----
+
----
$ oc create -f responder-simulator-mtls-destinationrule.yml -n $ERDEMO_NS
----

. Create a `VirtualService` resource for the responder simulator:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: responder-simulator-virtualservice
spec:
  hosts:
  - responder-simulator.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
  - erd-wildcard-gateway.$SM_CP_NS.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-responder-simulator.$ERDEMO_NS.svc.cluster.local
" > responder-simulator-virtualservice.yml
----
+
----
$ oc create -f responder-simulator-virtualservice.yml -n $ERDEMO_NS
----

. Create a route for the responder simulator:
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: responder-simulator
  name: responder-simulator-gateway
spec:
  host: responder-simulator.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" > responder-simulator-gateway.yml
----
+
----
$ oc create -f responder-simulator-gateway.yml -n $SM_CP_NS
----

. Delete the existing responder simulator route:
+
----
$ oc delete route $ERDEMO_USER-responder-simulator -n $ERDEMO_NS
----

. Test external access to the responder simulator:
+
----
$ curl -v -k https://responder-simulator.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/stats/mc
----

=== Configure Process Viewer

. Replace the health checks in the process viewer DeploymentConfig to use a command-based health check with `curl` rather than an HTTP-based health check. You can skip this step if you enabled HTTP probe rewriting in the service mesh.
.. Open the DeploymentConfig of the process viewer for editing:
+
----
$ oc edit dc $ERDEMO_USER-process-viewer -o yaml -n $ERDEMO_NS
----

.. Replace the existing liveness and readiness probes with command-based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/health'
          initialDelaySeconds: 15
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/health'
          initialDelaySeconds: 5
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
[TIP]
====
Alternatively, run the following `oc patch` command:

----
$ oc patch dc $ERDEMO_USER-process-viewer --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/health"]},"initialDelaySeconds": 15, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/health"]},"initialDelaySeconds": 5, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
====

. Annotate the process viewer pods with the `sidecar.istio.io/inject: "true"` annotation:
+
----
$ oc patch dc $ERDEMO_USER-process-viewer --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-process-viewer\"}}}]" -n $ERDEMO_NS
----

. Create a `Policy` resource for the process viewer service:
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: process-viewer-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: $ERDEMO_USER-process-viewer
" > process-viewer-policy.yml
----
+
----
$ oc create -f process-viewer-policy.yml -n $ERDEMO_NS
----

. Create a `DestinationRule` resource for the process viewer:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: process-viewer-client-mtls
spec:
  host: $ERDEMO_USER-process-viewer.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" > process-viewer-mtls-destinationrule.yml
----
+
----
$ oc create -f process-viewer-mtls-destinationrule.yml -n $ERDEMO_NS
----

. Create a `VirtualService` resource for the process viewer:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: process-viewer-virtualservice
spec:
  hosts:
  - process-viewer.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
  - erd-wildcard-gateway.$SM_CP_NS.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /image
    - uri:
        prefix: /data
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-process-viewer.$ERDEMO_NS.svc.cluster.local
" > process-viewer-virtualservice.yml
----
+
----
$ oc create -f process-viewer-virtualservice.yml -n $ERDEMO_NS
----

. Create a route for the process viewer:
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: process-viewer
  name: process-viewer-gateway
spec:
  host: process-viewer.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" > process-viewer-gateway.yml
----
+
----
$ oc create -f process-viewer-gateway.yml -n $SM_CP_NS
----

. Delete the existing process viewer route:
+
----
$ oc delete route $ERDEMO_USER-process-viewer -n $ERDEMO_NS
----

. Test external access to the process viewer service:
+
----
$ curl -v -k https://process-viewer.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/image/process/incident-process
----

=== Configure Emergency Console

. Replace the health checks in the emergency console DeploymentConfig to use a command-based health check with `curl` rather than an HTTP-based health check. You can skip this step if you enabled HTTP probe rewriting in the service mesh.
.. Open the DeploymentConfig of the emergency console for editing:
+
----
$ oc edit dc $ERDEMO_USER-emergency-console -o yaml -n $ERDEMO_NS
----

.. Replace the existing liveness and readiness probes with command-based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
[TIP]
====
Alternatively, run the following `oc patch` command:

----
$ oc patch dc $ERDEMO_USER-emergency-console --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080"]}, "initialDelaySeconds": 30, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080"]},"initialDelaySeconds": 30, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
====

. Annotate the emergency console pods with the `sidecar.istio.io/inject: "true"` annotation:
+
----
$ oc patch dc $ERDEMO_USER-emergency-console --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-emergency-console\"}}}]" -n $ERDEMO_NS
----

. Create a `Policy` resource for the emergency console service:
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: emergency-console-mtls
spec:
  peers:
  - mtls:
      mode: PERMISSIVE
  targets:
  - name: $ERDEMO_USER-emergency-console
" > emergency-console-policy.yml
----
+
----
$ oc create -f emergency-console-policy.yml -n $ERDEMO_NS
----

. Create a `DestinationRule` resource for the emergency console:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: emergency-console-client-mtls
spec:
  host: $ERDEMO_USER-emergency-console.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" > emergency-console-mtls-destinationrule.yml
----
+
----
$ oc create -f emergency-console-mtls-destinationrule.yml -n $ERDEMO_NS
----

. Create a `VirtualService` resource for the emergency console:
+
----
$ echo "---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: emergency-console-virtualservice
spec:
  hosts:
  - emergency-console.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
  - erd-wildcard-gateway.$SM_CP_NS.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-emergency-console.$ERDEMO_NS.svc.cluster.local
" > emergency-console-virtualservice.yml
----
+
----
$ oc create -f emergency-console-virtualservice.yml -n $ERDEMO_NS
----

. Create a route for the emergency console:
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: emergency-console
  name: emergency-console-gateway
spec:
  host: emergency-console.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" > emergency-console-gateway.yml
----
+
----
$ oc create -f emergency-console-gateway.yml -n $SM_CP_NS
----

. Delete the existing emergency console route:
+
----
$ oc delete route $ERDEMO_USER-emergency-console -n $ERDEMO_NS
----

. The redirect URL in the `emergency-realm` in the single sign-on for Red Hat solutions (SSO) instance needs to be changed. Obtain the URL for the administration console of the SSO instance:
+
----
$ echo -en "\n\nhttps://sso-user-sso.apps.$SUBDOMAIN_BASE/auth/admin/$ERDEMO_USER-emergency-realm/console\n\n"
----
. In a browser window, navigate to the SSO console and log in using the username and password you registered with in the Emergency Response Demo application.
* Expect to be routed to the administration section of your realm.
. In the left-hand menu, click *Clients* and select the `js` client.
. On the *Settings* tab, scroll to the *Valid Redirect URIs* field, remove the existing redirect URL for the emergency console, and add the new URL for the emergency console:
+
----
https://emergency-console.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/*
----

. Test access to the emergency console in a browser window using the `https://emergency-console.$ERDEMO_USER.apps.$SUBDOMAIN_BASE` URL.

=== Set Strict mTLS Policy

. Test the Emergency Response Demo application.
* Expect everything to work correctly.
. Edit the incident service `Policy` resource and set the mTLS mode to `STRICT`:
+
----
$ oc patch policy incident-service-mtls --type='json' -p '[{"op":"replace","path":"/spec/peers/0/mtls/mode", "value": "STRICT"}]' -n $ERDEMO_NS
----

. Repeat the previous step for the other `Policy` resources.
. Test the application again.
* Expect everything to work correctly.

// With strict mTLS enforced, the Prometheus pod in the `$ERDEMO_USER-er-metrics` namespace can no longer scrape metric data from the pods in the `$ERDEMO_USER-er-demo` namespace.
// Adding the `$ERDEMO_USER-er-metrics` namespace to the service mesh and injecting the Envoy proxy in the Prometheus pod does not work because the scraping targets for Prometheus are pod IP addresses, not service names. The Envoy proxy does not process the traffic properly when directed to a pod IP instead of a service URL.

// Possible solutions:

// - Applications expose Prometheus metrics on a separate port and TLS is disabled for that port in the DestinationRule.
// - Applications expose Prometheus metrics on a separate port, and that port is configured for pass-through in the service mesh proxy init container (which sets up the networking in the pod so that all traffic is intercepted by the proxy sidecar). This is a mesh-wide setting, so all application services must use the same port. Also it is not possible to configure the init container settings using the service mesh CR.

== Set Authorization Policy for Services using AuthorizationPolicy

Istio 1.4 introduces the _AuthorizationPolicy_ policy, which is a major update to the previous role-based access control (RBAC) policy. The new policy provides these improvements:

* Aligns with Istio configuration model.
* Improves the user experience by simplifying the API. AuthorizationPolicy only uses 1 CRD instead of three.
* Supports more use cases (e.g. Ingress/Egress gateway support) without added complexity.

With a service mesh authorization policy, you can define service-to-service authorization policies to control which services can communicate with other services.

To configure an authorization policy, you create an _AuthorizationPolicy_ custom resource. An authorization policy includes a selector, an action, and a list of rules:

* The _selector_ field specifies the target of the policy
* The _action_ field specifies whether to allow or deny the request. The default action is allow.
* The _rules_ specify when to trigger the action.
** The _from_ field in the rules specifies the sources of the request
** The _to_ field in the rules specifies the operations of the request
** The _when_ field specifies the conditions needed to apply the rule

As a use case for this lab, you define and enable the following policy:

* All services within the Emergency Response Demo application are allowed to call the incident service REST APIs, using all HTTP methods.
* When accessed from outside the cluster, only the APIs that use HTTP GET are allowed.

{nbsp}

. Create a deny-all policy for the incident service.
.. On the local file system, create a file called `deny-all-incident-service.yml`:
+
----
$ echo "---
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: deny-all-incident-service
spec:
  selector:
    matchLabels:
      app: $ERDEMO_USER-incident-service
      group: erd-services
" > deny-all-incident-service.yml
----
* The selector matches the labels on the incident service workload (pods).
* There are no rules defined, so no rules can be matched. This effectively corresponds to a deny-all policy.

.. Create the resource in the emergency response demo namespace:
+
----
$ oc create -f deny-all-incident-service.yml -n $ERDEMO_NS
----

. At this point, no service can call the incident service from within the mesh or from outside the mesh. Test this as follows:
.. Call the `/incidents` REST endpoint of the incident service with `curl`:
+
----
$ INCIDENT_SERVICE_URL=$(oc get route incident-service-gateway -n $SM_CP_NS -o template --template={{.spec.host}})
$ curl -v -k https://$INCIDENT_SERVICE_URL/incidents
----
+
.Sample Output
----           
*   Trying 35.158.48.203:443...                                       
* TCP_NODELAY set                                                     
* Connected to incident-service.user1.apps.cluster-18ac.18ac.example.opentlc.com (35.158.48.203) port 443 (#0)                               
* ALPN, offering h2                                                   
* ALPN, offering http/1.1                                             
* successfully set certificate verify locations:                      
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt                          
  CApath: none                                                        
* TLSv1.3 (OUT), TLS handshake, Client hello (1):                     
* TLSv1.3 (IN), TLS handshake, Server hello (2):                      
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):              
* TLSv1.3 (IN), TLS handshake, Certificate (11):                      
* TLSv1.3 (IN), TLS handshake, CERT verify (15):                      
* TLSv1.3 (IN), TLS handshake, Finished (20):                         
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):                                                                                  
* TLSv1.3 (OUT), TLS handshake, Finished (20):                        
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384               
* ALPN, server accepted to use h2                                     
* Server certificate:                                                 
*  subject: CN=user1.apps.cluster-18ac.18ac.example.opentlc.com                                                                              
*  start date: May  4 18:51:29 2020 GMT                               
*  expire date: May  4 18:51:29 2022 GMT                              
*  issuer: CN=user1.apps.cluster-18ac.18ac.example.opentlc.com                                                                               
*  SSL certificate verify result: self signed certificate (18), continuing anyway.                                                           
* Using HTTP2, server supports multi-use                              
* Connection state changed (HTTP/2 confirmed)                         
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0                                                             
* Using Stream ID: 1 (easy handle 0x55efdc8a4200)                     
> GET /incidents HTTP/2                                               
> Host: incident-service.user1.apps.cluster-18ac.18ac.example.opentlc.com                                                                    
> User-Agent: curl/7.66.0                                             
> Accept: */*                                                         
>                                                                     
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):                 
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):                 
* old SSL session ID is stale, removing                               
* Connection state changed (MAX_CONCURRENT_STREAMS == 4294967295)!                                                                           
< HTTP/2 403                                                          
< content-length: 19                                                  
< content-type: text/plain                                            
< date: Tue, 05 May 2020 15:37:51 GMT                                 
< server: istio-envoy                                                 
< x-envoy-upstream-service-time: 6                                    
<                                                                     
* Connection #0 to host incident-service.user1.apps.cluster-18ac.18ac.example.opentlc.com left intact                                        
RBAC: access denied
----
** The HTTP return code is 403, with message "RBAC: access denied".
.. Test a call to the incident service from within the cluster by opening the *Disaster Simulator* home page in a browser and clicking *Clear Incidents*. Then check the logs of the disaster simulator pod and expect to see something similar to this:
+
----
Oct 30, 2019 2:19:52 PM com.redhat.cajun.navy.datagenerate.RestClientVerticle lambda$resetIncidents$4
INFO: Reset incidents: HTTP response status 403
----

. Create an `AuthorizationPolicy` to allow workloads inside the service mesh to access the incident service using all HTTP methods.
.. On the local file system, create a file called `incident-service-internal.yml`:
+
----
$ echo "---
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: incident-service-internal
spec:
  rules:
  - from:
    - source:
        namespaces: 
        - $ERDEMO_NS
    to:
    - operation:
        methods:
        - '*'
  selector:
    matchLabels:
      app: $ERDEMO_USER-incident-service
      group: erd-services
" > incident-service-internal.yml
----
.. Create the resource in the emergency response demo namespace:
+
----
$ oc create -f incident-service-internal.yml -n $ERDEMO_NS
----

. Confirm that the incident service can now be called from other services in the Emergency Response Demo namespace, but not from services outside the namespace:
.. From the *Disaster Simulator* home page, click *Clear Incidents*. Review the disaster simulator pod logs and expect to see something similar to this:
+
----
Oct 30, 2019 3:02:31 PM com.redhat.cajun.navy.datagenerate.RestClientVerticle lambda$resetIncidents$4
INFO: Reset incidents: HTTP response status 200
----

. Create an `AuthorizationPolicy` resource for external access to the incident service. This service role grants only HTTP GET access to the incident service REST APIs for all traffic handled by the ingress gateway.
.. On the local file system, create a file called `incident-service-external.yml`:
+
----
$ echo "---
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: incident-service-external
spec:
  rules:
  - from:
    - source:
        principals:
        - cluster.local/ns/$SM_CP_NS/sa/istio-ingressgateway-service-account
    to:
    - operation:
        methods:
        - GET
  selector:
    matchLabels:
      app: $ERDEMO_USER-incident-service
      group: erd-services
" > incident-service-external.yml
----
* The ingress gateway is specified as the SPIFFE identity of its service account.

.. Create the resource in the emergency response demo namespace:
+
----
$ oc create -f incident-service-external.yml -n $ERDEMO_NS
----

. Confirm that the incident service can now be called from outside of the cluster, but only for HTTP GET endpoints:
.. A `curl` request to the `/incidents` endpoint of the incident service returns an HTTP 200 code.
.. A `curl` POST request to the `/incidents` endpoint returns an HTTP 403 error:
+
----
$ curl -k -v -X POST -H "Content-type: application/json" -d '{"lat": 34.14338, "lon": -77.86569, "numberOfPeople": 3, "medicalNeeded": true, "victimName": "victim",  "victimPhoneNumber": "111-111-111" }' https://$INCIDENT_SERVICE_URL/incidents
----
+
.Sample output
----
$ curl -k -v -X POST -H "Content-type: application/json" -d '{"lat": 34.14338, "lon": -77.86569, "numberOfPeople": 3, "medicalNeeded": true, "victimName": "victim",  "victimPhoneNumber": "111-111-111" }' https://$INCIDENT_SERVICE_URL/incidents
Note: Unnecessary use of -X or --request, POST is already inferred.                                                  
*   Trying 35.158.48.203:443...                                                                                      
* TCP_NODELAY set                                                                                                    
* Connected to incident-service.user1.apps.cluster-18ac.18ac.example.opentlc.com (35.158.48.203) port 443 (#0)       
* ALPN, offering h2                                                                                                  
* ALPN, offering http/1.1                                                                                            
* successfully set certificate verify locations:                                                                     
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt                                                                         
  CApath: none                                                                                                       
* TLSv1.3 (OUT), TLS handshake, Client hello (1):                                                                    
* TLSv1.3 (IN), TLS handshake, Server hello (2):                                                                     
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):                                                             
* TLSv1.3 (IN), TLS handshake, Certificate (11):                                                                     
* TLSv1.3 (IN), TLS handshake, CERT verify (15):                                                                     
* TLSv1.3 (IN), TLS handshake, Finished (20):                                                                        
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):                                                          
* TLSv1.3 (OUT), TLS handshake, Finished (20):                                                                       
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384                                                              
* ALPN, server accepted to use h2                                                                                    
* Server certificate:                                                                                                
*  subject: CN=user1.apps.cluster-18ac.18ac.example.opentlc.com                                                      
*  start date: May  4 18:51:29 2020 GMT                                                                              
*  expire date: May  4 18:51:29 2022 GMT                                                                             
*  issuer: CN=user1.apps.cluster-18ac.18ac.example.opentlc.com                                                       
*  SSL certificate verify result: self signed certificate (18), continuing anyway.                                                           
* Using HTTP2, server supports multi-use        
* Connection state changed (HTTP/2 confirmed)           
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0                                                             
* Using Stream ID: 1 (easy handle 0x5601e8398200)
> POST /incidents HTTP/2                                  
> Host: incident-service.user1.apps.cluster-18ac.18ac.example.opentlc.com
> User-Agent: curl/7.66.0                                 
> Accept: */*                                             
> Content-type: application/json 
> Content-Length: 141
>                                                                                                                    
* We are completely uploaded and fine  
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):                 
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):                 
* old SSL session ID is stale, removing                               
* Connection state changed (MAX_CONCURRENT_STREAMS == 4294967295)!                                                                           
< HTTP/2 403                                                          
< content-length: 19                                                  
< content-type: text/plain                                            
< date: Tue, 05 May 2020 15:45:36 GMT                                 
< server: istio-envoy                                                 
< x-envoy-upstream-service-time: 2                                    
<                                                                     
* Connection #0 to host incident-service.user1.apps.cluster-18ac.18ac.example.opentlc.com left intact                                        
RBAC: access denied
----

== Configure Origin Authentication

Istio supports end-user authentication using JSON Web Tokens (JWT). End-user authentication verifies the original client making the request as an end user (or device). The end-user request requires passing a JWT token to be verified by the target service proxy. End-user authentication is configured in the `origins` section of a `Policy` resource.

In this exercise, you secure the ingress gateway service with JWT. You use single sign-on for Red Hat solutions (SSO) as the JWT token provider.

=== Configure SSO Instance
In this section, you configure the SSO instance to be able to obtain a JWT token.

. Log in as realm administrator to the Admin console of the SSO instance deployed as part of the Emergency Response Demo application:
.. Obtain the URL for the Admin console of the SSO instance:
+
----
$ echo -en "\n\nhttps://sso-user-sso.apps.$SUBDOMAIN_BASE/auth/admin/$ERDEMO_USER-emergency-realm/console\n\n"
----
.. In a browser window, navigate to the SSO Admin console and log in using the username and password you registered with in the Emergency Response application.
* Expect to see the administration section of your realm.
. In the left-hand panel, click *Clients* and then click *Create* to add a new client.
. Enter `curl` as the *Client ID* and click *Save*:
+
image::images/sso-client-create.png[]
. In the *Settings* tab, disable *Standard Flow* and enable *Direct Access Grants*, then click *Save*:
+
image::images/sso-client-settings.png[]

=== Obtain JWT Token
. In a terminal, use the `curl` utility to obtain a token based on your SSO username and password:
+
----
$ export RHSSO_URL=https://sso-user-sso.apps.$SUBDOMAIN_BASE
$ export REALM=$ERDEMO_USER-emergency-realm
$ export USER=<realm username>
$ export PASSWD=<realm password>
$ TKN=$(curl -X POST "$RHSSO_URL/auth/realms/$REALM/protocol/openid-connect/token" \
 -H "Content-Type: application/x-www-form-urlencoded" \
 -d "username=$USER" \
 -d "password=$PASSWD" \
 -d "grant_type=password" \
 -d "client_id=curl" \
  --insecure \
 | sed 's/.*access_token":"//g' | sed 's/".*//g')
$ echo $TKN
----
+
.Sample Output
----
eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJrLVZtbjU2SG9keWZLSjBBLTFab01ZWUxvbDJXQlNsTjNRYVBjTzFzSTNJIn0.eyJqdGkiOiJjODgwM2M1NC1kMzRjLTRlNmYtOTM1OC0zYjhmNDQyZGE2MWMiLCJleHAiOjE1NzI1NjE1MjIsIm5iZiI6MCwiaWF0IjoxNTcyNTYxMjIyLCJpc3MiOiJodHRwczovL3Nzby11c2VyLXNzby5hcHBzLmNsdXN0ZXItYTNjNi5hM2M2LmV4YW1wbGUub3BlbnRsYy5jb20vYXV0aC9yZWFsbXMvZW1lcmdlbmN5LXJlYWxtIiwiYXVkIjoiYWNjb3VudCIsInN1YiI6ImVlNjBkNTBhLTllNGEtNDNjZi1iOWM2LWFlYzQwYjM3OWU4ZCIsInR5cCI6IkJlYXJlciIsImF6cCI6ImN1cmwiLCJhdXRoX3RpbWUiOjAsInNlc3Npb25fc3RhdGUiOiI4MDkxNTA5MC1jNzE1LTQxZDQtYTc1Mi02NjFmNmFhYjJlODQiLCJhY3IiOiIxIiwicmVhbG1fYWNjZXNzIjp7InJvbGVzIjpbIm9mZmxpbmVfYWNjZXNzIiwidW1hX2F1dGhvcml6YXRpb24iXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJzY29wZSI6ImVtYWlsIHByb2ZpbGUiLCJlbWFpbF92ZXJpZmllZCI6ZmFsc2UsIm5hbWUiOiJCZXJuYXJkIFRpc29uIiwicHJlZmVycmVkX3VzZXJuYW1lIjoiYnRpc29uIiwiZ2l2ZW5fbmFtZSI6IkJlcm5hcmQiLCJmYW1pbHlfbmFtZSI6IlRpc29uIiwiZW1haWwiOiJidGlzb25AcmVkaGF0LmNvbSJ9.dUQZM_tEcmyw4MMzkv4h6pV7yFwEyPtVkkXIbNYNgSrHU5-1sfIZlTjgtj22cOkTV6Eg6-3TN_xgduvJXJ7dcgyz-rQJ0RiYtt_U8JPl2lvAP1krFepZQzUoTZ1-Yk1ybnRamvsu3IS4gOr2e5c6H1Levy-h3ifirkO0MoAWS96tYHAYLMDedZGgxp9eNqrcF1XoyK9e-YFLufrdhIE2NyAi32mFhsGNKJMyy9Lb1DLMLuaiu6y3wVsKn9JROv_yyo9cN3y8DezDdd8KZ710m7kBCNdDU0UNq735L7aSuWzwTctiAOCaO-E5I_HH4UBWh3I2Vh2lZnBlTopA_hD7tQ
----
. Open a web browser, navigate to `https://jwt.io`, and paste the contents of the `$TKN` environment variable into the left-hand debugger window to view the actual contents of the JWT:
+
image::images/jwt-token.png[]

=== Create Policy
In this section, you create a `Policy` resource to configure end-user authentication for the Istio ingress gateway service.

. On the local file system, create a file called `ingressgateway-origin.yml`:
+
----
$ echo "---
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: ingressgateway-origin
spec:
  targets:
    - name: istio-ingressgateway
  origins:
    - jwt:
        issuer: https://sso-user-sso.apps.$SUBDOMAIN_BASE/auth/realms/$ERDEMO_USER-emergency-realm
        jwksUri: https://sso-user-sso.apps.$SUBDOMAIN_BASE/auth/realms/$ERDEMO_USER-emergency-realm/protocol/openid-connect/certs
  principalBinding: USE_ORIGIN
" > ingressgateway-origin.yml
----

.. Make sure that the value of the `issuer` element corresponds to the value of the `iss` field in the token.
.. Make sure that the value of the `jwksUri` element corresponds to the `issuer` URL followed by `/protocol/openid-connect/certs`.
. Create the `Policy` resource in the control plane namespace:
+
----
$ oc create -f ingressgateway-origin.yml -n $SM_CP_NS
----

=== Test
With the policy in place, all requests through the ingress gateway require a valid token.

. Try to access the `/incidents` endpoint of the incident service:
+
----
$ INCIDENT_SERVICE_URL=$(oc get route incident-service-gateway -n $SM_CP_NS -o template --template={{.spec.host}})
$ curl -v -k https://$INCIDENT_SERVICE_URL/incidents
----
+
.Sample Output
----
*   Trying 35.158.14.219:443...
* TCP_NODELAY set
* Connected to incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com (35.158.14.219) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20):
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=*.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
*  start date: Oct 25 15:12:23 2019 GMT
*  expire date: Jan 23 15:12:23 2020 GMT
*  issuer: C=US; O=Let's Encrypt; CN=Let's Encrypt Authority X3
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x561b88ce3a40)
> GET /incidents HTTP/2
> Host: incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
> User-Agent: curl/7.65.3
> Accept: */*
>
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
* Connection state changed (MAX_CONCURRENT_STREAMS == 4294967295)!
< HTTP/2 401
< content-length: 29
< content-type: text/plain
< date: Thu, 31 Oct 2019 22:58:37 GMT
< server: istio-envoy
<
* Connection #0 to host incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com left intact
Origin authentication failed.
----
* Expect to see an HTTP return code of 401, with the message "Origin authentication failed."
. Obtain a new token, and add the token as an `Authorization` header to the request:
+
----
$ TKN=$(curl -X POST "$RHSSO_URL/auth/realms/$REALM/protocol/openid-connect/token" \
 -H "Content-Type: application/x-www-form-urlencoded" \
 -d "username=$USER" \
 -d "password=$PASSWD" \
 -d "grant_type=password" \
 -d "client_id=curl" \
  --insecure \
 | sed 's/.*access_token":"//g' | sed 's/".*//g')
$ curl -v -k -H "Authorization: Bearer $TKN" https://$INCIDENT_SERVICE_URL/incidents
----
+
.Sample Output
----
*   Trying 35.156.140.248:443...
* TCP_NODELAY set
* Connected to incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com (35.156.140.248) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20):
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=*.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
*  start date: Oct 25 15:12:23 2019 GMT
*  expire date: Jan 23 15:12:23 2020 GMT
*  issuer: C=US; O=Let's Encrypt; CN=Let's Encrypt Authority X3
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x558e40afda40)
> GET /incidents HTTP/2
> Host: incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
> User-Agent: curl/7.65.3
> Accept: */*
> Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJrLVZtbjU2SG9keWZLSjBBLTFab01ZWUxvbDJXQlNsTjNRYVBjTzFzSTNJIn0.eyJqdGkiOiI2Y2IwYjlmMC1jMzFiLTQyOTQtYWJkMi1hZWVlMTEwNWQ2ZDEiLCJleHAiOjE1NzI1NjM0NTQsIm5iZiI6MCwiaWF0IjoxNTcyNTYzMTU0LCJpc3MiOiJodHRwczovL3Nzby11c2
VyLXNzby5hcHBzLmNsdXN0ZXItYTNjNi5hM2M2LmV4YW1wbGUub3BlbnRsYy5jb20vYXV0aC9yZWFsbXMvZW1lcmdlbmN5LXJlYWxtIiwiYXVkIjoiYWNjb3VudCIsInN1YiI6ImVlNjBkNTBhLTllNGEtNDNjZi1iOWM2LWFlYzQwYjM3OWU4ZCIsInR5cCI6IkJlYXJlciIsImF6cCI6ImN1cmwiLCJhdXRoX3RpbWUiOjAsInNlc3Npb25fc3RhdGUiOiJhY2MxMzQyOC0yOGFi
LTRiOWItYWEyNi02OWNlY2UxNmFiNWQiLCJhY3IiOiIxIiwicmVhbG1fYWNjZXNzIjp7InJvbGVzIjpbIm9mZmxpbmVfYWNjZXNzIiwidW1hX2F1dGhvcml6YXRpb24iXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJzY29wZSI6ImVtYWlsIH
Byb2ZpbGUiLCJlbWFpbF92ZXJpZmllZCI6ZmFsc2UsIm5hbWUiOiJCZXJuYXJkIFRpc29uIiwicHJlZmVycmVkX3VzZXJuYW1lIjoiYnRpc29uIiwiZ2l2ZW5fbmFtZSI6IkJlcm5hcmQiLCJmYW1pbHlfbmFtZSI6IlRpc29uIiwiZW1haWwiOiJidGlzb25AcmVkaGF0LmNvbSJ9.NvT75pNsT3PdQMbYFtbsPeNx9_bdZwDYdfBOO4BU5GPvgB6z3RTIBfKJCnGpBZmpZw46PgJ
8eLljOZ3eDmZzPGStSMSBvgDndNv7AhdscmXVisGGR-WCLzDYtkYrZXjGVLIzhpLP_lTfapFflXlQWNZk1DkPsY_xuJ7dIX-vMQPurn0fyBhFcNtl0F6uHd7s02bCMqsX5EoOa0mV5lkI-RIqNjb94h12lBRVza9Go7aQ7q6NWP5LRr3RkKmdaLtz4E--L3pK4fVGCuhFAJfmQk7LndIQESs4DZvdtYYvQuT6RQEaLvm_0dh77xsKkIuPI7jXCugyAl_8ODeXEqsQcg
>
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
* Connection state changed (MAX_CONCURRENT_STREAMS == 4294967295)!
< HTTP/2 200
< content-type: application/json;charset=UTF-8
< date: Thu, 31 Oct 2019 23:06:09 GMT
< x-envoy-upstream-service-time: 17
< server: istio-envoy
<
* Connection #0 to host incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com left intact
[]
----
* Expect the request to succeed with an HTTP return code of 200.
+
[NOTE]
====
* All external traffic to the Emergency Response Demo application goes through the ingress gateway, and all requests require a valid JWT token. This means that the disaster simulator UI and the emergency console will not work as expected anymore. To fix this would require a separate ingress gateway for these services that does not require origin authentication.

* The service mesh policy only checks for a valid token based on the issuer, the validity of the signature on the token, and the token expiry time. It is possible to configure more fine-grained authorization based on groups or roles, however, the default way SSO encodes role information in a JWT token is incompatible with how the service mesh expects this information. 

* The service mesh only propagates the JWT token one hop. The service proxy takes the body of the JWT token and sends it along to the application in a separate HTTP header. The JWT body is sent in the `sec-istio-auth-userinfo` header. It is the responsibility of the application to propagate the token to downstream services or resubmit for a new token based on the end user’s identity and the service’s identity.
====

. Before continuing with the remainder of the exercises, delete the end-user authentication policy:
+
----
$ oc delete policy ingressgateway-origin -n $SM_CP_NS
----

== Configure Citadel with External CA Certificates and Keys

By default, Citadel uses self-signed certificates for mTLS transport security. Citadel can also be configured to use a certificate and key from an external Certificate Authority (CA). In this exercise, you create your own CA and configure Citadel to use intermediate certificates issued and signed by the external CA.

. Create a CA root certificate and key:
.. Set the following environment variables:
+
----
$ export ROOTCA_ORG=Istio
$ export ROOTCA_CN="Root CA"
$ export ROOTCA_DAYS=3650
----
.. Create an `openssl` configuration file:
+
----
$ echo "
[ req ]
encrypt_key = no
prompt = no
utf8 = yes
default_md = sha256
default_bits = 4096
req_extensions = req_ext
x509_extensions = req_ext
distinguished_name = req_dn
[ req_ext ]
subjectKeyIdentifier = hash
basicConstraints = critical, CA:true
keyUsage = critical, digitalSignature, nonRepudiation, keyEncipherment, keyCertSign
[ req_dn ]
O = ${ROOTCA_ORG}
CN = ${ROOTCA_CN}
" > root-ca.conf
----
.. Generate a private key for the root CA:
+
----
$ openssl genrsa -out root-ca.key 4096
----
+
.Sample Output
----
Generating RSA private key, 4096 bit long modulus (2 primes)
...................................................................................................................
...................................................................................................................
.....++++
............++++
e is 65537 (0x010001)
----
.. Create a certificate signing request for the root CA:
+
----
$ openssl req -new -key root-ca.key -config root-ca.conf -out root-ca.csr
----
.. Generate the root CA certificate:
+
----
$ openssl x509 -req -days ${ROOTCA_DAYS} -signkey root-ca.key -extensions req_ext -extfile root-ca.conf -in root-ca.csr -out root-ca.crt
----

. Create an intermediate certificate for Citadel:
.. Create a directory for the certificate and set the following environment variables:
+
----
$ mkdir $SM_CP_ADMIN
$ export CITADEL_ORG=Istio
$ export CITADEL_CN="Intermediate CA"
$ export CITADEL_DAYS=365
$ export CITADEL_SERIAL=$RANDOM
----
.. Create an `openssl` configuration file for the intermediate certificate:
+
----
echo "
[ req ]
encrypt_key = no
prompt = no
utf8 = yes
default_md = sha256
default_bits = 4096
req_extensions = req_ext
x509_extensions = req_ext
distinguished_name = req_dn
[ req_ext ]
subjectKeyIdentifier = hash
basicConstraints = critical, CA:true, pathlen:0
keyUsage = critical, digitalSignature, nonRepudiation, keyEncipherment, keyCertSign
subjectAltName=@san
[ san ]
URI.1 = spiffe://cluster.local/ns/${SM_CP_NS}/sa/citadel
DNS.1 = localhost
[ req_dn ]
O = ${CITADEL_ORG}
CN = ${CITADEL_CN}
L = cluster.local
" > $SM_CP_ADMIN/intermediate.conf
----
.. Generate a private key for the intermediate certificate:
+
----
$ openssl genrsa -out $SM_CP_ADMIN/citadel-ca.key 4096
----
.. Generate a certificate signing request for the intermediate certificate:
+
----
$ openssl req -new -config $SM_CP_ADMIN/intermediate.conf -key $SM_CP_ADMIN/citadel-ca.key -out $SM_CP_ADMIN/citadel-ca.csr
----
.. Generate the intermediate certificate:
+
----
$ openssl x509 -req -days $CITADEL_DAYS -CA root-ca.crt -CAkey root-ca.key -set_serial $CITADEL_SERIAL -extensions req_ext -extfile $SM_CP_ADMIN/intermediate.conf -in $SM_CP_ADMIN/citadel-ca.csr -out $SM_CP_ADMIN/citadel-ca.crt
----
.. Create a certificate chain containing the intermediate certificate and the root certificate:
+
----
$ cat $SM_CP_ADMIN/citadel-ca.crt root-ca.crt > $SM_CP_ADMIN/citadel-ca-chain.crt
----

. Create a secret for the CA certificates in the service mesh control plane. The secret must be named `cacerts`. The secret contains the intermediate certificate and key, the root certificate, and the certificate chain. Note the specific names used for the certificates and keys within the secret:
+
----
$ oc create secret generic cacerts -n $SM_CP_NS --from-file=ca-cert.pem=$SM_CP_ADMIN/citadel-ca.crt --from-file=ca-key.pem=$SM_CP_ADMIN/citadel-ca.key --from-file=root-cert.pem=root-ca.crt --from-file=cert-chain.pem=$SM_CP_ADMIN/citadel-ca-chain.crt
----
. Configure Citadel to use the CA certificates rather than self-signed certificates by setting the `.spec.istio.security.selfSigned` element in the `ServiceMeshControlPlane` CR to `false`:
+
----
$ oc edit servicemeshcontrolplane.v1.maistra.io full-install -n $SM_CP_NS
----

.. Add the `.spec.istio.security.selfSigned` element to the CR:
+
----
apiVersion: maistra.io/v1
kind: ServiceMeshControlPlane
metadata:
  [...]
spec:
  istio:
    [...]
    pilot:
      autoscaleEnabled: false
      traceSampling: 100
    security:
      selfSigned: false
    tracing:
      enabled: true
  threeScale:
    enabled: false
----

.. Save the CR.

. As a result of modifying the `ServiceMeshControlPlane` CR, the Citadel pod in the control plane is redeployed. The `cacerts` secret is mounted in the Citadel pod. Verify that the Citadel deployment now includes the `cacerts` secret and references the CA certificates and key in the startup arguments of the pod:
+
----
$ oc get deployment istio-citadel -o yaml -n $SM_CP_NS
----
+
.Sample Output
----
kind: Deployment
apiVersion: apps/v1
metadata:
  [...]
spec:
  [...]
  template:
    metadata:
      [...]
    spec:
      [...]
      containers:
        - [...]
          volumeMounts:
            - name: cacerts
              readOnly: true
              mountPath: /etc/cacerts
          [...]
          args:
            - '--append-dns-names=true'
            - '--grpc-port=8060'
            - '--grpc-hostname=citadel'
            - '--citadel-storage-namespace=admin1-istio-system'
            - >-
              --custom-dns-names=istio-pilot-service-account.admin1-istio-system:istio-pilot.admin1-istio-system
            - '--monitoring-port=15014'
            - '--self-signed-ca=false'
            - '--signing-cert=/etc/cacerts/ca-cert.pem'
            - '--signing-key=/etc/cacerts/ca-key.pem'
            - '--root-cert=/etc/cacerts/root-cert.pem'
            - '--cert-chain=/etc/cacerts/cert-chain.pem'
            - '--member-roll-name=default'
      [...]
      volumes:
        - name: cacerts
          secret:
            secretName: cacerts
            defaultMode: 420
            optional: true
      [...]
----

. Delete all of the secrets created by Citadel in the service mesh data plane. This forces Citadel to re-create the secrets using the external CA certificates:
+
----
$ oc get secret -n $ERDEMO_NS | grep "istio." | awk '{print $1}' | xargs -I istio-secret oc delete secret istio-secret -n $ERDEMO_NS
----
+
.Sample Output
----
secret "istio.builder" deleted
secret "istio.default" deleted
secret "istio.deployer" deleted
secret "istio.disaster-simulator-service" deleted
secret "istio.emergency-console" deleted
secret "istio.incident-priority-service" deleted
secret "istio.incident-service" deleted
secret "istio.kafka-cluster-entity-operator" deleted
secret "istio.kafka-cluster-kafka" deleted
secret "istio.kafka-cluster-zookeeper" deleted
secret "istio.mission-service" deleted
secret "istio.postgresql" deleted
secret "istio.process-service" deleted
secret "istio.process-viewer" deleted
secret "istio.responder-client-app" deleted
secret "istio.responder-service" deleted
secret "istio.responder-simulator-service" deleted
----

. View the contents of the certificate chain in the `istio.incident-service` secret and note that the certificate chain contains three certificates:
+
----
$ oc get secret istio.incident-service -o jsonpath={.data.cert-chain\\.pem} -n $ERDEMO_NS | base64 --decode
----
+
.Sample Output
----
-----BEGIN CERTIFICATE-----
MIIEdjCCAl6gAwIBAgIQG86HyV61A25wA4z6fmUceDANBgkqhkiG9w0BAQsFADBC
MQ4wDAYDVQQKDAVJc3RpbzEYMBYGA1UEAwwPSW50ZXJtZWRpYXRlIENBMRYwFAYD
VQQHDA1jbHVzdGVyLmxvY2FsMB4XDTIwMDIwOTA5NTU1NVoXDTIwMDUwOTA5NTU1
NVowADCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKHQKOTArOM+Am8f
Pqmv5n73GHsnrfF3AYeBIGSTBTQ2UkaGk9iENFZSkiTM5XXhmdQID9eJ5Qm4irgy
TefLYQRx+82lqTENg1DEzjL9tA4rUSAdfKyTFoF477XkN2uVTxwGxEKwF62ZmOvK
pCVZHi+JHBmIqjrnnNXzR1nwjp3kduGsquwV5ZbSmjIArZ8co2VJyyCPmYkH1lri
F5UQou5sNxIaq67ZePj9Zz14RjrSqxr4EayafLrtch9gVatRl5Fj9gCHV81S5p0H
Q0m94slJZx+hyRjLhIUWcgkUjCDMa+wmSTjdZAeHBPK2rCE8ebQkb3qcXLOnt/c0
s+Xi9MUCAwEAAaOBqTCBpjAOBgNVHQ8BAf8EBAMCBaAwHQYDVR0lBBYwFAYIKwYB
BQUHAwEGCCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAUd4Fzy4u4
6BK1at1GyGKV6+g4zzAwRgYDVR0RBD8wPYY7c3BpZmZlOi8vY2x1c3Rlci5sb2Nh
bC9ucy91c2VyMS1lci1kZW1vL3NhL2luY2lkZW50LXNlcnZpY2UwDQYJKoZIhvcN
AQELBQADggIBABqrOsEcIB9kgbEsx0xEKAp8d/EQaExMruCk21VOk2JulGAt9cub
boA8/GoT81wRGhtsZLn0HnP68S/wshrlehG4in9y2PspAyXF8/hq3xFBj7jwOQnf
hFJfvjgcQI5IenThqJeLAy40qrg8NJL8yjSjo4yKY58E/0/4c8F9YpBdVsL189/p
fkDHBFF9wvWixZUVMKLXjzCj7+uoKKZq17OeyLArrITO/PcUyVP88xI1nvKD+5Gv
Ep10goeeZCk5V2GDjMO37s5Xzw01vQZ5eohg9EymWI0jcCblRjHxq8p1tsB7J9zM
bEAuTCqetiaDxOMmX+LErWZfRLSh/S6N8Rha+YhtVQXU0DhvyT5thxwe62xrviDb
3GWmOMf3xhW5PjIGqBMCCUXFKYT1IygwFzksrNZm7GwpKjejCT6r6h0/zWcJi+zU
o7YdYC5lopcw7JKrswlK7O5tS2cVhq87YKp0eWzSqLFoorV9ywm/C8B1Mip8Uzyd
7VnmZLtU0mx1vqD7Ji6yCxFX5uEbs5e2+GyJr4VKZ5P9mTWJeQjhnK8rMnsvJscd
TogDY/LEMz1VjkfUbwCxmxGp30wQbb4fEvCbbiRn/JKvKVwRYzRhZxHA5au8GZAu
rKndtwyvIVKOPWPZzMQUpipn5nE1pH/Tl7H0E4twdGB0CHoDqf1CW+Dm
-----END CERTIFICATE-----
-----BEGIN CERTIFICATE-----
MIIFdzCCA1+gAwIBAgICNCIwDQYJKoZIhvcNAQELBQAwIjEOMAwGA1UECgwFSXN0
aW8xEDAOBgNVBAMMB1Jvb3QgQ0EwHhcNMjAwMjA5MDkxMDE2WhcNMjEwMjA4MDkx
MDE2WjBCMQ4wDAYDVQQKDAVJc3RpbzEYMBYGA1UEAwwPSW50ZXJtZWRpYXRlIENB
MRYwFAYDVQQHDA1jbHVzdGVyLmxvY2FsMIICIjANBgkqhkiG9w0BAQEFAAOCAg8A
MIICCgKCAgEA7X8da7kNMU3p/XEcVhgSoggBg8MkTvk2B3KF5ZvV5z2ibRkx47Kk
F8Hm2ghfObERrXaWLdTooIWs/V24y9DKLqymqVTPzU9y5kei3QEK2EvV7QiXHIxF
DxOxWn6LeH2XGAUCwoKe8cZcDd+GZFaG4PHgaA6SfIYIssDXAEaDHmeHSfVR1xs4
RgEdJQu9EfOOgXONRMIN2DSElVyUF09TpQgZbBkh8+d+6R65nMBTZhO3t77xBtDF
rMojoUM1m3KF8xdYZglm2wZ7FD81Jj68J34pKDmeMEb5I7qYJV/vMXNDSxah/4GS
x6JQBx8tqh5qlI8XULCG5q5gY26/HLSuyoa3SOyMPJ2hiUJ+zu7cI2MhLdkYLsxp
m7H0w7iqOE9rKxRtFl5b1pVCaFgktGc/kK30DyL+GE7fKRQ4CQiwmc1JULGHr9Bh
CT9W44OB8YOenI95jAir1fPaE9/Skhd7daDogp1i0WTTZ7QYJFuyoPCL97qyYLnA
NfzWiA8MQZ7LcVtFVCqEebzjHZwPxfWheu+DceZrd0DEWYISjfZWUpRyA87tvcVt
kfnvv0QLEZEzZ6hjNYl98w8JdVYo8QqLbCDdovZcPHfwcVaFgOxTLCjoowA7KyKF
v9phmH2mDZuVrMRPWsGYn3m/GoPIPYugpqLMpnm2ULxK2tp2LX7hWjUCAwEAAaOB
ljCBkzAdBgNVHQ4EFgQUd4Fzy4u46BK1at1GyGKV6+g4zzAwEgYDVR0TAQH/BAgw
BgEB/wIBADAOBgNVHQ8BAf8EBAMCAuQwTgYDVR0RBEcwRYY4c3BpZmZlOi8vY2x1
c3Rlci5sb2NhbC9ucy9hZG1pbjEtaXN0aW8tc3lzdGVtL3NhL2NpdGFkZWyCCWxv
Y2FsaG9zdDANBgkqhkiG9w0BAQsFAAOCAgEA37EZr5BZYCA0KcYd9d0bFzvgle5m
Fjdvnl2HcBsJkXQm0mdOJBQDDbreaRJ+37EQVJYzsR4zRBAoICfy1kkkSH8xef5B
qg6/pGMGDeDFpl7bVfhxniPlutgl2tr6XTYPQfzVCvcHuZa62fdOqW5EMx5ju1vh
hLYTEIES4H/h6BoCgJEtDTthG1eraspCxg7anzIog5NYRMFvhArrwsCjrlTl4LMe
CNaBR2UNiClVTtRvzcOjwCMzoIFzTCaHg9XnGyDRRtjSMyJU6m7V1wxePhbwjqt6
/ObhCY1npsaoMQDtGIlqL798ud8oM2rd2BDVKlOnN30/C7f5V07mnl6tPtdjtcAj
UYJJb3BARn1YKsc/Dl48Y9tKGgh5taOxfTLAFbBhV51fXwdP0rFEQk1pSUVxJjaz
1TzDpM7sXVSRWySLnn7cbXZMMjl9jFDflaiOJw89us/YQro3QUNr5gUL28R7I7Ot
20Owq0aBemtFwsVPELN/rC2PxInBtCbaX/xKCvxE/dYLRQltlILtXYA5UdBb8XP3
lV7DZzYVeMLsNVpoBGanxAeu87xs0ixLU8jeNpsr8tFuOgrtGoaCP7q3HJ8neVb8
PxyTK4ian1qJnOLEuPB99/paUd8xWkKcaBW3ZQqHbTSaHccbOHWQU0e/TLbbrO6q
ZWhWOwjsLJXXhR4=
-----END CERTIFICATE-----
-----BEGIN CERTIFICATE-----
MIIFFDCCAvygAwIBAgIUCh89enz4qJiAzNs6MwukFhuLOawwDQYJKoZIhvcNAQEL
BQAwIjEOMAwGA1UECgwFSXN0aW8xEDAOBgNVBAMMB1Jvb3QgQ0EwHhcNMjAwMjA5
MDc1NTAxWhcNMzAwMjA2MDc1NTAxWjAiMQ4wDAYDVQQKDAVJc3RpbzEQMA4GA1UE
AwwHUm9vdCBDQTCCAiIwDQYJKoZIhvcNAQEBBQADggIPADCCAgoCggIBAOYZH5zb
FLN4hYFcSFYmcTAin8xHmg9btBc8GL+G6CPwH8O/ove5T+q41pmhsKdYGunKXMQv
sH1ZCJW2ugvVmHnV+62gmaD2HpdE/T6Do9BljRSWPyRwAuHanbCws421NkHTKFqz
Mim1o5fpnZkKxxoYjRhvlt/4hdHuOExrZmoOMAARjBGZlpY4nuE9mY9HvQim0CuP
0b+9A+oVoL2jK/QxKbUze+5HygFAI7SWsKajg1IM4P74/SJEt5ndYXyO1zDaObZe
AEz2qqoNwNs+WfHWnSAVdiKabWGEdOt6FcgpnDMN6ru7qFcdtnTAobPjjS3irRnU
5x6qxD0WidWsHaj+CTb44PTpOmkfusilkWyiDqeNmbI3ElGT+YMRqNuGpqf0Jd9K
DsW7v6QS/FRTW7LHwUx/CKR0Lqzoa23fELbkdgOBngWoGRz09FcIHUcKxmVX+g2e
6TH1mZi/QB968PVuuoBNogIW6HoMG1o1vDDqEZ/DfkqVOVy0jg5SWqG2npXxb+4h
UwdSf1JmxR+0VCA4C6aTduKZo+Kn7u3aa/omjnidMoC1hQwl8UOskFde5ZiToOEz
0tIiHFvXXUrqChdkMUM732ZQ33VjtjmbqTZb68kIqTRleH0JG58gKDgT7QNYPZJp
FlgiBz/Z0YiyWExbXlw2DrnwBLTnQklx8fhdAgMBAAGjQjBAMB0GA1UdDgQWBBRw
jOUwD4FkI8BkhgboB3PVCvYGkzAPBgNVHRMBAf8EBTADAQH/MA4GA1UdDwEB/wQE
AwIC5DANBgkqhkiG9w0BAQsFAAOCAgEAi5su11TX4bYcTEPMZy1mqckTEZeDBEE4
hFvlYBYeVztgvR9pzRlyawdKX0OtJ5hLJ4xjgyTXpKT2tsepas3AHQ+NfoKrUAQ/
Qt9oqj4z+Q5I+sHXs2Jo6blyJwLh9mUUSEhim5oIPBOAjqcS3zpkpZ8zApwPB34P
iiMYCzvJJOIOa2bgV600mvflDSKK2/nUxWmbzSKQcankxijhjmOJnDjS/9duZFrx
A6II5P5uZQyUOJ3p84dGlXh06shzszFs1sg0Wq7fs61Pj7zKK6rOknDpTV3pKdHu
fpSeORAf0Gv6HcdmgActbHHeBC3yV9EgoixqZSCV+KLZLZGab2DwTOSOOQZth3R2
qCXSDvbM1UI/qGwR+zwM7tCxTsaOWVblf6Nyn3bifXgm3wbWrsdIKYYoYn9TFOzF
NeC2G3KI/LyKrvryzm/YRiQFUgFvbXl0J9SyegBzhJJCu5eNZYteaKb8pN7ZsP5t
s8LbXg87fIyR4rT8WLqCu5LbrWsxkdFOV2LPQOXZ4saD/PWpiin5hABg90zInf7e
B5My/7w/QLJVLb9/oNYf6aCaunNJbfk0CKALGZatwppjcIyEF0ravVrfRajr7wgE
U4Cj+LEL3RcJb/s2OGk6nDPEa2LbrpDx/T6/TOIY6PjK69bQRT7Ym3QVM20a/bOl
zRPoVL83I3Q=
-----END CERTIFICATE-----
----
. Copy the text output of the first certificate.
. In a browser window, navigate to `https://www.sslshopper.com/certificate-decoder.html`, and paste the certificate text into the text box:
+
image::images/istio-citadel-ca-certificate.png[]
+
* Note the issuer of the certificate, which corresponds to the intermediate and root CAs.

. Restart the application services in the data plane to have the Envoy proxies mount the new secrets:
+
----
$ for dc in disaster-simulator emergency-console incident-priority-service incident-service mission-service process-service process-viewer responder-service responder-simulator
do
  oc rollout latest dc/$ERDEMO_USER-$dc -n $ERDEMO_NS
done
----
+
.Sample Output
----
deploymentconfig.apps.openshift.io/user1-disaster-simulator rolled out
deploymentconfig.apps.openshift.io/user1-emergency-console rolled out
deploymentconfig.apps.openshift.io/user1-incident-priority-service rolled out
deploymentconfig.apps.openshift.io/user1-incident-service rolled out
deploymentconfig.apps.openshift.io/user1-mission-service rolled out
deploymentconfig.apps.openshift.io/user1-process-service rolled out
deploymentconfig.apps.openshift.io/user1-process-viewer rolled out
deploymentconfig.apps.openshift.io/user1-responder-service rolled out
deploymentconfig.apps.openshift.io/user1-responder-simulator rolled out
----
+
[NOTE]
If you injected the PostgreSQL pods with the Envoy proxy, make sure to restart them too.

. Test the Emergency Response Demo application to make sure the application is working as expected.

This concludes the lab. You have successfully enabled mTLS and authorization policies, configured service mesh origin authentication, and configured Citadel to use certificates issued by an external CA.