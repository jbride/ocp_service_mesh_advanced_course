:noaudio:
:scrollbar:
:toc2:
:linkattrs:
:data-uri:

== Istio Security: mTLS and Authorization Policies Lab

.Prerequisites
* Access to the Red Hat^(R)^ OpenShift^(R)^ Container Platform 4.x cluster
* Istio control plane installed and ready
* Emergency Response Demo application installed

.Goals
* Enable transport security (mTLS) for inter-service communication in Emergency Response Demo
* Set authorization policies for inter-service communication in Emergency Response Demo
* Configure service mesh origin authentication
* Configure Citadel to use certificates issued by an external CA

:numbered:

== Review OpenShift Service Mesh Control Plane Installation

Before starting the lab, you review the Red Hat OpenShift Service Mesh installation.

. Make sure you are logged in to OpenShift Container Platform as the control plane admin user:
+
----
$ oc login $LAB_MASTER_API -u $SM_CP_ADMIN -p $OCP_PASSWD
----
. The service mesh control plane is installed in the `$SM_CP_ADMIN-istio-system` namespace and defined by the `ServiceMeshControlPlane` custom resource. View the custom resource:
+
----
$ oc get servicemeshcontrolplane -n $SM_CP_NS
----
+
.Sample Output
----
NAME           READY   STATUS            PROFILES      VERSION   AGE
full-install   9/9     ComponentsReady   ["default"]   2.0.2   43h
----

. Review the `ServiceMeshControlPlane` custom resource:
+
----
$ oc get servicemeshcontrolplane full-install -o yaml -n $SM_CP_NS
----
+
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"maistra.io/v2","kind":"ServiceMeshControlPlane","metadata":{"annotations":{},"name":"full-install","namespace":"admin1-istio-system"},"spec":{"addons":{"3scale":{"enabled":false},"kiali":{}},"gateways":{"egress":{"runtime":{"deployment":{"autoScaling":{"enabled":false}}},"service":{}},"ingress":{"runtime":{"deployment":{"autoScaling":{"enabled":false}}},"service":{}},"openshiftRoute":{"enabled":false}},"policy":{"mixer":{"enableChecks":false}},"profiles":["default"],"proxy":{"runtime":{"container":{"resources":{"limits":{"cpu":"500m","memory":"128Mi"},"requests":{"cpu":"100m","memory":"128Mi"}}}}},"runtime":{"components":{"mixer.policy":{"deployment":{"autoScaling":{"enabled":false}}},"mixer.telemetry":{"container":{"resources":{"limits":{"cpu":"500m","memory":"4G"},"requests":{"cpu":"100m","memory":"1G"}}},"deployment":{"autoScaling":{"enabled":false}}},"pilot":{"deployment":{"autoScaling":{"enabled":false}}}}},"security":{"dataPlane":{"automtls":false,"mtls":false}},"techPreview":{"kiali":{"dashboard":{"passphrase":"redhat","user":"admin"}}},"tracing":{"sampling":10000,"type":"Jaeger"}}}
  creationTimestamp: "2020-11-23T15:15:07Z"
  finalizers:
  - maistra.io/istio-operator
  generation: 1
  managedFields:
    [...]
  name: full-install
  namespace: admin1-istio-system
  resourceVersion: "926633"
  selfLink: /apis/maistra.io/v2/namespaces/admin1-istio-system/servicemeshcontrolplanes/full-install
  uid: 18fdfc5f-f6df-4148-b95e-590534a7d987
spec:
  addons:
    3scale:
      enabled: false
    kiali: {}
  gateways:
    egress:
      runtime:
        deployment:
          autoScaling:
            enabled: false
      service: {}
    ingress:
      runtime:
        deployment:
          autoScaling:
            enabled: false
      service: {}
    openshiftRoute:
      enabled: false
  policy:
    mixer:
      enableChecks: false
  profiles:
  - default
  proxy:
    runtime:
      container:
        resources:
          limits:
            cpu: 500m
            memory: 128Mi
          requests:
            cpu: 100m
            memory: 128Mi
  runtime:
    components:
      mixer.policy:
        deployment:
          autoScaling:
            enabled: false
      mixer.telemetry:
        container:
          resources:
            limits:
              cpu: 500m
              memory: 4G
            requests:
              cpu: 100m
              memory: 1G
        deployment:
          autoScaling:
            enabled: false
      pilot:
        deployment:
          autoScaling:
            enabled: false
  security:
    dataPlane:
      automtls: false
      mtls: false
  techPreview:
    kiali:
      dashboard:
        passphrase: redhat
        user: admin
  tracing:
    sampling: 10000
    type: Jaeger
  version: v2.0
status:
  [...]
----
+
* Note that `security.dataPlane.mtls` is set to `false`. This means that mTLS is not enforced, and services are able to communicate over plain HTTP.
* Setting `mTLS` as globally disabled is the recommended starting point when migrating an existing set of applications to use Istio and mTLS.
* `security.dataPlane.automtls` is also set to `false`. When enabled, the automatic mTLS feature allows you to adopt mutual TLS without creating any Destination Rule. +
With the default mesh-wide PERMISSIVE mTLS setting, the service mesh automatically configures client sidecar proxies to send mutual TLS traffic to workloads with Envoy sidecars, and to send plain text traffic to workloads without them. This process is fully automated, and there is no need to define Destination Rules. +
However, it turns out that automatic mTLS interferes with the ability of the pods in the data plane to connect to the Kafka broker pods. This is a known issue, which has been fixed in upstream Istio.

. As part of the OpenShift Service Mesh installation, a global `PeerAuthentication` policy is created in the service mesh control plane namespace.
The scope of `PeerAuthentication` is the service mesh. It can be overridden by namespace-scoped and service-scoped policies. Review the policy:
+
----
$ oc get peerauthentication default -o yaml -n $SM_CP_NS
----
+
.Sample Output
----
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  annotations:
    [...]
  creationTimestamp: "2020-11-23T15:19:06Z"
  generation: 1
  labels:
    [...]
  managedFields:
    [...]
  name: default
  namespace: admin1-istio-system
  ownerReferences:
  - [...]
  resourceVersion: "127344"
  selfLink: /apis/security.istio.io/v1beta1/namespaces/admin1-istio-system/peerauthentications/default
  uid: 090e90d4-c78b-4587-a7b2-8aa7ad6524e3
spec:
  mtls:
    mode: PERMISSIVE
----

* The global policy for mTLS is set to `PERMISSIVE`, which means that the proxy sidecar accepts non-mTLS encrypted traffic.

=== Review Certificates

In OpenShift Service Mesh 2.0, certificates used for mTLS communication between services in the cluster are provided to the Envoy sidecars through the _Secret Discovery Service_. This mechanism uses the following flow:

* istiod offers a gRPC service to take certificate signing requests (CSRs).
* The Envoy proxy sends a certificate and key request via the Envoy SDS API.
* Upon receiving the SDS request, the Istio agent creates the private key and CSR before sending the CSR with its credentials to istiod for signing.
* The CA validates the credentials carried in the CSR and signs the CSR to generate the certificate.
* The Istio agent sends the certificate received from istiod and the private key to Envoy via the Envoy SDS API.
* The above CSR process repeats periodically for certificate and key rotation.

image::images/sds.svg[]

The _Subject Alternative Names_ field on the certificate corresponds to the _SPIFFE_ identity name of the service account.
As part of client-side verification when using mTLS, this identity is verified against the _secure naming information_ maintained by the service mesh. The secure naming information contains N-to-N mappings from the server identities (encoded in certificates) to the service names.

Within the Envoy proxy, the certificates are inlined in the envoy configuration. The following steps demonstrate how to find and verify these certificates.

. Find the name of the incident service pod in the Emergency Response Demo project.
+
----
$ INCIDENT_SERVICE_POD=$(oc get pod -l app=$ERDEMO_USER-incident-service -o jsonpath={.items[0].metadata.name} -n $ERDEMO_NS)
----

. Dump the Envoy proxy config of the incident service sidecar to a file on your workstation:
+
----
$ oc rsh -n $ERDEMO_NS -c $ERDEMO_USER-incident-service $INCIDENT_SERVICE_POD curl http://127.0.0.1:15000/config_dump > envoy-config.json
----

. Open the `envoy-config.json` file in a text editor. Search in the file for `SecretsConfigDump`:
+
----
  {
   "@type": "type.googleapis.com/envoy.admin.v3.SecretsConfigDump",
   "dynamic_active_secrets": [
    {
     "name": "default",
     "version_info": "11-25 09:05:02.766",
     "last_updated": "2020-11-25T09:05:02.981Z",
     "secret": {
      "@type": "type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.Secret",
      "name": "default",
      "tls_certificate": {
       "certificate_chain": {
        "inline_bytes": "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"
       },
       "private_key": {
        "inline_bytes": "W3JlZGFjdGVkXQ=="
       }
      }
     }
    },
    {
     "name": "ROOTCA",
     "version_info": "2020-11-25 09:05:08.453074609 +0000 UTC m=+11.923741323",
     "last_updated": "2020-11-25T09:05:08.455Z",
     "secret": {
      "@type": "type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.Secret",
      "name": "ROOTCA",
      "validation_context": {
       "trusted_ca": {
        "inline_bytes": "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMzVENDQWNXZ0F3SUJBZ0lRSEdmL3ozOXl4MjJKWWhkWWtjREFCakFOQmdrcWhraUc5dzBCQVFzRkFEQVkKTVJZd0ZBWURWUVFLRXcxamJIVnpkR1Z5TG14dlkyRnNNQjRYRFRJd01URXlNekUxTVRnMU5sb1hEVE13TVRFeQpNVEUxTVRnMU5sb3dHREVXTUJRR0ExVUVDaE1OWTJ4MWMzUmxjaTVzYjJOaGJEQ0NBU0l3RFFZSktvWklodmNOCkFRRUJCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFLSU1RQXRrR3Y4eVBIUkVwUWJya081RDVWSmVlYU01TEcwaEJLNXoKZld4bkFsZFVNajlrUUhmRXpYVlZza3dwdlBqcTdNTTVTbW1DZEx2T0RUcEtsRXJOT2lLeHZ1YzN5djgraUE4UAo3VGN4UjFKK3o5NjA0MXp3dUJqelJDL09ZYjcveEFudVZGYjRjUG04YVhPTVhDY2hOaWpaTFRzemV3N3JsdytmCjdEU21iR2NEVTdzZ2pKdlBEdmJ2cnpEOGFSUXhYSldGdFZYeFVoQ2NRbjhkeElzQ0tySU4yQ0FQakNoL0UyREEKMXZ5WUQyWEY1cStTQm9EVHpsdG85cmpITm5lMFZtUjMyLzdqbFNhVEJ2VW1oblRMUFNCdzZ2bHF3RUlpZitDZQo2aFJlRjhGYjJURUFUY3FvU1NtczdDcXpEcEg2d2JWK3JneWhWQ2FHRkRQc0swc0NBd0VBQWFNak1DRXdEZ1lEClZSMFBBUUgvQkFRREFnSUVNQThHQTFVZEV3RUIvd1FGTUFNQkFmOHdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUIKQUZNNXlwMnUyenJ0Y0w0V2l5TzA1a2hjYm0xcXlrYW5xbExzR1A0eVZvZDhQZW0vaDNxTWp1WUUrWTFTNmJIOApCTFhwWExsejZwaTlqd3VUc09NNXc2cm96NlBvbTRZTzFTc201UkhzUldkSHBUYnZLTm5adGhrZUllSHkvZnVFCjdSOFdVKzVXbXlGRDB4Mit2M2YyYnNPaHFSL0REOU02eEVxU0s0T2tlMnZ1UVdsMUIyWXNzK1RzZ2UzYkVxZHUKMXFJKzhaS0tlUnErU0VFSzc1YTNyRzVGN3V2Tjdmbk1kc05mc3ZZSFJONXFkTU5rMlp3azhiSGpOK1cwOTBLUgpQL0pBQitkSzFzdjFNT0N5ZUNNbVRKMG1tMnBzMkJvclJqejlLUFRoaUZNM205bi9RSkxIMWdRUEhOLzlNdStSCmZQdXRHZTc0SnNYU2VIb3UwbG9TaTNnPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg=="
       }
      }
     }
    }
   ]
  }
----

. Copy the value of the `dynamic_active_secrets[0].secret.tls_certificate.certificate_chain.inline_bytes` element. In a terminal, base64 decode the value:
+
----
$ echo "<base64 encoded certificate chain>" | base64 -d
----
+
.Sample output
----
-----BEGIN CERTIFICATE-----
MIIDLzCCAhegAwIBAgIRAP/bYbKGZARyV+/D7BIXGSswDQYJKoZIhvcNAQELBQAw
GDEWMBQGA1UEChMNY2x1c3Rlci5sb2NhbDAeFw0yMDExMjUwOTA1MDJaFw0yMDEx
MjYwOTA1MDJaMAAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDY3TED
L2BEkeSy62b+Q0ViDzxwvjreI1ufw6RtejmMGqgflRnZ8qH0pg63TnOnH4pY7yVW
0kmHyB00bHhdx1wtynhaAyf0UrlinlWiVxR45HCUf6d6OqPrVDmKLZX1qf4eaxsG
EWK9Jx0MGUDdQMDeFbU0pph7wchJYUEUZQd7YkHysDdQpiHRJzO4B2Qod97poLdC
WjTebO5M45F/ODgKH+zRVhFSl3sQ/DdEuAbnhwvPCFe2x0LRMPzfkvG7BfXNvRbF
IBdDv7AAzaPToM+4+aqi4MhYz+kPmumtKGRGZeOFhx9ZA/FlHZlMub4rYXXB388V
p1ajRlX1YC9hMZqVAgMBAAGjgYswgYgwDgYDVR0PAQH/BAQDAgWgMB0GA1UdJQQW
MBQGCCsGAQUFBwMBBggrBgEFBQcDAjAMBgNVHRMBAf8EAjAAMEkGA1UdEQEB/wQ/
MD2GO3NwaWZmZTovL2NsdXN0ZXIubG9jYWwvbnMvdXNlcjEtZXItZGVtby9zYS9p
bmNpZGVudC1zZXJ2aWNlMA0GCSqGSIb3DQEBCwUAA4IBAQA2dFiVERlyJTo3dUBn
ejIT2MYJLnqRRxADYLmht5Pb1N8OkiJAy0oeQxK/QCRLlqU+LdUihMYArjkkPZ6v
k+lm3mR8vhRSQ8flCpoMtgVEjTZFGALW/LWpNNXA+QT6awGjFGtL3GsB3ooiuHAo
ffh9O1UaCXagyMVX4Z51CFPQS608QzWGQaZAdcWa9uVOm8Zwe5W+FH6IcqJlZhcX
E5THdTXNdlEXxwL+xZEL871CBR2WfQ06d2XmfUzv3svp28yfIxm+bbPafN80SyQo
Ds6/MT0ylvl4+NstC6L1hyIxq1FSDtkQowtJAKthcUmTKcBYEEXL1Q8+vdEmRF/5
N62z
-----END CERTIFICATE-----
-----BEGIN CERTIFICATE-----
MIIC3TCCAcWgAwIBAgIQHGf/z39yx22JYhdYkcDABjANBgkqhkiG9w0BAQsFADAY
MRYwFAYDVQQKEw1jbHVzdGVyLmxvY2FsMB4XDTIwMTEyMzE1MTg1NloXDTMwMTEy
MTE1MTg1NlowGDEWMBQGA1UEChMNY2x1c3Rlci5sb2NhbDCCASIwDQYJKoZIhvcN
AQEBBQADggEPADCCAQoCggEBAKIMQAtkGv8yPHREpQbrkO5D5VJeeaM5LG0hBK5z
fWxnAldUMj9kQHfEzXVVskwpvPjq7MM5SmmCdLvODTpKlErNOiKxvuc3yv8+iA8P
7TcxR1J+z96041zwuBjzRC/OYb7/xAnuVFb4cPm8aXOMXCchNijZLTszew7rlw+f
7DSmbGcDU7sgjJvPDvbvrzD8aRQxXJWFtVXxUhCcQn8dxIsCKrIN2CAPjCh/E2DA
1vyYD2XF5q+SBoDTzlto9rjHNne0VmR32/7jlSaTBvUmhnTLPSBw6vlqwEIif+Ce
6hReF8Fb2TEATcqoSSms7CqzDpH6wbV+rgyhVCaGFDPsK0sCAwEAAaMjMCEwDgYD
VR0PAQH/BAQDAgIEMA8GA1UdEwEB/wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggEB
AFM5yp2u2zrtcL4WiyO05khcbm1qykanqlLsGP4yVod8Pem/h3qMjuYE+Y1S6bH8
BLXpXLlz6pi9jwuTsOM5w6roz6Pom4YO1Ssm5RHsRWdHpTbvKNnZthkeIeHy/fuE
7R8WU+5WmyFD0x2+v3f2bsOhqR/DD9M6xEqSK4Oke2vuQWl1B2Yss+Tsge3bEqdu
1qI+8ZKKeRq+SEEK75a3rG5F7uvN7fnMdsNfsvYHRN5qdMNk2Zwk8bHjN+W090KR
P/JAB+dK1sv1MOCyeCMmTJ0mm2ps2BorRjz9KPThiFM3m9n/QJLH1gQPHN/9Mu+R
fPutGe74JsXSeHou0loSi3g=
-----END CERTIFICATE-----
----

. Copy the text output of the certificate chain.
. In a browser window, navigate to `https://www.sslshopper.com/certificate-decoder.html` and paste the certificate text into the text box there:
+
image::images/istio-workload-certificate.png[]
+
* Note that the certificate subject name is set to `URI:spiffe://cluster.local/ns/<emergency-response-demo namespace>/sa/incident-service`. This corresponds to the identity of the service as managed by Istio.
* Note also that the certificate validity timeframe is 1 day.
+
As an alternative, you can obtain the same information using the `istioctl` tool, in combination with `jq` and `openssl`:
+
----
$ istioctl proxy-config secret $INCIDENT_SERVICE_POD -n $ERDEMO_NS -i $SM_CP_NS -o json | jq '[.dynamicActiveSecrets[] | select(.name == "default")][0].secret.tlsCertificate.certificateChain.inlineBytes' -r | base64 -d | openssl x509 -noout -text
----
+
.Sample output
----
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number:
            db:4d:41:f3:fc:00:9a:1c:ee:f7:7f:02:d7:f8:b9:a8
        Signature Algorithm: sha256WithRSAEncryption
        Issuer: O = cluster.local
        Validity
            Not Before: Nov 27 08:57:00 2020 GMT
            Not After : Nov 28 08:57:00 2020 GMT
        Subject:
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                RSA Public-Key: (2048 bit)
                Modulus:
                    00:be:0c:f0:83:97:7f:45:54:4a:49:a3:bc:e0:34:
                    c7:cd:e6:d1:6b:a4:90:1b:d0:82:15:b3:01:03:88:
                    4b:0b:33:13:6f:94:61:a9:2c:d1:09:38:80:cf:ae:
                    e6:8a:b4:c3:1e:4f:e4:43:37:31:43:49:a6:07:eb:
                    9e:19:85:dd:27:1c:d5:86:bc:b8:fa:7a:df:59:fd:
                    63:b6:34:4c:77:a1:4e:d6:e7:26:36:59:26:02:f0:
                    b0:87:b6:52:47:40:6b:3e:96:56:35:ed:e0:26:01:
                    55:d9:1d:4d:2e:03:76:41:74:bd:c8:4a:58:f4:7f:
                    6e:18:e4:b5:1a:00:41:9c:95:0f:f8:ad:98:cf:0f:
                    2c:93:c6:53:2e:9d:47:81:5f:cc:bc:9f:57:03:60:
                    5f:bf:7b:08:00:5f:c9:e1:ed:6d:ff:70:cd:56:08:
                    39:75:bd:2e:91:2f:26:d6:02:96:d9:0a:86:cf:d2:
                    87:1a:13:3c:9e:87:35:c2:d8:30:8a:62:4d:8d:07:
                    81:32:74:b3:f1:92:d0:ce:49:a3:66:0b:34:a7:12:
                    a4:bb:77:86:94:d8:8a:bb:c4:6a:f9:47:56:ae:f7:
                    67:46:5d:48:75:d3:9f:79:b2:35:6e:b5:ca:1b:bd:
                    52:80:04:d7:fa:71:1a:ef:d9:de:74:c2:89:f9:b8:
                    da:21
                Exponent: 65537 (0x10001)
        X509v3 extensions:
            X509v3 Key Usage: critical
                Digital Signature, Key Encipherment
            X509v3 Extended Key Usage:
                TLS Web Server Authentication, TLS Web Client Authentication
            X509v3 Basic Constraints: critical
                CA:FALSE
            X509v3 Subject Alternative Name: critical
                URI:spiffe://cluster.local/ns/user1-er-demo/sa/incident-service
    Signature Algorithm: sha256WithRSAEncryption
         7f:69:7a:2e:69:a9:c6:42:0c:94:fe:56:86:e8:e0:30:82:a4:
         fb:8f:73:1f:89:d1:e8:c8:24:d4:ec:6c:05:d2:93:0f:55:27:
         ce:76:3c:d0:48:a4:c9:69:76:62:66:b8:5a:02:b1:99:dd:cd:
         8e:fc:da:66:d9:6f:75:38:d4:83:3a:b2:cb:97:f8:ba:19:57:
         5c:fa:cd:80:ca:ac:4d:d8:bb:50:94:cc:8a:cc:4f:0d:51:42:
         c5:de:fd:66:ad:df:bf:0b:bb:8b:6a:a6:e4:73:e4:46:d5:8c:
         c4:84:97:4a:85:92:e8:db:c8:e9:09:42:cb:a5:c8:b6:27:54:
         97:da:79:23:8b:9c:9a:e4:ca:cc:cf:e7:84:37:f3:20:84:ef:
         f1:bf:bc:78:38:fe:cf:33:fa:8e:c4:d9:72:93:ed:4d:18:d8:
         27:36:6e:d6:e7:16:f5:67:4f:bd:b6:20:ff:f3:04:21:cd:33:
         71:c2:4d:36:d6:03:6d:6d:f1:d3:d5:e8:1a:76:a1:d1:ea:12:
         91:05:62:ca:f6:70:a1:95:1e:b5:4a:c6:b4:8b:d8:c4:e5:dd:
         da:8f:37:ed:a7:0b:a9:bc:fb:5c:a2:38:a6:e2:7e:b3:a4:46:
         36:1a:a2:4a:07:f9:c0:c2:e5:64:95:07:0f:18:46:98:af:8e:
         b4:00:c2:e7
----

=== Understand Istio Ingress Gateways and Certificates

In an OpenShift environment, a route is used to expose services outside the cluster. Through the route, traffic is directed to the service pods.

In a service mesh, a better approach is to use a gateway for incoming traffic. This allows service mesh policies and routing rules to be applied to traffic entering the service mesh.

The service mesh installs an _Istio ingress gateway service_, which is an Envoy proxy container running on its own. All incoming traffic into the service mesh should be routed through the Istio ingress gateway to ensure that mesh policies and routing rules are applied to incoming traffic.

To route the traffic from the ingress gateway to the target services, `Gateway` and `VirtualService` resources are defined. One way to do this is to create `Gateway` and `VirtualService` resources for each service exposed outside of the cluster. An alternative is to use a single `Gateway` resource along with a `VirtualService` resource for each service. A third possibility consists of using a wildcard Gateway resource with a `VirtualService` resource for each service.

Which topology to choose will depend on an number of criteria, one of them being how to handle TLS certificates. If a wildcard certificate is acceptable for all services exposed outside of the mesh, a wildcard `Gateway` resource with a wildcard certificate can be used. If wildcard certificates are not desired, you can choose between a multi-host `Gateway` resource (which defines several hosts) or several single-host `Gateway` resources. The latter is the approach taken in this lab. When not using a single wildcard Gateway, a public certificate must be provided for every service exposed outside of the mesh.

Note that there is a limitation when combining certificates with `Gateway` and `VirtualService` resources to route traffic into the mesh with end-to-end transport security. Certificates cannot be shared by different `Gateway` resources. Configuring more than one `Gateway` resource using the same (wildcard) TLS certificate causes browsers that leverage HTTP/2 connection reuse--that is, most browsers--to produce 404 errors when accessing a second host after a connection to another host has already been established.
For a description of the problem, see the link:https://istio.io/docs/ops/common-problems/network-issues/#404-errors-occur-when-multiple-gateways-configured-with-same-tls-certificate[Istio documentation].

== Configure Emergency Response Demo Services for mTLS

In this part of the lab, you configure the different services of the Emergency Response Demo application for mTLS, as well as configuring the use of the Istio ingress gateway as an external entry point for the services.

=== Configure Incident Service

Enabling mTLS for the incident service involves the following tasks:

* Inject the Envoy proxy sidecar container into the incident service pod if you have not done this in a previous lab.
* Create a DestinationRule to enforce mTLS when calling the incident service.
* Create a certificate, a Gateway, a VirtualService and a route for TLS enabled external access to the incident service through the Istio ingress gateway.

* Ensure that health checks are still working when using strict mTLS.
** When enforcing strict mTLS when calling the incident service, the HTTP-based health check fails because it is executed from the kubelet and is not able to present a suitable certificate. The command-based health checks are executed in the container itself, so they are not impacted.
** The OpenShift Service Mesh sidecar injector service can be configured to rewrite HTTP probes at sidecar injection time, so that the requests are sent to the _Pilot agent_ in the Envoy sidecar, which then redirects to the application. This global configuration is set in the `ServiceMeshControlPlane` CR and is applied to all pods in the service mesh data plane that have the Envoy sidecar injected. HTTP probe rewriting can also be configured on individual deployments by annotating the Deployment or DeploymentConfig.

==== Inject Envoy Proxy Sidecar

In this section, you annotate the incident service pods with the `sidecar.istio.io/inject: "true"` annotation. You can skip this section if the service has been annotated in a previous lab.

. Open the DeploymentConfig of the incident service for editing:
+
----
oc edit dc $ERDEMO_USER-incident-service -o yaml -n $ERDEMO_NS
----

. In the `.spec.template.metadata` section, add the annotation:
+
----
[...]
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: "true"
      labels:
[...]
----
+
[TIP]
====
Alternatively, you can run the following `oc patch` command:

----
$ oc patch dc $ERDEMO_USER-incident-service --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-incident-service\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----

This forces a redeployment of the incident service.
====

. Verify that the Envoy proxy sidecar has been injected successfully and that the new pod consists of two containers, `incident-service` and `istio-proxy`:
+
image::images/incident-service-pod-sidecar.png[]

==== Configure HTTP-Based Health Checks

In this section, you configure the HTTP-based health checks to be rewritten and handled by the Pilot agent in the sidecar container.

[WARNING]
====
In the latest released version of the OpenShift Service Mesh operator, mesh wide rewrite of HTTP-based health checks does not longer work.

The alternatives are:

* Use command based HTTP health-checks
* Add the `sidecar.istio.io/rewriteAppHTTPProbers` annotation to the pod template in the DeploymentConfig of the service (at the same level as the `sidecar.istio.io/inject` annotation):
+
----
kind: DeploymentConfig
apiVersion: apps.openshift.io/v1
metadata:
  [...]
  namespace: user1-er-demo
  labels:
    app: user1-incident-service
spec:
  strategy:
    [...]
  triggers:
    [...]
  replicas: 1
  revisionHistoryLimit: 2
  test: false
  selector:
    app: user1-incident-service
    group: erd-services
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: user4-incident-service
        group: erd-services
      annotations:
        sidecar.istio.io/inject: 'true'
        sidecar.istio.io/rewriteAppHTTPProbers: 'true'
    spec:
      [...]
----
====

. Open the `ServiceMeshControlPlane` resource in the service mesh control plane for editing:
+
----
$ oc edit smcp full-install -o yaml -n $SM_CP_NS
----

. Add the configuration to allow rewriting of HTTP probes:
+
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  annotations:
    [...]]
  creationTimestamp: "2020-11-23T15:15:07Z"
  finalizers:
  - maistra.io/istio-operator
  generation: 1
  managedFields:
    [...]
  name: full-install
  namespace: admin1-istio-system
  resourceVersion: "926633"
  selfLink: /apis/maistra.io/v2/namespaces/admin1-istio-system/servicemeshcontrolplanes/full-install
  uid: 18fdfc5f-f6df-4148-b95e-590534a7d987
spec:
  addons:
    [...]
  gateways:
    [...]
  policy:
    [...]
  profiles:
  - default
  proxy:
    runtime:
      container:
        resources:
          limits:
            cpu: 500m
            memory: 128Mi
          requests:
            cpu: 100m
            memory: 128Mi
      readiness:
        rewriteApplicationProbes: true
  runtime:
    [...]
  security:
    [...]]
  techPreview:
    [...]]
  tracing:
    [...]]
  version: v2.0
status:
  [...]
----
. Save the `ServiceMeshControlPlane` resource.
. Redeploy the incident service:
+
----
$ oc rollout latest dc/$ERDEMO_USER-incident-service -n $ERDEMO_NS
----
. Once the new pod is up and running, examine the liveness and readiness probes:
+
----
$ oc get pod <incident service pod name> -o yaml -n $ERDEMO_NS
----
+
.Sample Output
----
apiVersion: v1
kind: Pod
metadata:
  [..]
spec:
  containers:
  - env:
    - name: KUBERNETES_NAMESPACE
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.namespace
    image: quay.io/emergencyresponsedemo/incident-service@sha256:6a25a3eb7ede1fb5ec23f506d3e5e3fb8b8aef783353bff63f438be784ecc6f4
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 3
      httpGet:
        path: /app-health/user1-incident-service/livez
        port: 15020
        scheme: HTTP
      initialDelaySeconds: 30
      periodSeconds: 30
      successThreshold: 1
      timeoutSeconds: 3
    name: user1-incident-service
    ports:
    - containerPort: 8080
      name: http
      protocol: TCP
    readinessProbe:
      failureThreshold: 3
      httpGet:
        path: /app-health/user1-incident-service/readyz
        port: 15020
        scheme: HTTP
      initialDelaySeconds: 30
      periodSeconds: 30
      successThreshold: 1
      timeoutSeconds: 3
    resources:
      limits:
        cpu: 500m
        memory: 500Mi
      requests:
        cpu: 100m
        memory: 200Mi
    securityContext:
      capabilities:
        drop:
        - KILL
        - MKNOD
        - SETGID
        - SETUID
      privileged: false
      procMount: Default
      runAsUser: 1000670000
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /app/logging
      name: logging
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: incident-service-token-qfx22
      readOnly: true
  - [...]
  volumes:
  [...]
status:
  [...]
----

* Note that the HTTP probes point to port 15020 instead of 8080.
* In the sidecar container, the Pilot agent is listening on port 15020. This is the process that bootstraps the Envoy proxy.
* The Pilot agent redirects the call to the application, strips the response body and returns the response code to the kubelet.

====  Configure HTTP-Based Health Checks with Annotation

. Open the DeploymentConfig of the incident service for editing:
+
----
oc edit dc $ERDEMO_USER-incident-service -o yaml -n $ERDEMO_NS
----

. In the `.spec.template.metadata` section, add the `sidecar.istio.io/rewriteAppHTTPProbers` annotation:
+
----
[...]
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: 'true'
        sidecar.istio.io/rewriteAppHTTPProbers: 'true'
      labels:
[...]
----
+
[TIP]
====
Alternatively, you can run the following `oc patch` command:

----
$ oc patch dc $ERDEMO_USER-incident-service --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\", \"sidecar.istio.io/rewriteAppHTTPProbers\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-incident-service\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----
====
. Check the liveness and readiness probes of the incident service pod.

==== Configure Command-Based Health Checks

In this section, you replace the health checks in the DeploymentConfig to use a command-based health check with `curl` rather than an HTTP-based health check. You can skip this step if you applied HTTP probe rewriting.

. Open the DeploymentConfig of the incident service for editing:
+
----
$ oc edit dc $ERDEMO_USER-incident-service -o yaml -n $ERDEMO_NS
----

. Replace the existing liveness and readiness probes with command-based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/health/live'
          initialDelaySeconds: 20
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/health/ready'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
[TIP]
====
Alternatively you can use the following `oc patch` command to patch the DeploymentConfig:

----
$ oc patch dc $ERDEMO_USER-incident-service --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/health/ready"]}, "initialDelaySeconds": 20, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/health/ready"]}, "initialDelaySeconds": 10, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
====

==== Create `DestinationRule`
In this section, you create a `DestinationRule` resource for the incident service. A `DestinationRule` resource defines policies that apply to traffic intended for a service after routing has occurred. In this case, you configure clients of the incident service to use mTLS using the certificates generated by Citadel.

. Create a file called `incident-service-mtls-destinationrule.yml`:
+
----
$ echo "---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: incident-service-client-mtls
spec:
  host: $ERDEMO_USER-incident-service.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" | tee incident-service-mtls-destinationrule.yml
----
. Create the `DestinationRule` resource:
+
----
$ oc create -f incident-service-mtls-destinationrule.yml -n $ERDEMO_NS
----

==== Create `Gateway` and External Certificate

. Obtain a certificate and key for incident service.
* To distinguish services exposed through the service mesh ingress gateway, you use a subdomain of the global OpenShift cluster application domain.
* In this lab, you use a self-signed certificate.

. Create a configuration file for OpenSSL:
+
----
$ echo "
[ req ]
req_extensions     = req_ext
distinguished_name = req_distinguished_name
prompt             = no

[req_distinguished_name]
commonName=incident-service.$ERDEMO_USER

[req_ext]
subjectAltName   = @alt_names

[alt_names]
DNS.1  = incident-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
" | tee incident-service-cert.cfg
----
. Create a self-signed certificate and private key:
+
----
$ openssl req -x509 -config incident-service-cert.cfg -extensions req_ext -nodes -days 730 -newkey rsa:2048 -sha256 -keyout incident-service-tls.key -out incident-service-tls.crt
----

. Create a secret named `incident-service-certs` in the service mesh control plane namespace with the certificates:
+
----
$ oc create secret tls incident-service-certs --cert incident-service-tls.crt --key incident-service-tls.key -n $SM_CP_NS
----

. Create a file called `incident-service-gateway.yml` with the definition of the gateway for the incident service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: incident-service-gateway
spec:
  selector:
    istio: ingressgateway # use istio default controller
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: incident-service-certs
    hosts:
    - \"incident-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE\"
" | tee incident-service-gateway.yml
----

. Create the gateway:
+
----
$ oc create -f incident-service-gateway.yml -n $SM_CP_NS
----

==== Create `VirtualService`

In this section, you create a `VirtualService` resource for the incident service. A `VirtualService` resource defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service.

. Create a file called `incident-service-virtualservice.yml`:
+
----
$ echo "---
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: incident-service-virtualservice
spec:
  hosts:
  - incident-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
  - $SM_CP_NS/incident-service-gateway
  http:
  - match:
    - uri:
        prefix: /incidents
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-incident-service.$ERDEMO_NS.svc.cluster.local
" | tee incident-service-virtualservice.yml
----
. Create the `VirtualService` resource:
+
----
$ oc create -f incident-service-virtualservice.yml -n $ERDEMO_NS
----

==== Create Route
Next, you create a route for the incident service which points to the Istio ingress gateway service.

. Create a file called `incident-service-route.yml`:
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: \"true\"
  labels:
    app: incident-service
  name: incident-service-route
spec:
  host: incident-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" | tee incident-service-route.yml
----
. Create the route in the control plane namespace:
+
----
$ oc create -f incident-service-route.yml -n $SM_CP_NS
----
+
[NOTE]
====
A route can be automatically created when a `Gateway` resource is deployed in the control plane. The host of the route is set to the host defined in the gateway.
Automatic route creation can be enabled in the `ServiceMeshControlPlane` resource:

. Open the `ServiceMeshControlPlane` resource in the service mesh control plane for editing:
+
----
$ oc edit smcp full-install -o yaml -n $SM_CP_NS
----

. Add the configuration to allow automatic creation of routes:
+
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  annotations:
    [...]]
  creationTimestamp: "2020-11-23T15:15:07Z"
  finalizers:
  - maistra.io/istio-operator
  generation: 1
  managedFields:
    [...]
  name: full-install
  namespace: admin1-istio-system
  resourceVersion: "926633"
  selfLink: /apis/maistra.io/v2/namespaces/admin1-istio-system/servicemeshcontrolplanes/full-install
  uid: 18fdfc5f-f6df-4148-b95e-590534a7d987
spec:
  addons:
    [...]
  gateways:
    [...]
    openshiftRoute:
      enabled: true
  policy:
    [...]
  profiles:
  - default
  proxy:
    [...]
  runtime:
    [...]
  security:
    [...]]
  techPreview:
    [...]]
  tracing:
    [...]]
  version: v2.0
status:
  [...]
----
. Save the `ServiceMeshControlPlane` resource.

. Alternatively, apply the following patch command:
+
----
$ oc patch smcp full-install --type='json' -p '[{"op": "replace", "path": "/spec/gateways/openshiftRoute/enabled", "value": true}]' -n $SM_CP_NS
----

====

. Delete the existing incident service route:
+
----
$ oc delete route $ERDEMO_USER-incident-service -n $ERDEMO_NS
----

. Verify that the incident service can be reached through the Istio ingress gateway:
+
----
$ curl -v -k https://incident-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/incidents
----
+
.Sample Output
----
*   Trying 18.159.31.177:443...
* Connected to incident-service.user1.apps.cluster-951e.951e.example.opentlc.com (18.159.31.177) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20):
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=incident-service.user1
*  start date: Nov 25 19:04:32 2020 GMT
*  expire date: Nov 25 19:04:32 2022 GMT
*  issuer: CN=incident-service.user1
*  SSL certificate verify result: self signed certificate (18), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x56071ea25b10)
> GET /incidents HTTP/2
> Host: incident-service.user1.apps.cluster-951e.951e.example.opentlc.com
> user-agent: curl/7.69.1
> accept: */*
>
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
* Connection state changed (MAX_CONCURRENT_STREAMS == 2147483647)!
< HTTP/2 200
< content-length: 3507
< content-type: application/json
< x-envoy-upstream-service-time: 2914
< date: Wed, 25 Nov 2020 19:24:03 GMT
< server: istio-envoy
<
[]
----

==== Verify Use of mTLS

. Use the `istioctl` tool to check that the traffic between the Istio ingress gateway service and the incident service uses mTLS:
+
----
$ INCIDENT_SERVICE_POD=$(oc get pod -l app=$ERDEMO_USER-incident-service -o jsonpath={.items[0].metadata.name} -n $ERDEMO_NS)
$ istioctl -n $ERDEMO_NS -i $SM_CP_NS x describe pod ${INCIDENT_SERVICE_POD}
----
+
.Sample Output
----
Pod: user1-incident-service-3-prkhd
   Pod Ports: 8080 (user1-incident-service), 15090 (istio-proxy)
Suggestion: add 'version' label to pod for Istio telemetry.
--------------------
Service: user1-incident-service
   Port: http 8080/HTTP targets pod port 8080
DestinationRule: incident-service-client-mtls for "user1-incident-service.user1-er-demo.svc.cluster.local"
   Traffic Policy TLS Mode: ISTIO_MUTUAL
----

. Use the Istio Grafana dashboards to verify that the traffic between the Istio ingress gateway and the incident service uses mTLS:
.. In a browser window, navigate to the Istio Grafana instance at `https://grafana-$SM_CP_NS.apps.$SUBDOMAIN_BASE` and log in with your OpenShift admin credentials.
.. On the left hand side, select the Square Icon. Then select *Dashboard > Manage*
+
image::images/dashboard-manage.png[]

.. Locate the *Istio Workload Dashboard*.
.. Select the data plane namespace and the Incident Service workload.
+
image::images/select-namespace-and-workload.png[]

.. Scroll down to the *Inbound Workloads* section and use `curl` to send some requests to the incident service.
.. Observe the graphs and expect to see a spike in the incoming requests from the ingress gateway.
** Note that the traffic is marked as mTLS.
+
image::images/istio-grafana-workload-inbound.png[]

. Perform a run of the Emergency Response Demo application to validate that the application is still working as expected.

=== Configure Responder Service

The procedure for enabling mTLS communication for the responder service and other services in the Emergency Response Demo application is very similar to that of the incident service. Only some differences are highlighted in the sections that follow.

. Annotate the responder service pods with the `sidecar.istio.io/inject: "true"` annotation. You can skip this section if the service has been annotated in a previous lab.
+
----
$ oc patch dc $ERDEMO_USER-responder-service --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-responder-service\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----

. To use HTTP probe rewriting, annotate the responder service pods with the `sidecar.istio.io/rewriteAppHTTPProbers: "true"` annotation:
+
----
$ oc patch dc $ERDEMO_USER-responder-service --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\", \"sidecar.istio.io/rewriteAppHTTPProbers\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-responder-service\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----

. As an alternative, replace the health checks in the responder service DeploymentConfig to use a command-based health check with `curl` rather than an HTTP-based health check. You can skip this step if you enabled HTTP probe rewriting in the service mesh.
.. Open the DeploymentConfig of the responder service for editing:
+
----
$ oc edit dc $ERDEMO_USER-responder-service -o yaml -n $ERDEMO_NS
----

.. Replace the existing liveness and readiness probes with command-based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/health/live'
          initialDelaySeconds: 20
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/health/ready'
          initialDelaySeconds: 3
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
[TIP]
====
Alternatively, you can run the following `oc patch` command:

----
$ oc patch dc $ERDEMO_USER-responder-service --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/health/live"]},"initialDelaySeconds": 20, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/health/ready"]},"initialDelaySeconds": 3, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
====

. Create a `DestinationRule` resource for the responder service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: responder-service-client-mtls
spec:
  host: $ERDEMO_USER-responder-service.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" | tee responder-service-mtls-destinationrule.yml
----
+
----
$ oc create -f responder-service-mtls-destinationrule.yml -n $ERDEMO_NS
----

. Create a configuration file for OpenSSL:
+
----
$ echo "
[ req ]
req_extensions     = req_ext
distinguished_name = req_distinguished_name
prompt             = no

[req_distinguished_name]
commonName=responder-service.$ERDEMO_USER

[req_ext]
subjectAltName   = @alt_names

[alt_names]
DNS.1  = responder-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
" | tee responder-service-cert.cfg
----

. Create a self-signed certificate and private key:
+
----
$ openssl req -x509 -config responder-service-cert.cfg -extensions req_ext -nodes -days 730 -newkey rsa:2048 -sha256 -keyout responder-service-tls.key -out responder-service-tls.crt
----

. Create a secret named `responder-service-certs` in the service mesh control plane namespace with the certificates:
+
----
$ oc create secret tls responder-service-certs --cert responder-service-tls.crt --key responder-service-tls.key -n $SM_CP_NS
----

. Create a file called `responder-service-gateway.yml` with the definition of the gateway for the responder service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: responder-service-gateway
spec:
  selector:
    istio: ingressgateway # use istio default controller
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: responder-service-certs
    hosts:
    - \"responder-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE\"
" | tee responder-service-gateway.yml
----

. Create the gateway:
+
----
$ oc create -f responder-service-gateway.yml -n $SM_CP_NS
----

. Create a `VirtualService` resource for the responder service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: responder-service-virtualservice
spec:
  hosts:
  - \"responder-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE\"
  gateways:
  - $SM_CP_NS/responder-service-gateway
  http:
  - match:
    - uri:
        prefix: /responders
    - uri:
        prefix: /responder
    - uri:
        exact: /stats
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-responder-service.$ERDEMO_NS.svc.cluster.local
" | tee responder-service-virtualservice.yml
----
+
----
$ oc create -f responder-service-virtualservice.yml -n $ERDEMO_NS
----
. Create a route for the responder service. Skip this step if you enabled automatic route creation.
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: responder-service
  name: responder-service-route
spec:
  host: "responder-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE"
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" | tee responder-service-route.yml
----
+
----
$ oc create -f responder-service-route.yml -n $SM_CP_NS
----
. Delete the existing responder service route:
+
----
$ oc delete route $ERDEMO_USER-responder-service -n $ERDEMO_NS
----
. Test external access to the responder service:
+
----
$ curl -v -k https://responder-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/responders/available
----

=== Configure Disaster Simulator

. Annotate the disaster simulator pods with the `sidecar.istio.io/inject: "true"` annotation. You can skip this section if the service has been annotated in a previous lab.
+
----
$ oc patch dc $ERDEMO_USER-disaster-simulator --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-disaster-simulator\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----

. To use HTTP probe rewriting, annotate the disaster simulator pods with the `sidecar.istio.io/rewriteAppHTTPProbers: "true"` annotation:
+
----
$ oc patch dc $ERDEMO_USER-disaster-simulator --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\", \"sidecar.istio.io/rewriteAppHTTPProbers\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-disaster-simulator\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----

. As an  alternative, replace the health checks in the disaster simulator DeploymentConfig to use a command-based health check with `curl` rather than an HTTP-based health check. You can skip this step if you enabled HTTP probe rewriting in the service mesh.
.. Open the DeploymentConfig of the disaster simulator for editing:
+
----
$ oc edit dc $ERDEMO_USER-disaster-simulator -o yaml -n $ERDEMO_NS
----

.. Replace the existing liveness and readiness probes with command-based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
[TIP]
====
Alternatively, run the following `oc patch` command:

----
$ oc patch dc $ERDEMO_USER-disaster-simulator --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080"]}, "initialDelaySeconds": 10, "timeoutSeconds": 1, "periodSeconds": 10, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080"]},"initialDelaySeconds": 10, "timeoutSeconds": 1, "periodSeconds": 10, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
====

. Create a `DestinationRule` resource for the disaster simulator service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: disaster-simulator-client-mtls
spec:
  host: $ERDEMO_USER-disaster-simulator.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" | tee disaster-simulator-mtls-destinationrule.yml
----
+
----
$ oc create -f disaster-simulator-mtls-destinationrule.yml -n $ERDEMO_NS
----

. Create a configuration file for OpenSSL:
+
----
$ echo "
[ req ]
req_extensions     = req_ext
distinguished_name = req_distinguished_name
prompt             = no

[req_distinguished_name]
commonName=disaster-simulator.$ERDEMO_USER

[req_ext]
subjectAltName   = @alt_names

[alt_names]
DNS.1  = disaster-simulator.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
" | tee disaster-simulator-cert.cfg
----

. Create a self-signed certificate and private key:
+
----
$ openssl req -x509 -config disaster-simulator-cert.cfg -extensions req_ext -nodes -days 730 -newkey rsa:2048 -sha256 -keyout disaster-simulator-tls.key -out disaster-simulator-tls.crt
----

. Create a secret named `disaster-simulator-certs` in the service mesh control plane namespace with the certificates:
+
----
$ oc create secret tls disaster-simulator-certs --cert disaster-simulator-tls.crt --key disaster-simulator-tls.key -n $SM_CP_NS
----

. Create a file called `disaster-simulator-gateway.yml` with the definition of the gateway for the disaster simulator:
+
----
$ echo "---
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: disaster-simulator-gateway
spec:
  selector:
    istio: ingressgateway # use istio default controller
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: disaster-simulator-certs
    hosts:
    - \"disaster-simulator.$ERDEMO_USER.apps.$SUBDOMAIN_BASE\"
" | tee disaster-simulator-gateway.yml
----

. Create the gateway:
+
----
$ oc create -f disaster-simulator-gateway.yml -n $SM_CP_NS
----

. Create a `VirtualService` resource for the disaster simulator service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: disaster-simulator-virtualservice
spec:
  hosts:
  - disaster-simulator.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
  - $SM_CP_NS/disaster-simulator-gateway
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-disaster-simulator.$ERDEMO_NS.svc.cluster.local
" | tee disaster-simulator-virtualservice.yml
----
+
----
$ oc create -f disaster-simulator-virtualservice.yml -n $ERDEMO_NS
----

. Create a route for the disaster simulator service. Skip this step if you enabled automatic route creation.
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: disaster-simulator
  name: disaster-simulator-route
spec:
  host: disaster-simulator.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" | tee disaster-simulator-route.yml
----
+
----
$ oc create -f disaster-simulator-route.yml -n $SM_CP_NS
----

. Delete the existing disaster simulator route:
+
----
$ oc delete route $ERDEMO_USER-disaster-simulator -n $ERDEMO_NS
----

. Test external access to the disaster simulator service:
+
----
$ curl -v -k https://disaster-simulator.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
----

=== Configure Incident Priority Service

. Annotate the incident priority service pods with the `sidecar.istio.io/inject: "true"` annotation. You can skip this section if the service has been annotated in a previous lab.
+
----
$ oc patch dc $ERDEMO_USER-incident-priority-service --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-incident-priority-service\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----

. To use HTTP probe rewriting, annotate the incident priority service pods with the `sidecar.istio.io/rewriteAppHTTPProbers: "true"` annotation:
+
----
$ oc patch dc $ERDEMO_USER-incident-priority-service  --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\", \"sidecar.istio.io/rewriteAppHTTPProbers\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-incident-priority-service\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----

. As an alternative, replace the health checks in the incident priority service DeploymentConfig to use a command-based health check with `curl` rather than an HTTP-based health check. You can skip this step if you enabled HTTP probe rewriting in the service mesh.
.. Open the DeploymentConfig of the incident priority service for editing:
+
----
$ oc edit dc $ERDEMO_USER-incident-priority-service -o yaml -n $ERDEMO_NS
----

.. Replace the existing liveness and readiness probes with command-based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/health'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/health'
          initialDelaySeconds: 10
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
[TIP]
====
Alternatively, run the following `oc patch` command:

----
$ oc patch dc $ERDEMO_USER-incident-priority-service --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/health"]}, "initialDelaySeconds": 30, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/health"]},"initialDelaySeconds": 10, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
====

. Create a `DestinationRule` resource for the incident priority service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: incident-priority-service-client-mtls
spec:
  host: $ERDEMO_USER-incident-priority-service.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" | tee incident-priority-service-mtls-destinationrule.yml
----
+
----
$ oc create -f incident-priority-service-mtls-destinationrule.yml -n $ERDEMO_NS
----

. Create a configuration file for OpenSSL:
+
----
$ echo "
[ req ]
req_extensions     = req_ext
distinguished_name = req_distinguished_name
prompt             = no

[req_distinguished_name]
commonName=incident-priority-service.$ERDEMO_USER

[req_ext]
subjectAltName   = @alt_names

[alt_names]
DNS.1  = incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
" | tee incident-priority-service-cert.cfg
----

. Create a self-signed certificate and private key:
+
----
$ openssl req -x509 -config incident-priority-service-cert.cfg -extensions req_ext -nodes -days 730 -newkey rsa:2048 -sha256 -keyout incident-priority-service-tls.key -out incident-priority-service-tls.crt
----

. Create a secret named `incident-priority-service-certs` in the service mesh control plane namespace with the certificates:
+
----
$ oc create secret tls incident-priority-service-certs --cert incident-priority-service-tls.crt --key incident-priority-service-tls.key -n $SM_CP_NS
----

. Create a file called `incident-priority-service-gateway.yml` with the definition of the gateway for the incident priority service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: incident-priority-service-gateway
spec:
  selector:
    istio: ingressgateway # use istio default controller
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: incident-priority-service-certs
    hosts:
    - \"incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE\"
" | tee incident-priority-service-gateway.yml
----

. Create the gateway:
+
----
$ oc create -f incident-priority-service-gateway.yml -n $SM_CP_NS
----

. Create a `VirtualService` resource for the incident priority service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: incident-priority-service-virtualservice
spec:
  hosts:
  - incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
  - $SM_CP_NS/incident-priority-service-gateway
  http:
  - match:
    - uri:
        prefix: /priority
    - uri:
        exact: /reset
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-incident-priority-service.$ERDEMO_NS.svc.cluster.local
" | tee incident-priority-service-virtualservice.yml
----
+
----
$ oc create -f incident-priority-service-virtualservice.yml -n $ERDEMO_NS
----

. Create a route for the incident priority service. Skip this step if you enabled automatic route creation.
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: incident-priority-service
  name: incident-priority-service-route
spec:
  host: incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" | tee incident-priority-service-route.yml
----
+
----
$ oc create -f incident-priority-service-route.yml -n $SM_CP_NS
----

. Delete the existing incident priority service route:
+
----
$ oc delete route $ERDEMO_USER-incident-priority-service -n $ERDEMO_NS
----

. Test external access to the incident priority service:
+
----
$ curl -v -k https://incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/priority/qwerty
----

=== Configure Process Service

The process service is not exposed outside of the cluster, so there is no need for a `Gateway`, a `VirtualService` resource or a route.

. Annotate the process service pods with the `sidecar.istio.io/inject: "true"` annotation. You can skip this section if the service has been annotated in a previous lab.
+
----
$ oc patch dc $ERDEMO_USER-process-service --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-process-service\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----

. To use HTTP probe rewriting, annotate the process service pods with the `sidecar.istio.io/rewriteAppHTTPProbers: "true"` annotation:
+
----
$ oc patch dc $ERDEMO_USER-process-service --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\", \"sidecar.istio.io/rewriteAppHTTPProbers\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-process-service\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----

. As an alternative, replace the health checks in the process service DeploymentConfig to use a command-based health check with `curl` rather than an HTTP-based health check. You can skip this step if you enabled HTTP probe rewriting in the service mesh.
.. Open the DeploymentConfig of the process service for editing:
+
----
$ oc edit dc $ERDEMO_USER-process-service -o yaml -n $ERDEMO_NS
----

.. Replace the existing liveness and readiness probes with command-based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/actuator/health'
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/actuator/health'
          initialDelaySeconds: 45
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
[TIP]
====
Alternatively, run the following `oc patch` command:

----
$ oc patch dc $ERDEMO_USER-process-service --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/actuator/health"]},"initialDelaySeconds": 60, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/actuator/health"]},"initialDelaySeconds": 45, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
====

. Create a `DestinationRule` resource for the process service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: process-service-client-mtls
spec:
  host: $ERDEMO_USER-process-service.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" | tee process-service-mtls-destinationrule.yml
----
+
----
$ oc create -f process-service-mtls-destinationrule.yml -n $ERDEMO_NS
----

=== Configure Mission Service

The mission service is not exposed outside of the cluster, so there is no need for a `Gateway`, a `VirtualService` resource or a route.

. Annotate the mission service pods with the `sidecar.istio.io/inject: "true"` annotation. You can skip this section if the service has been annotated in a previous lab.
+
----
$ oc patch dc $ERDEMO_USER-mission-service --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-mission-service\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----

. To use HTTP probe rewriting, annotate the mission service pods with the `sidecar.istio.io/rewriteAppHTTPProbers: "true"` annotation:
+
----
$ oc patch dc $ERDEMO_USER-mission-service --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\", \"sidecar.istio.io/rewriteAppHTTPProbers\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-mission-service\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----

. As an alternative, replace the health checks in the mission service DeploymentConfig to use a command-based health check with `curl` rather than an HTTP-based health check. You can skip this step if you enabled HTTP probe rewriting in the service mesh.
.. Open the DeploymentConfig of the mission service for editing:
+
----
$ oc edit dc $ERDEMO_USER-mission-service -o yaml -n $ERDEMO_NS
----

.. Replace the existing liveness and readiness probes with command-based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/health/live'
          initialDelaySeconds: 20
          periodSeconds: 10
          timeoutSeconds: 1
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/health/ready'
          initialDelaySeconds: 3
          periodSeconds: 10
          timeoutSeconds: 1
[...]
----
+
[TIP]
====
Alternatively, run the following `oc patch` command:

----
$ oc patch dc $ERDEMO_USER-mission-service --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/health/live"]},"initialDelaySeconds": 20, "timeoutSeconds": 1, "periodSeconds": 10, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/health/ready"]},"initialDelaySeconds": 3, "timeoutSeconds": 1, "periodSeconds": 10, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
====

. Create a `DestinationRule` resource for the mission service:
+
----
$ echo "---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: mission-service-client-mtls
spec:
  host: $ERDEMO_USER-mission-service.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" | tee mission-service-mtls-destinationrule.yml
----
+
----
$ oc create -f mission-service-mtls-destinationrule.yml -n $ERDEMO_NS
----

=== Configure Responder Simulator

The responder simulator service is not exposed outside of the cluster, so there is no need for a `Gateway`, a `VirtualService` resource or a route.

. Annotate the responder simulator pods with the `sidecar.istio.io/inject: "true"` annotation. You can skip this section if the service has been annotated in a previous lab.
+
----
$ oc patch dc $ERDEMO_USER-responder-simulator --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-responder-simulator\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----

. To use HTTP probe rewriting, annotate the responder simulator pods with the `sidecar.istio.io/rewriteAppHTTPProbers: "true"` annotation:
+
----
$ oc patch dc $ERDEMO_USER-responder-simulator --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\", \"sidecar.istio.io/rewriteAppHTTPProbers\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-responder-simulator\",\"group\":\"erd-services\"}}}]" -n $ERDEMO_NS
----

. As an alternative, replace the health checks in the responder simulator DeploymentConfig to use a command-based health check with `curl` rather than an HTTP-based health check. You can skip this step if you enabled HTTP probe rewriting in the service mesh.
.. Open the DeploymentConfig of the responder simulator for editing:
+
----
$ oc edit dc $ERDEMO_USER-responder-simulator -o yaml -n $ERDEMO_NS
----

.. Replace the existing liveness and readiness probes with command-based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/health'
          initialDelaySeconds: 20
          periodSeconds: 10
          timeoutSeconds: 1
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/health'
          initialDelaySeconds: 3
          periodSeconds: 10
          timeoutSeconds: 1
[...]
----
+
[TIP]
====
Alternatively, run the following `oc patch` command:

----
$ oc patch dc $ERDEMO_USER-responder-simulator --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/health"]},"initialDelaySeconds": 20, "timeoutSeconds": 1, "periodSeconds": 10, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/health"]},"initialDelaySeconds": 3, "timeoutSeconds": 1, "periodSeconds": 10, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
====

. Create a `DestinationRule` resource for the responder simulator:
+
----
$ echo "---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: responder-simulator-client-mtls
spec:
  host: $ERDEMO_USER-responder-simulator.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" | tee responder-simulator-mtls-destinationrule.yml
----
+
----
$ oc create -f responder-simulator-mtls-destinationrule.yml -n $ERDEMO_NS
----

=== Configure Process Viewer

The process viewer service is not exposed outside of the cluster, so there is no need for a `Gateway`, a `VirtualService` resource or a route.

. Annotate the process viewer pods with the `sidecar.istio.io/inject: "true"` annotatio. You can skip this section if the service has been annotated in a previous lab.
+
----
$ oc patch dc $ERDEMO_USER-process-viewer --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-process-viewer\"}}}]" -n $ERDEMO_NS
----

. To use HTTP probe rewriting, annotate the process viewer pods with the `sidecar.istio.io/rewriteAppHTTPProbers: "true"` annotation:
+
----
$ oc patch dc $ERDEMO_USER-process-viewer --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\", \"sidecar.istio.io/rewriteAppHTTPProbers\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-process-viewer\"}}}]" -n $ERDEMO_NS
----

. As an altenative, replace the health checks in the process viewer DeploymentConfig to use a command-based health check with `curl` rather than an HTTP-based health check. You can skip this step if you enabled HTTP probe rewriting in the service mesh.
.. Open the DeploymentConfig of the process viewer for editing:
+
----
$ oc edit dc $ERDEMO_USER-process-viewer -o yaml -n $ERDEMO_NS
----

.. Replace the existing liveness and readiness probes with command-based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/health'
          initialDelaySeconds: 15
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080/health'
          initialDelaySeconds: 5
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
[TIP]
====
Alternatively, run the following `oc patch` command:

----
$ oc patch dc $ERDEMO_USER-process-viewer --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/health"]},"initialDelaySeconds": 15, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080/health"]},"initialDelaySeconds": 5, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
====

. Create a `DestinationRule` resource for the process viewer:
+
----
$ echo "---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: process-viewer-client-mtls
spec:
  host: $ERDEMO_USER-process-viewer.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" | tee process-viewer-mtls-destinationrule.yml
----
+
----
$ oc create -f process-viewer-mtls-destinationrule.yml -n $ERDEMO_NS
----

=== Configure Emergency Console

. Annotate the emergency console pods with the `sidecar.istio.io/inject: "true"` annotation. You can skip this section if the service has been annotated in a previous lab.
+
----
$ oc patch dc $ERDEMO_USER-emergency-console --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-emergency-console\"}}}]" -n $ERDEMO_NS
----

. To use HTTP probe rewriting, annotate the emergency console pods with the `sidecar.istio.io/rewriteAppHTTPProbers: "true"` annotation:
+
----
$ oc patch dc $ERDEMO_USER-emergency-console --type='json' -p "[{\"op\": \"add\", \"path\": \"/spec/template/metadata\", \"value\": {\"annotations\":{\"sidecar.istio.io/inject\": \"true\", \"sidecar.istio.io/rewriteAppHTTPProbers\": \"true\"}, \"labels\":{\"app\":\"$ERDEMO_USER-emergency-console\"}}}]" -n $ERDEMO_NS
----

. As an alternative, replace the health checks in the emergency console DeploymentConfig to use a command-based health check with `curl` rather than an HTTP-based health check. You can skip this step if you enabled HTTP probe rewriting in the service mesh.
.. Open the DeploymentConfig of the emergency console for editing:
+
----
$ oc edit dc $ERDEMO_USER-emergency-console -o yaml -n $ERDEMO_NS
----

.. Replace the existing liveness and readiness probes with command-based probes:
+
----
[...]
        livenessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
        readinessProbe:
          failureThreshold: 3
          exec:
            command:
              - curl
              - 'http://127.0.0.1:8080'
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 3
[...]
----
+
[TIP]
====
Alternatively, run the following `oc patch` command:

----
$ oc patch dc $ERDEMO_USER-emergency-console --type='json' -p '[{"op": "remove", "path": "/spec/template/spec/containers/0/livenessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/livenessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080"]}, "initialDelaySeconds": 30, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}, {"op": "remove", "path": "/spec/template/spec/containers/0/readinessProbe/httpGet"}, {"op": "add", "path": "/spec/template/spec/containers/0/readinessProbe", "value": { "exec": { "command" : ["curl", "http://127.0.0.1:8080"]},"initialDelaySeconds": 30, "timeoutSeconds": 3, "periodSeconds": 30, "successThreshold": 1, "failureThreshold": 3}}]' -n $ERDEMO_NS
----
====

. Create a `DestinationRule` resource for the emergency console:
+
----
$ echo "---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: emergency-console-client-mtls
spec:
  host: $ERDEMO_USER-emergency-console.$ERDEMO_NS.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
" | tee emergency-console-mtls-destinationrule.yml
----
+
----
$ oc create -f emergency-console-mtls-destinationrule.yml -n $ERDEMO_NS
----

. Create a configuration file for OpenSSL:
+
----
$ echo "
[ req ]
req_extensions     = req_ext
distinguished_name = req_distinguished_name
prompt             = no

[req_distinguished_name]
commonName=emergency-console.$ERDEMO_USER

[req_ext]
subjectAltName   = @alt_names

[alt_names]
DNS.1  = emergency-console.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
" | tee emergency-console-cert.cfg
----
. Create a self-signed certificate and private key:
+
----
$ openssl req -x509 -config emergency-console-cert.cfg -extensions req_ext -nodes -days 730 -newkey rsa:2048 -sha256 -keyout emergency-console-tls.key -out emergency-console-tls.crt
----

. Create a secret named `emergency-console-certs` in the service mesh control plane namespace with the certificates:
+
----
$ oc create secret tls emergency-console-certs --cert emergency-console-tls.crt --key emergency-console-tls.key -n $SM_CP_NS
----

. Create a file called `emergency-console-gateway.yml` with the definition of the gateway for the emergency console:
+
----
$ echo "---
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: emergency-console-gateway
spec:
  selector:
    istio: ingressgateway # use istio default controller
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: emergency-console-certs
    hosts:
    - \"emergency-console.$ERDEMO_USER.apps.$SUBDOMAIN_BASE\"
" | tee emergency-console-gateway.yml
----

. Create the gateway:
+
----
$ oc create -f emergency-console-gateway.yml -n $SM_CP_NS
----

. Create a `VirtualService` resource for the emergency console:
+
----
$ echo "---
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: emergency-console-virtualservice
spec:
  hosts:
  - emergency-console.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
  - $SM_CP_NS/emergency-console-gateway
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        port:
          number: 8080
        host: $ERDEMO_USER-emergency-console.$ERDEMO_NS.svc.cluster.local
" | tee emergency-console-virtualservice.yml
----
+
----
$ oc create -f emergency-console-virtualservice.yml -n $ERDEMO_NS
----

. Create a route for the emergency console. Skip this step if you enabled automatic route creation.
+
----
$ echo "---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: 'true'
  labels:
    app: emergency-console
  name: emergency-console-route
spec:
  host: emergency-console.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  port:
    targetPort: https
  tls:
    termination: passthrough
  to:
    kind: Service
    name: istio-ingressgateway
    weight: 100
  wildcardPolicy: None
" | tee emergency-console-route.yml
----
+
----
$ oc create -f emergency-console-route.yml -n $SM_CP_NS
----

. Delete the existing emergency console route:
+
----
$ oc delete route $ERDEMO_USER-emergency-console -n $ERDEMO_NS
----

. The redirect URL in the `emergency-realm` in the SSO instance needs to be changed. Obtain the URL for the administration console of the SSO instance:
+
----
$ echo -en "\n\nhttps://sso-erd-sso.apps.$SUBDOMAIN_BASE/auth/admin/$ERDEMO_USER-emergency-realm/console\n\n"
----
. In a browser window, navigate to the SSO console and log in with `incident_commander/r3dh4t1!`.
* Expect to be routed to the administration section of your realm.
. In the left-hand menu, click *Clients* and select the `js` client.
. On the *Settings* tab, scroll to the *Valid Redirect URIs* field, remove the existing redirect URL for the emergency console, and add the new URL for the emergency console:
+
----
https://emergency-console.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/*
----

. Test access to the emergency console in a browser window using the `https://emergency-console.$ERDEMO_USER.apps.$SUBDOMAIN_BASE` URL.

=== Set Strict mTLS Policy

. Test the Emergency Response Demo application.
* Expect everything to work correctly.
. At this point, you can turn on strict mTLS mode for the applications in the data plane. One way to do so is to create a namespace scoped `PeerAuthentication` resource:
+
----
$ echo "---
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: er-demo-peerauthentication
spec:
  mtls:
    mode: STRICT
" | tee er-demo-peerauthentication.yml
----
+
----
$ oc create -f er-demo-peerauthentication.yml -n $ERDEMO_NS
----

. Test the application again.
* Expect everything to work correctly.

== Set Authorization Policy for Services using AuthorizationPolicy

OpenShift Service Mesh introduced the _AuthorizationPolicy_ policy, which is a major update to the previous role-based access control (RBAC) policy. The new policy provides these improvements:

* Aligns with Istio configuration model.
* Improves the user experience by simplifying the API. AuthorizationPolicy only uses 1 CRD instead of three.
* Supports more use cases (e.g. Ingress/Egress gateway support) without added complexity.

With a service mesh authorization policy, you can define service-to-service authorization policies to control which services can communicate with other services.

To configure an authorization policy, you create an _AuthorizationPolicy_ custom resource. An authorization policy includes a selector, an action, and a list of rules:

* The _selector_ field specifies the target of the policy
* The _action_ field specifies whether to allow or deny the request. The default action is allow.
* The _rules_ specify when to trigger the action.
** The _from_ field in the rules specifies the sources of the request
** The _to_ field in the rules specifies the operations of the request
** The _when_ field specifies the conditions needed to apply the rule

As a use case for this lab, you define and enable the following policy:

* All services within the Emergency Response Demo application are allowed to call the incident service REST APIs, using all HTTP methods.
* When accessed from outside the cluster, only the APIs that use HTTP GET are allowed.

{nbsp}

. Create a deny-all policy for the incident service.
.. On the local file system, create a file called `deny-all-incident-service.yml`:
+
----
$ echo "---
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: deny-all-incident-service
spec:
  selector:
    matchLabels:
      app: $ERDEMO_USER-incident-service
      group: erd-services
" | tee deny-all-incident-service.yml
----
* The selector matches the labels on the incident service workload (pods).
* There are no rules defined, so no rules can be matched. This effectively corresponds to a deny-all policy.

.. Create the resource in the emergency response demo namespace:
+
----
$ oc create -f deny-all-incident-service.yml -n $ERDEMO_NS
----

. At this point, no service can call the incident service from within the mesh or from outside the mesh. Test this as follows:
.. Call the `/incidents` REST endpoint of the incident service with `curl`:
+
----
$ INCIDENT_SERVICE_URL=$(oc get route incident-service-route -n $SM_CP_NS -o template --template={{.spec.host}})
$ curl -v -k https://$INCIDENT_SERVICE_URL/incidents
----
+
.Sample Output
----
*   Trying 35.158.48.203:443...
* TCP_NODELAY set
* Connected to incident-service.user1.apps.cluster-18ac.18ac.example.opentlc.com (35.158.48.203) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20):
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=user1.apps.cluster-18ac.18ac.example.opentlc.com
*  start date: May  4 18:51:29 2020 GMT
*  expire date: May  4 18:51:29 2022 GMT
*  issuer: CN=user1.apps.cluster-18ac.18ac.example.opentlc.com
*  SSL certificate verify result: self signed certificate (18), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x55efdc8a4200)
> GET /incidents HTTP/2
> Host: incident-service.user1.apps.cluster-18ac.18ac.example.opentlc.com
> User-Agent: curl/7.66.0
> Accept: */*
>
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
* Connection state changed (MAX_CONCURRENT_STREAMS == 4294967295)!
< HTTP/2 403
< content-length: 19
< content-type: text/plain
< date: Tue, 05 May 2020 15:37:51 GMT
< server: istio-envoy
< x-envoy-upstream-service-time: 6
<
* Connection #0 to host incident-service.user1.apps.cluster-18ac.18ac.example.opentlc.com left intact
RBAC: access denied
----
** The HTTP return code is 403, with message "RBAC: access denied".
.. Test a call to the incident service from within the cluster by opening the *Disaster Simulator* home page in a browser and clicking *Clear Incidents*. Then check the logs of the disaster simulator pod and expect to see something similar to this:
+
----
Oct 30, 2019 2:19:52 PM com.redhat.cajun.navy.datagenerate.RestClientVerticle lambda$resetIncidents$4
INFO: Reset incidents: HTTP response status 403
----

. Create an `AuthorizationPolicy` to allow workloads inside the service mesh to access the incident service using all HTTP methods.
.. On the local file system, create a file called `incident-service-internal.yml`:
+
----
$ echo "---
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: incident-service-internal
spec:
  rules:
  - from:
    - source:
        namespaces:
        - $ERDEMO_NS
    to:
    - operation:
        methods:
        - '*'
  selector:
    matchLabels:
      app: $ERDEMO_USER-incident-service
      group: erd-services
" | tee incident-service-internal.yml
----
.. Create the resource in the emergency response demo namespace:
+
----
$ oc create -f incident-service-internal.yml -n $ERDEMO_NS
----

. Confirm that the incident service can now be called from other services in the Emergency Response Demo namespace, but not from services outside the namespace:
.. From the *Disaster Simulator* home page, click *Clear Incidents*. Review the disaster simulator pod logs and expect to see something similar to this:
+
----
Oct 30, 2019 3:02:31 PM com.redhat.cajun.navy.datagenerate.RestClientVerticle lambda$resetIncidents$4
INFO: Reset incidents: HTTP response status 200
----

. Create an `AuthorizationPolicy` resource for external access to the incident service. This service role grants only HTTP GET access to the incident service REST APIs for all traffic handled by the ingress gateway.
.. On the local file system, create a file called `incident-service-external.yml`:
+
----
$ echo "---
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: incident-service-external
spec:
  rules:
  - from:
    - source:
        principals:
        - cluster.local/ns/$SM_CP_NS/sa/istio-ingressgateway-service-account
    to:
    - operation:
        methods:
        - GET
  selector:
    matchLabels:
      app: $ERDEMO_USER-incident-service
      group: erd-services
" | tee incident-service-external.yml
----
* The ingress gateway is specified as the SPIFFE identity of its service account.

.. Create the resource in the emergency response demo namespace:
+
----
$ oc create -f incident-service-external.yml -n $ERDEMO_NS
----

. Confirm that the incident service can now be called from outside of the cluster, but only for HTTP GET endpoints:
.. A `curl` request to the `/incidents` endpoint of the incident service returns an HTTP 200 code.
.. A `curl` POST request to the `/incidents` endpoint returns an HTTP 403 error:
+
----
$ curl -k -v -X POST -H "Content-type: application/json" -d '{"lat": 34.14338, "lon": -77.86569, "numberOfPeople": 3, "medicalNeeded": true, "victimName": "victim",  "victimPhoneNumber": "111-111-111" }' https://$INCIDENT_SERVICE_URL/incidents
----
+
.Sample output
----
$ curl -k -v -X POST -H "Content-type: application/json" -d '{"lat": 34.14338, "lon": -77.86569, "numberOfPeople": 3, "medicalNeeded": true, "victimName": "victim",  "victimPhoneNumber": "111-111-111" }' https://$INCIDENT_SERVICE_URL/incidents
Note: Unnecessary use of -X or --request, POST is already inferred.
*   Trying 35.158.48.203:443...
* TCP_NODELAY set
* Connected to incident-service.user1.apps.cluster-18ac.18ac.example.opentlc.com (35.158.48.203) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20):
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=user1.apps.cluster-18ac.18ac.example.opentlc.com
*  start date: May  4 18:51:29 2020 GMT
*  expire date: May  4 18:51:29 2022 GMT
*  issuer: CN=user1.apps.cluster-18ac.18ac.example.opentlc.com
*  SSL certificate verify result: self signed certificate (18), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x5601e8398200)
> POST /incidents HTTP/2
> Host: incident-service.user1.apps.cluster-18ac.18ac.example.opentlc.com
> User-Agent: curl/7.66.0
> Accept: */*
> Content-type: application/json
> Content-Length: 141
>
* We are completely uploaded and fine
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
* Connection state changed (MAX_CONCURRENT_STREAMS == 4294967295)!
< HTTP/2 403
< content-length: 19
< content-type: text/plain
< date: Tue, 05 May 2020 15:45:36 GMT
< server: istio-envoy
< x-envoy-upstream-service-time: 2
<
* Connection #0 to host incident-service.user1.apps.cluster-18ac.18ac.example.opentlc.com left intact
RBAC: access denied
----

== Configure Origin Authentication

Istio supports end-user authentication using JSON Web Tokens (JWT). End-user authentication verifies the original client making the request as an end user (or device). The end-user request requires passing a JWT token to be verified by the target service proxy. End-user authentication is configured in a `RequestAuthentication` resource.

In this exercise, you secure the ingress gateway service with JWT. You use single sign-on for Red Hat solutions (SSO) as the JWT token provider.

=== Configure SSO Instance
In this section, you configure the SSO instance to be able to obtain a JWT token.

. Log in as realm administrator to the Admin console of the SSO instance deployed as part of the Emergency Response Demo application:
.. Obtain the URL for the Admin console of the SSO instance:
+
----
$ echo -en "\n\nhttps://sso-erd-sso.apps.$SUBDOMAIN_BASE/auth/admin/$ERDEMO_USER-emergency-realm/console\n\n"
----
.. In a browser window, navigate to the SSO Admin console and log in using `incident_commander/r3dh4t1!`.
* Expect to see the administration section of your realm.
. In the left-hand panel, click *Clients* and then click *Create* to add a new client.
. Enter `curl` as the *Client ID* and click *Save*:
+
image::images/sso-client-create.png[]
. In the *Settings* tab, disable *Standard Flow* and enable *Direct Access Grants*, then click *Save*:
+
image::images/sso-client-settings.png[]

=== Obtain JWT Token
. In a terminal, use the `curl` utility to obtain a token based on your SSO username and password:
+
----
$ export RHSSO_URL=https://sso-erd-sso.apps.$SUBDOMAIN_BASE
$ export REALM=$ERDEMO_USER-emergency-realm
$ TKN=$(curl -X POST "$RHSSO_URL/auth/realms/$REALM/protocol/openid-connect/token" \
 -H 'Content-Type: application/x-www-form-urlencoded' \
 -d 'username=incident_commander' \
 -d 'password=r3dh4t1!' \
 -d 'grant_type=password' \
 -d 'client_id=curl' \
  --insecure \
 | sed 's/.*access_token":"//g' | sed 's/".*//g')
$ echo $TKN
----
+
.Sample Output
----
eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJBRC12YkxLSEhhd19RQWo2cXA2WnJFaTI1bFJLelNhWDBXNUd5M1hoREdnIn0.eyJleHAiOjE2MDY1MDMwNzIsImlhdCI6MTYwNjUwMjc3MiwianRpIjoiZjVjMmY1ZGItNjRhOS00N2Y4LTljMWUtOWI5M2E0ZDQ0M2M3IiwiaXNzIjoiaHR0cHM6Ly9zc28tZXJkLXNzby5hcHBzLmNsdXN0ZXItOTUxZS45NTFlLmV4YW1wbGUub3BlbnRsYy5jb20vYXV0aC9yZWFsbXMvdXNlcjEtZW1lcmdlbmN5LXJlYWxtIiwiYXVkIjpbInJlYWxtLW1hbmFnZW1lbnQiLCJhY2NvdW50Il0sInN1YiI6ImUyYTI5MTViLWFhZDEtNDdlYy04ZmQwLTVlYzQ1MThhYzkzMSIsInR5cCI6IkJlYXJlciIsImF6cCI6ImN1cmwiLCJzZXNzaW9uX3N0YXRlIjoiZjgwM2Q1MGEtZDE5MS00ZTQwLWE3MzktNDYxYjI1NjY5NWRhIiwiYWNyIjoiMSIsInJlYWxtX2FjY2VzcyI6eyJyb2xlcyI6WyJvZmZsaW5lX2FjY2VzcyIsImluY2lkZW50X2NvbW1hbmRlciIsInVtYV9hdXRob3JpemF0aW9uIl19LCJyZXNvdXJjZV9hY2Nlc3MiOnsicmVhbG0tbWFuYWdlbWVudCI6eyJyb2xlcyI6WyJ2aWV3LXJlYWxtIiwidmlldy1pZGVudGl0eS1wcm92aWRlcnMiLCJtYW5hZ2UtaWRlbnRpdHktcHJvdmlkZXJzIiwiaW1wZXJzb25hdGlvbiIsInJlYWxtLWFkbWluIiwiY3JlYXRlLWNsaWVudCIsIm1hbmFnZS11c2VycyIsInF1ZXJ5LXJlYWxtcyIsInZpZXctYXV0aG9yaXphdGlvbiIsInF1ZXJ5LWNsaWVudHMiLCJxdWVyeS11c2VycyIsIm1hbmFnZS1ldmVudHMiLCJtYW5hZ2UtcmVhbG0iLCJ2aWV3LWV2ZW50cyIsInZpZXctdXNlcnMiLCJ2aWV3LWNsaWVudHMiLCJtYW5hZ2UtYXV0aG9yaXphdGlvbiIsIm1hbmFnZS1jbGllbnRzIiwicXVlcnktZ3JvdXBzIl19LCJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJzY29wZSI6ImVtYWlsIHByb2ZpbGUiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwibmFtZSI6IkluY2lkZW50IENvbW1hbmRlciIsInByZWZlcnJlZF91c2VybmFtZSI6ImluY2lkZW50X2NvbW1hbmRlciIsImdpdmVuX25hbWUiOiJJbmNpZGVudCIsImZhbWlseV9uYW1lIjoiQ29tbWFuZGVyIiwiZW1haWwiOiJjb21tYW5kZXJAZXhhbXBsZS5jb20ifQ.YvHnvnb8tqArQJbxm9RfZsD_pRgGrWTC1A6n4wCLt3XDC7wAAqsv5j8NIaTUFG50i9wYXe5ypMvPw3WWwrm1YG0KvVj6ENDEzRkdFhhAkK36wE-rLZa_S01QLEboWVzalugTDwgB_GwDNsKTo1NJEyJLToMoINjJukHPczu9vTT9QVi2fKMpjyz-dIm2chq5aghvfYWvez7DK1mgC7vAWS6lqDceZ5pmxNrbMhjeJWXvD4FBhww710uxFAZxaI0F5p_dAb-4dQ1dXTXOLLx01rxrdAQNgaG4fYPRszg0W3kY3T-WorRFmESs6CuiyvvX2m8rXJnCQiPDmJC6mzhYjg
----
. Open a web browser, navigate to `https://jwt.io`, and paste the contents of the `$TKN` environment variable into the left-hand debugger window to view the actual contents of the JWT:
+
image::images/jwt-token.png[]

=== Create RequestAuthentication
In this section, you create a `RequestAuthentication` resource in combination with an `AuthorizationPolicy` resource to configure end-user authentication for the Istio ingress gateway service.

. On the local file system, create a file called `ingressgateway-origin-req-auth.yml`:
+
----
$ echo "---
apiVersion: security.istio.io/v1beta1
kind: RequestAuthentication
metadata:
  name: ingressgateway-origin
spec:
  selector:
    matchLabels:
      app: istio-ingressgateway
  jwtRules:
    - issuer: https://sso-erd-sso.apps.$SUBDOMAIN_BASE/auth/realms/$ERDEMO_USER-emergency-realm
      jwksUri: https://sso-erd-sso.apps.$SUBDOMAIN_BASE/auth/realms/$ERDEMO_USER-emergency-realm/protocol/openid-connect/certs
" | tee ingressgateway-origin-req-auth.yml
----

.. Make sure that the value of the `issuer` element corresponds to the value of the `iss` field in the token.
.. Make sure that the value of the `jwksUri` element corresponds to the `issuer` URL followed by `/protocol/openid-connect/certs`.
.. The selector determines the workloads to apply the `RequestAuthentication` on. If not set, the policy will be applied to all workloads in the same namespace as the policy.

. Create the `RequestAuthentication` resource in the control plane namespace:
+
----
$ oc create -f ingressgateway-origin-req-auth.yml -n $SM_CP_NS
----

Create an `AuthorizationPolicy` resource to only allow requests to the ingress gateway if a JWT token is present in the request authorization headers, and is issued by the expected issuer:
----
$ echo "---
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: ingressgateway-origin
spec:
  selector:
    matchLabels:
      app: istio-ingressgateway
  rules:
  - from:
    - source:
        requestPrincipals:
          - https://sso-erd-sso.apps.$SUBDOMAIN_BASE/auth/realms/$ERDEMO_USER-emergency-realm/*
" | tee ingressgateway-origin-auth-policy.yml
----
+
----
$ oc create -f ingressgateway-origin-auth-policy.yml -n $SM_CP_NS
----

=== Test
With the policy in place, all requests through the ingress gateway require a valid token.

. Try to access the `/incidents` endpoint of the incident service:
+
----
$ INCIDENT_SERVICE_URL=$(oc get route incident-service-route -n $SM_CP_NS -o template --template={{.spec.host}})
$ curl -v -k https://$INCIDENT_SERVICE_URL/incidents
----
+
.Sample Output
----
*   Trying 18.159.31.177:443...
* Connected to incident-service.user1.apps.cluster-951e.951e.example.opentlc.com (18.159.31.177) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20):
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=incident-service.user1
*  start date: Nov 25 19:04:32 2020 GMT
*  expire date: Nov 25 19:04:32 2022 GMT
*  issuer: CN=incident-service.user1
*  SSL certificate verify result: self signed certificate (18), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x557bcb709b10)
> GET /incidents HTTP/2
> Host: incident-service.user1.apps.cluster-951e.951e.example.opentlc.com
> user-agent: curl/7.69.1
> accept: */*
>
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
* Connection state changed (MAX_CONCURRENT_STREAMS == 2147483647)!
< HTTP/2 403
< content-length: 19
< content-type: text/plain
< date: Fri, 27 Nov 2020 19:00:35 GMT
< server: istio-envoy
<
* Connection #0 to host incident-service.user1.apps.cluster-951e.951e.example.opentlc.com left intact
RBAC: access denied
----
* Expect to see an HTTP return code of 403, with the message "RBAC: access denied".
. Obtain a new token, and add the token as an `Authorization` header to the request:
+
----
$ TKN=$(curl -X POST "$RHSSO_URL/auth/realms/$REALM/protocol/openid-connect/token" \
 -H "Content-Type: application/x-www-form-urlencoded" \
 -d "username=incident_commander" \
 -d "password=r3dh4t1!" \
 -d "grant_type=password" \
 -d "client_id=curl" \
  --insecure \
 | sed 's/.*access_token":"//g' | sed 's/".*//g')
$ curl -v -k -H "Authorization: Bearer $TKN" https://$INCIDENT_SERVICE_URL/incidents
----
+
.Sample Output
----
*   Trying 35.156.140.248:443...
* TCP_NODELAY set
* Connected to incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com (35.156.140.248) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20):
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=*.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
*  start date: Oct 25 15:12:23 2019 GMT
*  expire date: Jan 23 15:12:23 2020 GMT
*  issuer: C=US; O=Let's Encrypt; CN=Let's Encrypt Authority X3
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x558e40afda40)
> GET /incidents HTTP/2
> Host: incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com
> User-Agent: curl/7.65.3
> Accept: */*
> Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJrLVZtbjU2SG9keWZLSjBBLTFab01ZWUxvbDJXQlNsTjNRYVBjTzFzSTNJIn0.eyJqdGkiOiI2Y2IwYjlmMC1jMzFiLTQyOTQtYWJkMi1hZWVlMTEwNWQ2ZDEiLCJleHAiOjE1NzI1NjM0NTQsIm5iZiI6MCwiaWF0IjoxNTcyNTYzMTU0LCJpc3MiOiJodHRwczovL3Nzby11c2
VyLXNzby5hcHBzLmNsdXN0ZXItYTNjNi5hM2M2LmV4YW1wbGUub3BlbnRsYy5jb20vYXV0aC9yZWFsbXMvZW1lcmdlbmN5LXJlYWxtIiwiYXVkIjoiYWNjb3VudCIsInN1YiI6ImVlNjBkNTBhLTllNGEtNDNjZi1iOWM2LWFlYzQwYjM3OWU4ZCIsInR5cCI6IkJlYXJlciIsImF6cCI6ImN1cmwiLCJhdXRoX3RpbWUiOjAsInNlc3Npb25fc3RhdGUiOiJhY2MxMzQyOC0yOGFi
LTRiOWItYWEyNi02OWNlY2UxNmFiNWQiLCJhY3IiOiIxIiwicmVhbG1fYWNjZXNzIjp7InJvbGVzIjpbIm9mZmxpbmVfYWNjZXNzIiwidW1hX2F1dGhvcml6YXRpb24iXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJzY29wZSI6ImVtYWlsIH
Byb2ZpbGUiLCJlbWFpbF92ZXJpZmllZCI6ZmFsc2UsIm5hbWUiOiJCZXJuYXJkIFRpc29uIiwicHJlZmVycmVkX3VzZXJuYW1lIjoiYnRpc29uIiwiZ2l2ZW5fbmFtZSI6IkJlcm5hcmQiLCJmYW1pbHlfbmFtZSI6IlRpc29uIiwiZW1haWwiOiJidGlzb25AcmVkaGF0LmNvbSJ9.NvT75pNsT3PdQMbYFtbsPeNx9_bdZwDYdfBOO4BU5GPvgB6z3RTIBfKJCnGpBZmpZw46PgJ
8eLljOZ3eDmZzPGStSMSBvgDndNv7AhdscmXVisGGR-WCLzDYtkYrZXjGVLIzhpLP_lTfapFflXlQWNZk1DkPsY_xuJ7dIX-vMQPurn0fyBhFcNtl0F6uHd7s02bCMqsX5EoOa0mV5lkI-RIqNjb94h12lBRVza9Go7aQ7q6NWP5LRr3RkKmdaLtz4E--L3pK4fVGCuhFAJfmQk7LndIQESs4DZvdtYYvQuT6RQEaLvm_0dh77xsKkIuPI7jXCugyAl_8ODeXEqsQcg
>
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* old SSL session ID is stale, removing
* Connection state changed (MAX_CONCURRENT_STREAMS == 4294967295)!
< HTTP/2 200
< content-type: application/json;charset=UTF-8
< date: Thu, 31 Oct 2019 23:06:09 GMT
< x-envoy-upstream-service-time: 17
< server: istio-envoy
<
* Connection #0 to host incident-service.erd.apps.cluster-a3c6.a3c6.example.opentlc.com left intact
[]
----
* Expect the request to succeed with an HTTP return code of 200.
+
[NOTE]
====
* All external traffic to the Emergency Response Demo application goes through the ingress gateway, and all requests require a valid JWT token. This means that the disaster simulator UI and the emergency console will not work as expected anymore. To fix this would require a separate ingress gateway for these services that does not require origin authentication.

* The service mesh policy only checks for a valid token based on the issuer, the validity of the signature on the token, and the token expiry time. It is possible to configure more fine-grained authorization based on claims present in the JWT token.
* The service mesh only propagates the JWT token one hop. The service proxy takes the body of the JWT token and sends it along to the application in a separate HTTP header. The JWT body is sent in the `sec-istio-auth-userinfo` header. It is the responsibility of the application to propagate the token to downstream services or resubmit for a new token based on the end user’s identity and the service’s identity.
====

. Before continuing with the remainder of the exercises, delete the `RequestAuthentication` and `AuthorizationPolicy` resources:

+
----
$ oc delete requestauthentication ingressgateway-origin -n $SM_CP_NS
$ oc delete authorizationpolicy ingressgateway-origin -n $SM_CP_NS
----

== Configure Istiod with External CA Certificates and Keys

By default, Istiod uses self-signed certificates for mTLS transport security. Istiod can also be configured to use a certificate and key from an external Certificate Authority (CA). In this exercise, you create your own CA and configure Istiod to use intermediate certificates issued and signed by the external CA.

. Create a CA root certificate and key:
.. Set the following environment variables:
+
----
$ export ROOTCA_ORG=Istio
$ export ROOTCA_CN="Root CA"
$ export ROOTCA_DAYS=3650
----
.. Create an `openssl` configuration file:
+
----
$ echo "
[ req ]
encrypt_key = no
prompt = no
utf8 = yes
default_md = sha256
default_bits = 4096
req_extensions = req_ext
x509_extensions = req_ext
distinguished_name = req_dn
[ req_ext ]
subjectKeyIdentifier = hash
basicConstraints = critical, CA:true
keyUsage = critical, digitalSignature, nonRepudiation, keyEncipherment, keyCertSign
[ req_dn ]
O = ${ROOTCA_ORG}
CN = ${ROOTCA_CN}
" | tee root-ca.conf
----
.. Generate a private key for the root CA:
+
----
$ openssl genrsa -out root-ca.key 4096
----
+
.Sample Output
----
Generating RSA private key, 4096 bit long modulus (2 primes)
...................................................................................................................
...................................................................................................................
.....++++
............++++
e is 65537 (0x010001)
----
.. Create a certificate signing request for the root CA:
+
----
$ openssl req -new -key root-ca.key -config root-ca.conf -out root-ca.csr
----
.. Generate the root CA certificate:
+
----
$ openssl x509 -req -days ${ROOTCA_DAYS} -signkey root-ca.key -extensions req_ext -extfile root-ca.conf -in root-ca.csr -out root-ca.crt
----

. Create an intermediate certificate for Istiod:
.. Create a directory for the certificate and set the following environment variables:
+
----
$ mkdir $SM_CP_ADMIN
$ export ISTIOD_ORG=Istio
$ export ISTIOD_CN="Intermediate CA"
$ export ISTIOD_DAYS=365
$ export ISTIOD_SERIAL=$RANDOM
----
.. Create an `openssl` configuration file for the intermediate certificate:
+
----
$ echo "
[ req ]
encrypt_key = no
prompt = no
utf8 = yes
default_md = sha256
default_bits = 4096
req_extensions = req_ext
x509_extensions = req_ext
distinguished_name = req_dn
[ req_ext ]
subjectKeyIdentifier = hash
basicConstraints = critical, CA:true, pathlen:0
keyUsage = critical, digitalSignature, nonRepudiation, keyEncipherment, keyCertSign
subjectAltName=@san
[ san ]
DNS.1 = istiod.$SM_CP_ADMIN-istio-system.svc
[ req_dn ]
O = ${ISTIOD_ORG}
CN = ${ISTIOD_CN}
L = ${SM_CP_ADMIN}
" | tee $SM_CP_ADMIN/intermediate.conf
----
.. Generate a private key for the intermediate certificate:
+
----
$ openssl genrsa -out $SM_CP_ADMIN/istiod-ca.key 4096
----
.. Generate a certificate signing request for the intermediate certificate:
+
----
$ openssl req -new -config $SM_CP_ADMIN/intermediate.conf -key $SM_CP_ADMIN/istiod-ca.key -out $SM_CP_ADMIN/istiod-ca.csr
----
.. Generate the intermediate certificate:
+
----
$ openssl x509 -req -days $ISTIOD_DAYS -CA root-ca.crt -CAkey root-ca.key -set_serial $ISTIOD_SERIAL -extensions req_ext -extfile $SM_CP_ADMIN/intermediate.conf -in $SM_CP_ADMIN/istiod-ca.csr -out $SM_CP_ADMIN/istiod-ca.crt
----
.. Create a certificate chain containing the intermediate certificate and the root certificate:
+
----
$ cat $SM_CP_ADMIN/istiod-ca.crt root-ca.crt > $SM_CP_ADMIN/istiod-ca-chain.crt
----

. Create a secret for the CA certificates in the service mesh control plane. The secret must be named `cacerts`. The secret contains the intermediate certificate and key, the root certificate, and the certificate chain. Note the specific names used for the certificates and keys within the secret:
+
----
$ oc create secret generic cacerts -n $SM_CP_NS --from-file=ca-cert.pem=$SM_CP_ADMIN/istiod-ca.crt --from-file=ca-key.pem=$SM_CP_ADMIN/istiod-ca.key --from-file=root-cert.pem=root-ca.crt --from-file=cert-chain.pem=$SM_CP_ADMIN/istiod-ca-chain.crt
----

. Replace the existing `istio-ca-secret` in the service mesh control plane with a secet containing the intermediate certificate and key, the root certificate, and the certificate chain:
+
----
$ oc delete secret istio-ca-secret -n $SM_CP_NS
$ oc create secret generic istio-ca-secret -n $SM_CP_NS --from-file=ca-cert.pem=$SM_CP_ADMIN/istiod-ca.crt --from-file=ca-key.pem=$SM_CP_ADMIN/istiod-ca.key --from-file=root-cert.pem=root-ca.crt --from-file=cert-chain.pem=$SM_CP_ADMIN/istiod-ca-chain.crt
----

. Configure Istiod to use the CA certificates rather than self-signed certificates by setting the `.spec.security.certificateAuthority.istiod.type` element in the `ServiceMeshControlPlane` CR to `PrivateKey`:
+
----
$ oc edit smcp -n $SM_CP_NS
----

.. Add the `.spec.security.certificateAuthority.istiod.type` element to the CR:
+
----
apiVersion: maistra.io/v1
kind: ServiceMeshControlPlane
metadata:
  [...]
spec:
  addons:
    [...]
  gateways:
    [...]
  policy:
    [...]
  profiles:
  - default
  proxy:
    [...]
  runtime:
    [...]
  security:
    dataPlane:
      automtls: false
      mtls: false
    certificateAuthority:
      istiod:
        type: PrivateKey
      type: Istiod
  [...]
----

.. Save the CR.

. Restart the istiod pod to mount the new certificates:
+
----
$ oc patch deployment istiod-full-install -p '{"spec":{"template":{"metadata":{"annotations":{"kubectl.kubernetes.io/restartedAt": "'`date +%FT%T%z`'"}}}}}' -n $SM_CP_NS
----

. Verify that the control plane `ValidatingWebHookConfiguration` uses the new certificate:
+
----
$ oc get validatingwebhookconfiguration istiod-full-install-$SM_CP_NS -o jsonpath={.webhooks[0].clientConfig.caBundle} --as=system:admin | base64 --decode | openssl x509 -noout -text
----
+
.Sample Output
----
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number: 14170 (0x375a)
        Signature Algorithm: sha256WithRSAEncryption
        Issuer: O = Istio, CN = Root CA
        Validity
            Not Before: Dec  3 18:49:36 2020 GMT
            Not After : Dec  3 18:49:36 2021 GMT
        Subject: O = Istio, CN = Intermediate CA, L = admin2
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                RSA Public-Key: (4096 bit)
                Modulus:
                    00:e4:c8:6a:eb:a0:cb:ed:a1:fc:65:53:2e:b3:03:
                    33:a5:d0:25:45:b3:09:0f:8a:52:c7:d0:10:23:f4:
                    72:6a:49:c4:bf:2f:18:12:45:0f:6b:63:a3:e6:b1:
                    3a:a7:de:88:47:5c:61:30:84:24:25:cc:93:7b:3c:
                    f8:c1:5a:fb:04:af:2d:77:cc:a8:19:78:e8:62:19:
                    76:1e:36:35:96:99:05:2b:78:84:58:cc:03:a9:7a:
                    71:99:a4:09:72:21:a4:ea:bf:e5:83:57:fe:df:91:
                    41:9e:68:f8:fd:55:fe:da:d2:e3:f1:37:d1:18:85:
                    6b:f3:a4:2b:79:b9:45:d1:48:da:32:9e:fa:02:b1:
                    f8:a3:7c:04:b0:d5:cf:73:c2:43:38:b0:8c:bc:e4:
                    53:27:69:2a:07:8d:8a:82:38:37:f6:50:61:07:9c:
                    a3:36:50:2c:0e:89:2d:66:a0:5e:39:98:1a:95:5d:
                    ac:fb:35:d2:82:dc:c7:9e:8d:29:73:b2:fe:b3:f5:
                    b9:ad:37:4e:31:7f:c6:9b:ac:cc:53:27:dc:cf:bc:
                    16:e9:a0:cc:e5:3c:5e:fd:2b:01:c8:38:4f:dd:e6:
                    16:90:7b:59:4e:64:7d:6f:65:7d:46:da:69:7a:d7:
                    3c:20:c5:f8:75:50:5e:82:ff:a5:ae:54:82:38:77:
                    af:90:12:f9:de:b4:b8:ad:17:b9:39:3e:88:06:06:
                    4a:32:d7:3d:68:f5:60:8a:e8:77:15:60:27:ca:1a:
                    84:8f:88:84:b3:2e:be:e4:14:c3:9a:e3:b7:e7:fd:
                    4c:16:4d:87:c9:eb:66:b0:d5:7a:a7:83:19:22:90:
                    34:d1:b3:5d:85:45:de:2b:d8:0d:fd:ad:2d:88:0f:
                    8d:3a:ee:93:98:92:72:cd:15:23:bb:08:57:2f:06:
                    cb:74:3a:b1:0d:31:f8:fd:6d:4f:00:ba:4b:c5:0f:
                    fc:af:ac:c0:62:bf:7f:1d:7c:6d:ea:49:73:3e:83:
                    f8:fe:85:20:3f:4a:97:e9:e3:af:43:ed:33:d0:22:
                    71:63:00:dd:f1:ff:a3:74:ab:ef:6a:06:b4:c2:33:
                    a5:ad:06:d4:3d:d6:48:1a:1d:5c:1c:97:2a:42:6f:
                    da:ef:72:6a:d2:44:84:08:1f:42:b9:08:cb:c8:a8:
                    c3:50:14:38:77:7a:28:1f:02:a6:31:32:0f:bb:3e:
                    79:65:e3:23:b0:16:fc:e2:35:11:7b:2f:21:d8:00:
                    6e:a9:d8:34:11:e1:32:73:3a:fe:f3:08:8f:fd:dd:
                    82:e1:9c:cb:95:c4:33:a3:e6:04:74:e1:93:c7:4a:
                    24:e4:0e:a0:e3:c0:f7:b1:1f:55:64:27:6f:22:3a:
                    7e:95:69
                Exponent: 65537 (0x10001)
        X509v3 extensions:
            X509v3 Subject Key Identifier:
                88:3A:C4:7D:DD:58:46:56:B1:8B:7B:40:23:97:DC:A3:15:F2:01:0E
            X509v3 Basic Constraints: critical
                CA:TRUE, pathlen:0
            X509v3 Key Usage: critical
                Digital Signature, Non Repudiation, Key Encipherment, Certificate Sign
            X509v3 Subject Alternative Name:
                DNS:istiod.admin2-istio-system.svc
    Signature Algorithm: sha256WithRSAEncryption
         9a:d4:f9:0a:ac:68:8d:c6:79:70:13:e2:eb:24:6c:62:c7:3c:
         bd:e1:a3:0b:96:1a:39:2b:bf:ce:44:4f:91:72:b0:c5:be:a1:
         a7:02:82:7b:29:ae:26:02:8b:32:e6:16:79:5f:70:3a:d5:1e:
         ac:f2:4b:a2:e9:5e:50:b3:8a:8c:55:1f:f9:cf:31:38:56:d5:
         4f:cd:5f:39:66:b3:7c:4a:11:60:c8:61:35:8e:96:9d:13:3c:
         b0:e4:84:c5:ea:8d:08:40:80:f2:27:3d:13:b9:e1:e3:09:b5:
         7f:76:2f:6a:9c:04:7a:50:c9:72:38:ba:b7:55:08:ce:dd:95:
         bb:71:54:81:cc:5d:3f:63:69:c3:dc:89:71:51:14:45:ef:1d:
         bf:80:13:db:43:08:40:66:74:06:d1:ae:ae:3c:4a:ec:2d:ae:
         9c:31:b7:e2:cd:2a:e7:9b:0b:ce:da:95:b1:8e:1a:34:71:8d:
         dc:22:18:b2:ba:5b:ab:b6:af:42:e2:83:26:8b:41:43:61:bc:
         b6:f4:9e:17:5e:01:48:59:7e:09:8e:20:16:45:f0:96:b3:0e:
         31:0c:5c:9a:70:84:dd:9f:59:57:5a:7b:39:aa:ed:dd:ae:f9:
         4f:a7:52:3c:6b:54:9b:6a:1b:b5:8d:e4:97:e6:94:33:cb:66:
         f1:42:ed:a8:cb:90:39:81:4c:cf:bc:1c:aa:82:35:a4:6e:44:
         4b:e3:dc:4e:31:e1:71:cf:aa:4d:df:6b:b2:4f:a6:a2:d5:a9:
         ba:30:67:fe:93:f3:c4:c0:68:ac:96:fc:de:83:8a:9e:27:86:
         4c:89:df:1a:1d:c1:1c:38:79:52:37:61:a2:5e:db:1d:8b:19:
         0b:20:33:52:57:bf:a9:e8:aa:ae:bf:3b:9f:b3:eb:dd:db:cf:
         c9:f0:9f:62:53:cc:fd:58:88:17:81:a9:17:ab:f6:d1:93:f4:
         ec:9b:0a:42:60:e2:e4:38:d2:db:a2:6e:1c:bc:2a:3a:d9:c1:
         6c:ab:91:e0:db:92:35:d3:6c:11:9c:39:f8:0a:ce:69:a5:ee:
         ed:9c:61:fc:a5:58:09:4f:a6:b8:47:63:d4:07:ab:55:f3:da:
         9e:f7:1e:1a:59:5f:e2:dc:57:96:cf:64:eb:d8:7c:d3:c3:98:
         68:60:7d:d7:a2:0d:5c:42:7b:fb:0f:c1:83:b0:40:e1:dc:12:
         63:c4:db:97:c2:a8:b9:28:22:00:b4:6a:50:c8:84:74:4b:f2:
         a1:08:e6:ac:91:5a:0c:28:37:e6:34:59:3a:a9:51:01:09:10:
         21:ca:46:7e:2e:57:af:3e:b2:16:52:72:be:22:66:fe:59:e3:
         21:0a:5f:eb:16:07:4b:85
----
* Expect the Subject and the Issuer to match the custom certificates you created.

. Redeploy the incident service pod for the proxy to use the custom certificate:
+
----
$ oc rollout latest dc/$ERDEMO_USER-incident-service -n $ERDEMO_NS
----

. Verify the certificate used by the incident service pod proxy:
+
----
$ INCIDENT_SERVICE_POD=$(oc get pod -l app=$ERDEMO_USER-incident-service -o jsonpath={.items[0].metadata.name} -n $ERDEMO_NS)
$ istioctl proxy-config secret $INCIDENT_SERVICE_POD -n $ERDEMO_NS -i $SM_CP_NS -o json | jq '[.dynamicActiveSecrets[] | select(.name == "default")][0].secret.tlsCertificate.certificateChain.inlineBytes' -r | base64 -d | openssl x509 -noout -text
----
+
.Sample Output
----
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number:
            d4:0e:05:f1:a8:4c:a3:c1:35:ed:93:af:0c:62:d6:04
        Signature Algorithm: sha256WithRSAEncryption
        Issuer: O = Istio, CN = Intermediate CA, L = admin1
        Validity
            Not Before: Dec  3 19:13:37 2020 GMT
            Not After : Dec  4 19:13:37 2020 GMT
        Subject:
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                RSA Public-Key: (2048 bit)
                Modulus:
                    00:9e:13:55:7e:35:1a:9b:c2:7b:15:6e:15:38:93:
                    f9:b8:25:cf:be:1f:c5:2d:e1:a4:40:85:15:33:73:
                    54:9d:93:5a:0c:7d:56:02:e8:fa:12:6f:62:da:02:
                    fa:c9:a1:2a:c2:75:7e:b6:06:b3:04:16:47:70:c8:
                    a3:82:f2:f6:80:c7:ae:1a:bf:92:72:91:15:a7:55:
                    95:f6:cf:5b:3d:6f:c0:52:77:2a:bf:1f:1c:60:c0:
                    ec:68:08:a5:a1:d4:02:45:e3:ec:e8:8d:1d:0e:c3:
                    c7:ea:f1:79:44:3d:72:e6:87:3e:3d:4c:8e:aa:4b:
                    e2:3a:44:85:29:ee:2a:0a:c8:17:21:e5:4b:73:1d:
                    04:7d:3e:db:97:b1:70:74:1c:e9:3a:e8:5a:2e:e0:
                    82:58:76:bf:ad:4e:13:a8:99:a2:78:2d:34:73:b8:
                    2f:81:ea:da:93:b9:16:0b:ba:6e:30:3d:82:47:3b:
                    19:bb:43:63:5a:19:67:6c:4e:06:af:3d:21:17:fc:
                    d1:82:e1:70:af:ce:92:d9:76:e3:b2:9a:e4:64:23:
                    f1:66:41:08:8c:79:30:a5:ed:b7:fc:fe:88:ae:1d:
                    4b:af:be:5d:60:4b:4b:33:ed:ca:c1:f3:cc:c2:19:
                    3c:78:83:94:48:4b:9d:31:1d:9a:74:48:8c:82:0b:
                    b8:f3
                Exponent: 65537 (0x10001)
        X509v3 extensions:
            X509v3 Key Usage: critical
                Digital Signature, Key Encipherment
            X509v3 Extended Key Usage:
                TLS Web Server Authentication, TLS Web Client Authentication
            X509v3 Basic Constraints: critical
                CA:FALSE
            X509v3 Authority Key Identifier:
                keyid:88:3A:C4:7D:DD:58:46:56:B1:8B:7B:40:23:97:DC:A3:15:F2:01:0E

            X509v3 Subject Alternative Name: critical
                URI:spiffe://cluster.local/ns/user2-er-demo/sa/incident-service
    Signature Algorithm: sha256WithRSAEncryption
         ac:6f:df:fc:3f:5a:b9:f1:92:12:2d:1b:70:7c:e7:e6:25:32:
         76:1c:fb:f8:fa:83:bd:e6:2e:a3:4d:9c:65:49:5d:c0:98:87:
         cb:a3:56:3b:5b:87:4b:5e:34:7d:c9:2e:aa:ab:8d:59:35:ec:
         de:51:19:4e:28:f7:52:ea:4a:c8:11:d9:f8:fa:a2:68:c0:2d:
         b1:a6:e3:d7:ef:2c:2c:90:cd:a2:35:a6:b0:42:d0:c5:33:b3:
         af:e2:f7:32:71:7c:af:bc:df:46:af:77:c0:05:0b:88:fe:91:
         a7:c1:3a:a6:67:86:af:08:5d:63:3a:b1:86:1e:5d:7d:de:a9:
         88:d6:8a:7c:5d:04:bd:a3:2e:49:1b:44:5f:01:45:5a:85:69:
         84:e3:fa:6f:c8:9c:68:3d:d1:5d:94:46:36:0f:0c:68:cd:e3:
         04:93:87:d0:19:06:73:91:b6:07:48:1a:2a:a0:e2:36:11:70:
         62:9c:c7:93:fb:02:99:42:e2:35:e6:f3:3a:e6:15:a5:a8:0a:
         32:d7:be:10:32:1f:6d:67:72:93:db:7b:df:4b:94:1a:81:32:
         7b:55:d4:18:ab:fd:e7:7e:2b:72:db:e8:13:98:3e:46:6a:57:
         9e:98:09:9c:63:36:37:fb:ad:4a:27:2e:b0:60:85:94:00:df:
         fa:7d:37:a1:b4:df:6d:37:45:8a:3a:ff:83:6e:42:b0:9a:61:
         f2:73:a5:d8:0c:97:da:c6:84:97:97:04:b5:a4:e9:03:e9:48:
         a6:17:02:e5:ee:b1:e0:d1:7d:31:14:13:97:c1:2a:ab:20:c7:
         c3:d8:78:53:11:96:79:86:d1:0b:e7:0a:b2:88:a9:9f:09:d1:
         d0:3d:35:51:d1:e2:57:59:dd:9a:ae:81:77:d1:be:21:9b:b1:
         aa:61:89:b7:be:95:57:89:5f:a7:d9:a9:d9:04:1f:dd:94:47:
         8b:29:a3:49:48:dd:74:79:74:19:0e:df:4f:1a:da:bd:e9:4d:
         11:a0:91:2a:08:ea:68:96:d6:7e:8d:53:92:1c:37:62:24:08:
         cc:b6:3d:a8:95:34:8f:cc:58:35:ec:fc:e4:9e:74:ca:6a:82:
         98:13:0d:98:e9:2d:dc:b1:a7:ad:c7:f1:9f:01:92:cb:d6:c4:
         a1:e2:20:f2:af:7a:65:52:5a:fc:11:be:dc:e9:12:84:e2:4e:
         49:22:10:95:46:af:f7:9c:19:33:ff:27:61:fb:9d:83:01:64:
         de:cc:07:7d:f4:53:aa:a9:f5:d6:07:99:41:e2:41:63:b9:2c:
         e1:96:35:02:11:97:93:ae:33:5d:a5:0a:ea:88:e0:87:02:c5:
         9a:2e:51:20:05:80:07:8a
----
* Expect the Issuer to match the custom intermediate certificate.

. Restart the other application services in the data plane to have the Envoy proxies request new certificates:
+
----
$ for dc in disaster-simulator emergency-console incident-priority-service mission-service process-service process-viewer responder-service responder-simulator
do
  oc rollout latest dc/$ERDEMO_USER-$dc -n $ERDEMO_NS
done
----
+
.Sample Output
----
deploymentconfig.apps.openshift.io/user1-disaster-simulator rolled out
deploymentconfig.apps.openshift.io/user1-emergency-console rolled out
deploymentconfig.apps.openshift.io/user1-incident-priority-service rolled out
deploymentconfig.apps.openshift.io/user1-mission-service rolled out
deploymentconfig.apps.openshift.io/user1-process-service rolled out
deploymentconfig.apps.openshift.io/user1-process-viewer rolled out
deploymentconfig.apps.openshift.io/user1-responder-service rolled out
deploymentconfig.apps.openshift.io/user1-responder-simulator rolled out
----
+
[NOTE]
If you injected the PostgreSQL pods with the Envoy proxy, make sure to restart them too.

. Restart the control plane ingress and egress pods:
+
----
$ oc patch deployment istio-ingressgateway -p '{"spec":{"template":{"metadata":{"annotations":{"kubectl.kubernetes.io/restartedAt": "'`date +%FT%T%z`'"}}}}}' -n $SM_CP_NS
$ oc patch deployment istio-egressgateway -p '{"spec":{"template":{"metadata":{"annotations":{"kubectl.kubernetes.io/restartedAt": "'`date +%FT%T%z`'"}}}}}' -n $SM_CP_NS
----

. Once all the service pods are back up, test the Emergency Response Demo application to make sure the application is working as expected.

This concludes the lab. You have successfully enabled mTLS and authorization policies and configured service mesh origin authentication.