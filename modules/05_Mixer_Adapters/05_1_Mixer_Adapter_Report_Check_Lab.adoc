:noaudio:
:scrollbar:
:toc2:
:linkattrs:
:data-uri:

== Mixer Adapter Report and Check Lab

.Goals
* Understand service mesh Mixer adapters
* Create and deploy custom resources to configure built-in Mixer adapters

.Overview

Mixer is the Red Hat^(R)^ OpenShift^(R)^ Service Mesh component responsible for policy checks and telemetry reporting. Mixer interfaces with infrastructure back ends--access control systems, telemetry capturing systems, and quota enforcement systems--through adapters.

Adapters fall into two categories--check or report--depending on whether they are involved in policy enforcement or telemetry reporting. Policy checks happen in the service proxy before a request is transmitted to the target service. Telemetry data is sent by the service proxy to Mixer after a request to the target service.

Mixer comes with a set of built-in adapters for a range of back-end systems such as Prometheus, as well as a number of adapters that are self-contained.
Configuring a Mixer adapter requires a combination of `instance`, `handler`, and `rule` resources.

In this lab, you explore some of the built-in Mixer adapters for telemetry reporting and policy checks.

:numbered:
== Explore Stdio Mixer Adapter

The `stdio` adapter enables Istio to output logs and metrics to the local machine. Logs and metrics can be directed to Mixer’s standard output or error stream or to any locally reachable file. When outputting to files, you can enable file rotation so that the adapter automatically manages a set of file backups as data is generated.
The logs can also be captured by a FluentD daemon for log aggregation.

This adapter supports the `logentry` template and the `metric` template.

[NOTE]
OpenShift Service Mesh also comes with a FluentD Mixer adapter, which allows you to deliver logs directly to a FluentD daemon.

In this section of the lab, you configure the `stdio` Mixer adapter to log HTTP requests to the services of the Emergency Response Demo application.

. Log in to OpenShift Container Platform as the control plane admin user:
+
----
$ oc login $LAB_MASTER_API -u $SM_CP_ADMIN -p $OCP_PASSWD
----

. Create an `instance` CR in the Emergency Response Demo namespace:
+
----
$ echo "---
apiVersion: config.istio.io/v1alpha2
kind: instance
metadata:
  name: access-log
spec:
  compiledTemplate: logentry
  params:
    severity: '\"info\"'
    timestamp: request.time
    variables:
      method: request.method | \"\"
      host: request.host | \"\"
      sourceIp: source.ip | ip(\"0.0.0.0\")
      responseSize: response.size | 0
      forwardedFor: 'request.headers[\"x-forwarded-for\"] | \"unknown\"'
      user: source.user | \"unknown\"
      userAgent: 'request.headers[\"user-agent\"] | \"unknown\"'
      destination: 'destination.labels[\"app\"] | destination.workload.name | \"unknown\"'
      responseCode: response.code | 0
      url: request.path | \"\"
      protocol: context.protocol | \"\"
      source: 'source.labels[\"app\"] | source.workload.name | \"unknown\"'
      latency: response.duration | \"0ms\"
    monitored_resource_type: '\"UNSPECIFIED\"'
" | oc create -f - -n $ERDEMO_NS
----
* The `instance` resource uses the `logentry` template, which is one of the templates supported by the `stdio` adapter.
* The `logentry` template supports the following parameters:
+
[cols="1,1,3",options="header"]
|====
|Parameter  | Type  | Description
| `severity` | `string` | Indicates the importance of the log entry.
| `timestamp` | `TimeStamp` | Time value for the log value. Set to the `request.time` attribute, which represents the timestamp when the destination receives the request, as provided by Envoy.
| `variables` |  `map<string, Value>` | Each element in the map becomes an entry in the produced log entry. The elements are typically populated with attributes provided by Envoy.
|====
+
[TIP]
Refer to the Istio upstream documentation for the complete list of supported parameters.

. Create a `handler` CR in the Emergency Response Demo namespace:
+
----
$ echo "---
apiVersion: config.istio.io/v1alpha2
kind: handler
metadata:
  name: access-log-handler
spec:
  compiledAdapter: stdio
  params:
    severity_levels:
      info: 0 # Params.Level.INFO
    outputAsJson: true
" | oc create -f - -n $ERDEMO_NS
----
+
* The handler references the built-in `stdio` adapter.
* The `stdio` adapter is configured with the following parameters:
+
[cols="1,1,3",options="header"]
|====
|Parameter  | Type  | Description
| `severity_levels` | `map<string, Level>` | In this example, the `info` level defined in the instance resource is mapped to the `INFO` level supported by the adapter.
| `outputAsJson`| `bool` | Directs the adapter to generate JSON-formatted log lines.
|====

. Create a `rule` CR in the Emergency Response Demo namespace:
+
----
$ echo "---
apiVersion: config.istio.io/v1alpha2
kind: rule
metadata:
  name: access-log-stdio
spec:
  match: context.protocol == \"http\" # match for http requests
  actions:
   - handler: access-log-handler
     instances:
     - access-log
" | oc create -f - -n $ERDEMO_NS
----
* The rule directs Mixer to send all `access-log` instances to the `access-log-handler` handler.
* The match parameter predicate evaluates to true if the protocol of the request is HTTP, so the rule is executed for all HTTP requests.

. In a browser window, navigate to the disaster simulator application at `https://disaster-simulator.$ERDEMO_USER.apps.$SUBDOMAIN_BASE`.
. Click the *Clear Incidents* button a few times.
* This causes a REST call to the incident service and the incident priority service.

. View the logs of the `istio-telemetry` pod in the service mesh control plane:
+
----
oc logs -f $(oc get pods -n $SM_CP_NS | grep Running | \
  grep istio-telemetry | awk '{ print $1 }') \
  -c mixer -n $SM_CP_NS
----

* Expect to see entries similar to this:
+
----
{"level":"info","time":"2020-01-24T11:46:01.266021Z","instance":"access-log.instance.user1-er-demo","destination":"user1-disaster-simulator","forwardedFor":"10.128.2.115","host":"disaster-simulator.user1.apps.cluster-44e5.44e5.example.opentlc.com","latency":"967.171µs","method":"GET","protocol":"http","responseCode":200,"responseSize":107,"source":"istio-ingressgateway","sourceIp":"10.131.0.167","url":"/c/incidents?clearIncidents=true","user":"cluster.local/ns/admin1-istio-system/sa/istio-ingressgateway-service-account","userAgent":"Mozilla/5.0 (X11; Fedora; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0"}
{"level":"info","time":"2020-01-24T11:46:01.274869Z","instance":"access-log.instance.user1-er-demo","destination":"user1-incident-priority-service","forwardedFor":"unknown","host":"user1-incident-priority-service.user1-er-demo.svc:8080","latency":"1.575412ms","method":"POST","protocol":"http","responseCode":200,"responseSize":0,"source":"user1-disaster-simulator","sourceIp":"10.128.2.123","url":"/reset","user":"cluster.local/ns/user1-er-demo/sa/disaster-simulator-service","userAgent":"Vert.x-WebClient/3.7.0"}
{"level":"info","time":"2020-01-24T11:46:01.274493Z","instance":"access-log.instance.user1-er-demo","destination":"user1-incident-priority-service","forwardedFor":"unknown","host":"user1-incident-priority-service.user1-er-demo.svc:8080","latency":"2.298198ms","method":"POST","protocol":"http","responseCode":200,"responseSize":0,"source":"user1-disaster-simulator","sourceIp":"10.128.2.123","url":"/reset","user":"unknown","userAgent":"Vert.x-WebClient/3.7.0"}
{"level":"info","time":"2020-01-24T11:46:01.273916Z","instance":"access-log.instance.user1-er-demo","destination":"user1-incident-service","forwardedFor":"unknown","host":"user1-incident-service.user1-er-demo.svc:8080","latency":"7.231309ms","method":"POST","protocol":"http","responseCode":200,"responseSize":0,"source":"user1-disaster-simulator","sourceIp":"10.128.2.123","url":"/incidents/reset","user":"unknown","userAgent":"Vert.x-WebClient/3.7.0"}
{"level":"info","time":"2020-01-24T11:46:01.275598Z","instance":"access-log.instance.user1-er-demo","destination":"user1-incident-service","forwardedFor":"unknown","host":"user1-incident-service.user1-er-demo.svc:8080","latency":"5.8609ms","method":"POST","protocol":"http","responseCode":200,"responseSize":0,"source":"user1-disaster-simulator","sourceIp":"10.128.2.123","url":"/incidents/reset","user":"cluster.local/ns/user1-er-demo/sa/disaster-simulator-service","userAgent":"Vert.x-WebClient/3.7.0"}
----

. To see log entries from requests coming from outside the mesh, execute some `curl` requests against the incident service:
+
----
$ curl -k https://incident-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/incidents
----

. View the logs of the `istio-telemetry` pod:
+
----
{"level":"info","time":"2020-01-24T11:48:13.715957Z","instance":"access-log.instance.user1-er-demo","destination":"user1-incident-service","forwardedFor":"10.131.0.172","host":"incident-service.user1.apps.cluster-44e5.44e5.example.opentlc.com","latency":"19.43911ms","method":"GET","protocol":"http","responseCode":200,"responseSize":2,"source":"istio-ingressgateway","sourceIp":"10.131.0.167","url":"/incidents","user":"cluster.local/ns/admin1-istio-system/sa/istio-ingressgateway-service-account","userAgent":"curl/7.66.0"}
----

== Explore Memquota Mixer Adapter

The OpenShift Service Mesh built-in `memquota` and `redisquota` adapters can be used to support the service mesh quota management system and to rate limit traffic to services based on, for example, the originating IP address.
The `memquota` adapter stores the quota values in memory in the Mixer pod, and as such is not suitable for production usage. The `redisquota` adapter relies on a Redis server to store the quota values. In this exercise, you use the `memquota` adapter.

Rate limit configuration is split into two parts:

* Client-side configuration:
** `QuotaSpec`: Defines the quota name and the amount that the client should request.
** `QuotaSpecBinding`: Associates `QuotaSpec` with one or more services.
* Mixer-side configuration: A combination of `instance`, `handler`, and `rule` resources.

In this exercise, you introduce a rate limit for calls to the incident service coming from outside the OpenShift Container Platform cluster.

. By default, policy checks are disabled in the service mesh. Use the `oc` client to edit the `servicemeshcontrolplane` resource in the control plane namespace to enable policy checks:
+
----
$ oc edit servicemeshcontrolplane.v1.maistra.io -n $SM_CP_NS
----

. In the `global` section, change the value of `disablePolicyChecks` to `false`, and then save your changes:
+
----
    global:
      disablePolicyChecks: false
----

. Create the `instance` CR in the service mesh control plane:
+
----
$ echo "---
apiVersion: config.istio.io/v1alpha2
kind: instance
metadata:
  name: requestcountquota
spec:
  compiledTemplate: quota
  params:
    dimensions:
      sourceIp: 'request.headers[\"x-forwarded-for\"] | \"unknown\"'
      source: 'source.labels[\"app\"] | source.workload.name | \"unknown\"'
      destination: 'destination.labels[\"app\"] | destination.service.name | \"unknown\"'
" | oc create -f - -n $SM_CP_NS
----
* The instance references the `quota` template.
* The `dimensions` parameter defines a set of criteria against which quota can be defined.

. Create the `handler` CR in the control plane namespace:
+
----
$ echo "---
apiVersion: config.istio.io/v1alpha2
kind: handler
metadata:
  name: quotahandler
spec:
  compiledAdapter: memquota
  params:
    quotas:
    - name: requestcountquota.instance.$SM_CP_NS
      maxAmount: 500
      validDuration: 1s
      overrides:
      # The following override applies to 'incident-service' if called from outside the mesh
      - dimensions:
          source: istio-ingressgateway
          destination: $ERDEMO_USER-incident-service
        maxAmount: 1
        validDuration: 5s
" | oc create -f - -n $SM_CP_NS
----
* The handler references the built-in `memquota` adapter.
* The handler defines two rate-limit schemes.
** The default, if no overrides match, is 500 requests per one second.
** One override is defined: If the request has the service mesh ingress gateway as the source and the incident service as the target, the request rate is limited to one request every five seconds. 
* When a request is processed, the first matching override is picked (reading from top to bottom).

. Create the `rule` CR in the control plane namespace:
+
----
$ echo "---
apiVersion: config.istio.io/v1alpha2
kind: rule
metadata:
  name: quota
spec:
  actions:
  - handler: quotahandler
    instances:
    - requestcountquota
" | oc create -f - -n $SM_CP_NS
----
* The rule matches the handler with the instance. There is no match element in the rule, so it matches all requests.

. Create the client-side `QuotaSpec` CR:
+
----
$ echo "---
apiVersion: config.istio.io/v1alpha2
kind: QuotaSpec
metadata:
  name: request-count
spec:
  rules:
  - quotas:
    - charge: 1
      quota: requestcountquota
" | oc create -f - -n $SM_CP_NS
----
* The `QuotaSpec` CR references the `requestcountquota` instance, and sets a charge of one per instance.

. Create the client-side `QuotaSpecBinding` CR:
+
----
$ echo "---
apiVersion: config.istio.io/v1alpha2
kind: QuotaSpecBinding
metadata:
  name: request-count
spec:
  quotaSpecs:
  - name: request-count
    namespace: $SM_CP_NS
  services:
  - name: $ERDEMO_USER-incident-service
    namespace: $ERDEMO_NS
" | oc create -f - -n $SM_CP_NS
----
* The `QuotaSpecBinding` CR binds the `QuotaSpec` CR to the incident service in the Emergency Response Demo namespace.

. Using `curl`, execute a series of calls to the incident service. Note that after a couple of calls, the quota is exceeded and the service returns an error message:
+
----
$ curl -k https://incident-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/incidents
[]
$ curl -k https://incident-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/incidents
[]
$ curl -k https://incident-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/incidents
RESOURCE_EXHAUSTED:Quota is exhausted for: requestcountquota
$ curl -k https://incident-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/incidents
RESOURCE_EXHAUSTED:Quota is exhausted for: requestcountquota
----

. Using `curl -v`, verify that the response code is equal to `429 Too Many Requests`.

== Explore Denier Mixer Adapter

The service mesh's built-in `denier`, `whitelist`, and `blacklist` adapters allow you to control access to a service using simple denials, attribute-based white or black listing, or IP-based white or black listing.

In this exercise, you use the `denier` adapter to prevent access to the incident service when using `curl` from outside the mesh.

. Create the `instance` CR:
+
----
$ echo "---
apiVersion: "config.istio.io/v1alpha2"
kind: instance
metadata:
  name: deny-curl
spec:
  compiledTemplate: checknothing
" | oc create -f - -n $SM_CP_NS
----
* The instance references the `checknothing` template, one of the templates supported by the `denier` adapter.
* CheckNothing represents an empty block of data that is used for policy-checking adapters that do not require any parameters.

. Create the `handler` CR:
+
----
$ echo "---
apiVersion: config.istio.io/v1alpha2
kind: handler
metadata:
  name: deny-curl-handler
spec:
  compiledAdapter: denier
  params:
    status:
      code: 7 # google.rpc.Code enum "PERMISSION_DENIED"
      message: not allowed
" | oc create -f - -n $SM_CP_NS
----
* The handler references the `denier` adapter. The `status` parameter of the adapter allows you to specify the return code and error message returned in case of a denial.

. Create the `rule` CR:
+
----
$ echo "---
apiVersion: config.istio.io/v1alpha2
kind: rule
metadata:
  name: incident-service-deny-curl
spec:
  match: match(request.headers[\"user-agent\"], \"curl*\") && source.labels[\"istio\"] == \"ingressgateway\" && destination.labels[\"app\"] == \"$ERDEMO_USER-incident-service\"
  actions:
   - handler: deny-curl-handler
     instances:
     - deny-curl
" | oc create -f - -n $SM_CP_NS
----
* The rule matches requests to the incident service that originate from the ingress gateway and that have a `user-agent` header with a value that starts with `curl`.

. Using `curl`, execute a call to the incident service:
+
----
$ curl -k -v https://incident-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/incidents
----
+
.Sample Output
----
*   Trying 35.156.184.150:80...
* TCP_NODELAY set
* Connected to incident-service.user1.apps.cluster-44e5.44e5.example.opentlc.com (35.156.184.150) port 80 (#0)
> GET /incidents HTTP/1.1
> Host: incident-service.user1.apps.cluster-44e5.44e5.example.opentlc.com
> User-Agent: curl/7.66.0
> Accept: */*
>
* Mark bundle as not supporting multiuse
< HTTP/1.1 403 Forbidden
< content-length: 67
< content-type: text/plain
< date: Thu, 23 Jan 2020 08:21:53 GMT
< server: istio-envoy
< x-envoy-upstream-service-time: 2
* Added cookie 61eaba591e4e09ee0032a9ddba4ed948="d0600b67ba8b55b0c485b915d38335c1" for domain incident-service.user1.apps.cluster-44e5.44e5.example.opentlc.com, path /, expire 0
< Set-Cookie: 61eaba591e4e09ee0032a9ddba4ed948=d0600b67ba8b55b0c485b915d38335c1; path=/; HttpOnly
<
* Connection #0 to host incident-service.user1.apps.cluster-44e5.44e5.example.opentlc.com left intact
PERMISSION_DENIED:deny-curl-handler.admin1-istio-system:not allowed
----
* Note that the call fails with a `403 Forbidden` return code and a `not allowed` message.

. Obtain a remote shell (`oc rsh`) in the the disaster simulator pod and perform a `curl` request to the incident service.
+
----
oc -n $ERDEMO_NS rsh -c $ERDEMO_USER-disaster-simulator \
    `oc get pod -n $ERDEMO_NS | grep "disaster-simulator" | grep "Running" | awk '{print $1}'` \
    curl -v http://$ERDEMO_USER-incident-service.$ERDEMO_USER-er-demo.svc:8080/incidents
----

* Verify the requests returns a successful 200 response code.

This concludes the lab. You learned about a few of the built-in Mixer adapters and created and deployed custom resources to configure them. To learn more, see this list of link:https://istio.io/docs/reference/config/policy-and-telemetry/adapters/[built-in Mixer adapters].
