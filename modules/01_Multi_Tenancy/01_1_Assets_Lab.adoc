:noaudio:
:scrollbar:
:toc2:
:linkattrs:
:data-uri:

== Assets and Tooling Lab

*IMPORTANT: This course is intended for use in an instructor-led classroom environment. While you can access the content online and attempt to perform the labs, Red Hat does not provide instructor support for use of this course outside of an instructor-led classroom environment.*


.Goals
* Review lab environment
* Review preinstalled Red Hat^(R)^ OpenShift^(R)^ Service Mesh _operator_
* Review preinstalled OpenShift Service Mesh _control plane_
* Review the three user roles typically involved in an OpenShift Service Mesh-managed environment
* Become familiar with Emergency Response Demo

:numbered:

== Set Up Workstation

In the labs for this course, you use the command line for many of the tasks. 
The labs assume that you are using the proper versions of these tools, all of which can be found on the provided Student VM.

. Go to the link:https://labs.opentlc.com/[OPENTLC lab portal^] and use your OPENTLC credentials to log in.
. Navigate to *Services -> Catalogs -> All Services -> OPENTLC OpenShift 4 Labs*.
. On the left, select *OpenShift 4 Student VM*.
. On the right, click *Order*.
. On the *Lab Information* Tab make the following selections:
.. *4.5* for *OpenShift Release*
.. The region closest to you for *Region* (North America, EMEA, APAC or LATAM)
. On the bottom right, click *Submit*.
+
[IMPORTANT]
Do not select *App Control -> Start* after ordering the client VM. This is not necessary in this environment. Selecting *Start* may corrupt the lab environment or cause other complications.

* After a few minutes, expect to receive an email with instructions on how to connect to the environment.

. Read the email carefully and specifically note the following:
* The hostname for your Student VM.
* The username and password for your Student VM

=== Start Client VM After Shut Down

To conserve resources, the Student VM shuts down automatically after eight hours. 
In this section, you restart the Student VM for this course after it has shut down automatically.

. Go to the link:https://labs.opentlc.com/[OPENTLC lab portal^] and use your OPENTLC credentials to log in.
. Navigate to *Services -> My Services* (this should be the screen shown right after logging in).
. In the list of your services, select your *OpenShift 4 Student VM*.
. Select *App Control -> Start* to start your *OpenShift 4 Student VM*.
. Select *Yes* at the *Are you sure?* prompt.
. On the bottom right, click *Submit*.

After a few minutes, expect to receive an email letting you know that the Student VM has been started.

=== Set Environment Variables

. Ask your instructor to share the lab details spreadsheet:
+
image::images/lab_details_spreadsheet.png[]

. Open a terminal window on your local machine and download the script that sets the environment variables required for the labs in this course:
+
-----
$ mkdir -p $HOME/lab

$ curl https://raw.githubusercontent.com/gpe-mw-training/ocp_service_mesh_advanced/master/utils/set_env_vars.sh \
    -o $HOME/lab/set_env_vars.sh && \
    chmod 775 $HOME/lab/set_env_vars.sh
-----

. Using your favorite text editor, modify the environment variables in this downloaded shell script.
* Refer to the lab details spreadsheet shared by your instructor.

. Execute the shell script so that the environment variables are applied to your existing shell:
+
-----
$ $HOME/lab/set_env_vars.sh
-----

. Ensure that the environment variables are set correctly.
+
----
$ echo $GW_PROJECT
----
+
.Sample Output
----
user2-gw
----
+
NOTE: If you see empty output, make sure you source your `.bashrc` file using `source $HOME/.bashrc`.

== Review OpenShift Container Platform Assets

Your lab is built on a shared OpenShift Container Platform 4 cluster in the cloud.

=== Explore OpenShift User: `$ERDEMO_USER`

This user is an administrator of your microservice-architected business application used for this course--the _Emergency Response Demo_ application.
In addition, this user has view access to a variety of other namespaces.

. At the terminal, authenticate into OpenShift Container Platform as the `$ERDEMO_USER` user:
+
-----
$ oc login $LAB_MASTER_API -u $ERDEMO_USER -p $OCP_PASSWD
-----

. View a listing of OpenShift projects that you have access to:
+
-----
$ oc get projects
-----
+
.Sample Output
-----
NAME                  DISPLAY NAME                        STATUS
3scale-mt-api0        3scale-mt-api0                      Active
admin50-istio-system  admin50 Service Mesh Control Plane  Active
istio-operator        istio-operator                      Active
kafka-operator-erd                                        Active
tools-erd                                                 Active
user-sso                                                  Active
user50-er-demo                                            Active
user50-gw                                                 Active
-----

* Subsequent sections of this lab introduce you to each of these namespaces.

=== Explore OpenShift User: `$SM_CP_ADMIN`

This user is your OpenShift Service Mesh control plane administrator.
In addition, this user has view access to a variety of other namespaces.

. At the terminal, authenticate into OpenShift Container Platform as the `$SM_CP_ADMIN` user:
+
-----
$ oc login $LAB_MASTER_API -u $SM_CP_ADMIN -p $OCP_PASSWD
-----

. View a listing of OpenShift projects that you have access to:
+
-----
$ oc get projects
-----
+
.Sample Output
-----
NAME                  DISPLAY NAME                        STATUS
3scale-mt-api0        3scale-mt-api0                      Active
admin50-istio-system  admin50 Service Mesh Control Plane  Active
istio-operator        istio-operator                      Active
user50-er-demo                                            Active
-----

=== Access OpenShift Container Platform Web Console

. At the terminal, determine the URL of the OpenShift Container Platform web console:
+
-----
$ oc whoami --show-console
-----

. Open a browser and navigate to the URL revealed in the previous step.
. Authenticate using the values of `$ERDEMO_USER` and `$OCP_PASSWD`.

== Review OpenShift Service Mesh Resources: `cluster-admin`

OpenShift Service Mesh implements _soft multi-tenancy_ that provides a three-tier RBAC model comprising the roles of `cluster-admin`, `mesh-admin`, and `namespace-admin`.

In this section of the lab, you review the OpenShift Service Mesh resources owned by an OpenShift user with the `cluster-admin` role--in this case, your instructor.

=== Explore Operator

. Switch to the administrator of your Service Mesh control plane (who also has view access to the `istio-operator` namespace):
+
-----
$ oc login -u $SM_CP_ADMIN -p $OCP_PASSWD
-----

. View the previously installed Service Mesh operator found in the `istio-operator` namespace:
+
-----
$ oc get deploy istio-operator -n istio-operator
-----
+
.Sample Output
-----
NAME             READY   UP-TO-DATE   AVAILABLE   AGE
istio-operator   1/1     1            1           15h
-----

* This operator is global in scope to the OpenShift cluster.
* The administrator of the `istio-operator` namespace is the OpenShift user with `cluster-admin` rights.


=== Explore Istio CNI Plug-in

When the link:https://istio.io/[Istio] community project injects the _Envoy_ service proxy sidecar into an application pod, it typically uses link:https://kubernetes.io/docs/concepts/workloads/pods/init-containers/[init containers] to manipulate the iptables rules of the OpenShift node where the application pod runs.
It modifies these iptables in order to intercept requests to application containers.
Although the Envoy service proxy sidecar does not require `root` to run, this short-lived `init-container` container does require link:https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities[`cap_net_admin` privileges].
This use of `init container` in each application pod with elevated `cap_net_admin` privileges is a security vulnerability.

OpenShift Service Mesh avoids this approach.
Instead, it makes use of the link:https://istio.io/docs/setup/additional-setup/cni/[`istio-cni` plug-in].
This plug-in is an implementation of the link:https://github.com/containernetworking/cni[Linux container network interface] specification.
It is responsible for manipulating iptables routing rules on a pod injected with the Envoy sidecar container.

The `istio-cni` plug-in still runs with elevated privileges.
Subsequently, it is implemented as a DaemonSet in the `istio-operator` namespace, which is typically owned by the OpenShift user with `cluster-admin` privileges.


. View the previously installed `istio-cni` pods implemented as an OpenShift DaemonSet in the `istio-operator` namespace:
+
-----
$ oc get daemonset istio-node -n istio-operator
-----
+
.Sample Output
-----
NAME         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                 AGE

istio-node   12        12        12      12           12          beta.kubernetes.io/os=linux   4d21h
-----

* As a DaemonSet, an Istio CNI pod runs on every node of the OpenShift cluster.

. Examine the use of the Red Hat-supported `istio-cni` Linux container image:
+
-----
$ oc describe daemonset istio-node -n istio-operator | grep Image
-----
+
.Sample Output
-----
registry.redhat.io/openshift-service-mesh/istio-cni-rhel8@sha256:6312d3d...
-----

=== Explore CRDs
Custom Resource Definitions (CRDs) facilitate domain-specific extensions to the OpenShift master API.
OpenShift Service Mesh defines several CRDs to facilitate the provisioning and life cycle of a service mesh.


. View the service mesh-related CRDs that extend the OpenShift master API:
+
-----
$ oc get crd --as=system:admin | grep 'maistra\|istio'
-----
+
[NOTE]
You need to impersonate an OpenShift cluster admin to do this because a service mesh control plane administrator does not have access to this in a production environment.

* Expect to see about 26 CRDs.

. Review some of the more prominent service mesh-related CRD extensions to the OpenShift master API including those in the following table:
+
[cols="2",options="header"]
|======
|CRD | Description
| `adapters.config.istio.io`
|link:https://istio.io/docs/reference/config/policy-and-telemetry/adapters/[Mixer adapters] allow Istio to interface to a variety of infrastructure back ends for things such as metrics and logs.
| `destinationrules.networking.istio.io`
|link:https://istio.io/docs/reference/config/networking/destination-rule/[`DestinationRule`] defines policies that apply to traffic intended for a service after routing has occurred.
| `gateways.networking.istio.io`
|link:https://istio.io/docs/reference/config/networking/gateway/[Gateway] describes a load balancer operating at the edge of the mesh receiving incoming or outgoing HTTP/TCP connections.
| `servicemeshcontrolplanes.maistra.io`
|Defines the details of a service mesh control plane.
| `servicemeshmemberrolls.maistra.io`
|The `ServiceMeshMemberRoll` resource lists the projects belonging to the control plane.
| `servicemeshpolicies.authentication.maistra.io`
|Allows for overriding of the `ServiceMeshControlPlane` settings with either namespace-scoped or service-scoped policies.
| `virtualservices.networking.istio.io`
|link:https://istio.io/docs/reference/config/networking/virtual-service/[Virtual Service] defines traffic routing.
|======

=== Explore Mutating Webhooks

You can inject the Envoy sidecar container into a deployment manually or automatically.
Most of the time, you use automatic injection, which requires an OpenShift _admission controller_.

An OpenShift admission controller is a piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object, but after the request is authenticated and authorized.
You can define two types of admission webhooks:

* *Validating admission webhook*: Allows you to reject requests to enforce custom admission policies.

* *Mutating admission webhook*: Allows you to change requests to enforce custom defaults.

In this section, you explore the mutating admission webhook.

. Impersonate `cluster-admin` and get a list of `mutatingwebhookconfiguration` resources on the OpenShift cluster:
+
-----
$ oc get mutatingwebhookconfiguration --as=system:admin | grep $SM_CP_NS
-----
+
.Sample Output
-----
[...]

istio-sidecar-injector-admin50-istio-system        1          19h
-----

. Extract the details of `mutatingwebhookconfiguration` specific to your OpenShift Service Mesh installation:
+
-----
$ oc get mutatingwebhookconfiguration istio-sidecar-injector-$SM_CP_NS \
       -o yaml \
       --as=system:admin \
       > $HOME/lab/$SM_CP_NS-mutatingwebhookconfiguration.yaml
-----

. Study the content of the `$HOME/lab/$SM_CP_NS-mutatingwebhookconfiguration.yaml` output file and note the following:

* The `/inject` endpoint of the `istio-sidecar-injector` service from your service mesh control plane is invoked when auto-injecting the Envoy service proxy into an application pod:
+
-----
    service:
      name: istio-sidecar-injector
      namespace: admin25-istio-system
      path: /inject
-----

* The scope of your mutating webhook are the namespaces of your service mesh data plane. Namespaces belonging to a service mesh instance have a label `maistra.io/member-of` with value equal to the namespace of the service mesh control plane.
+
-----
  namespaceSelector:
    matchExpressions:
    - key: maistra.io/member-of
      operator: In
      values:
      - admin25-istio-system
    - key: maistra.io/ignore-namespace
      operator: DoesNotExist
-----


== Review OpenShift Service Mesh Resources: `mesh-admin`

Your lab environment includes a dedicated OpenShift Service Mesh control plane.
This dedicated control plane is owned by your `mesh-admin` user.
The life cycle of your service mesh control plane is managed by the cluster-scoped OpenShift Service Mesh operator.

=== Explore Resources

. Make sure you are still authenticated into OpenShift as the `mesh-admin` user:
+
-----
$ oc login -u $SM_CP_ADMIN -p $OCP_PASSWD
-----

. Determine the version of OpenShift Service Mesh being used in your lab environment:
+
-----
$ istioctl version --remote=true -i $SM_CP_ADMIN-istio-system
-----
+
.Sample Output
-----
client version: 1.4.10
control plane version: OSSM_1.1.10
data plane version: 1.4.10 (2 proxies)
-----

. Examine the service mesh control plane deployments:
+
-----
$ oc get deployments -n $SM_CP_NS
-----
+
.Sample Output
-----
NAME                     READY   UP-TO-DATE   AVAILABLE   AGE
grafana                  1/1     1            1           24h
istio-citadel            1/1     1            1           24h
istio-egressgateway      1/1     1            1           24h
istio-galley             1/1     1            1           24h
istio-ingressgateway     1/1     1            1           24h
istio-pilot              1/1     1            1           24h
istio-policy             1/1     1            1           24h
istio-sidecar-injector   1/1     1            1           24h
istio-telemetry          1/1     1            1           24h
jaeger                   1/1     1            1           24h
kiali                    1/1     1            1           24h
prometheus               1/1     1            1           24h
-----

. Examine the `ServiceMeshControlPlane` custom resource:
+
-----
$ oc get ServiceMeshControlPlane -n $SM_CP_NS
-----
+
.Sample Output
-----
NAME           READY   STATUS            PROFILES    VERSION   AGE
full-install   11/11   ComponentsReady   [default]   1.1.10    40m
-----

* During the installation of your lab environment, the `$SM_CP_NS` namespace was created and this link:https://github.com/gpe-mw-training/ocp_service_mesh_advanced/blob/master/ansible/roles/maistra_control_plane/tasks/main.yml#L3-L5[`ServiceMeshControlPlane` custom resource] was applied to it.
The OpenShift Service Mesh operator detected the presence of this new custom resource and subsequently provisioned the service mesh control plane.
+
[NOTE]
If the status in the `READY` column is not `11/11`, this means that the service mesh control plane did not install correctly. If this occurs, examine the status of the `ServiceMeshControlPlane` custom resource to determine the issue.

. Note the existence of a _ServiceMeshPolicy_:
+
-----
$ oc get ServiceMeshPolicies -n $SM_CP_NS
-----
+
.Sample Output
-----
NAME      AGE
default   24h
-----
+
NOTE: This OpenShift Service Mesh-specific resource replaces the upstream Istio community `MeshPolicy` resource, which is cluster-scoped and not compatible with multi-tenancy. In a later lab of this course, this resource is used to configure mutual Transport Layer Security (mTLS) within a single service mesh.

. Examine the various namespace-scoped `RoleBinding` resources in the service mesh control plane:
+
-----
$ oc get RoleBinding -n $SM_CP_NS
-----
+
.Sample Output
-----
admin                                                           4h8m
istio-citadel-admin50-istio-system                              4h8m
istio-galley-admin-role-binding-admin50-istio-system            4h8m
istio-ingressgateway-sds                                        4h7m
istio-mixer-admin-role-binding-admin50-istio-system             4h7m
istio-pilot-admin50-istio-system                                4h7m
istio-sidecar-injector-admin-role-binding-admin50-istio-system  4h7m
kiali                                                           4h6m
mesh-users                                                      4h8m
prometheus-admin50-istio-system                                 4h8m
system:deployers                                                4h8m
system:image-builders                                           4h8m
system:image-pullers                                            4h8m
view                                                            3h33m
-----
+
NOTE: The use of a project-scoped `RoleBinding` resource rather than a cluster-scoped `ClusterRoleBinding` resource is a key enabler of the multi-tenant capabilities of the OpenShift Service Mesh component.

. Note the existence of an empty `ServiceMeshMemberRoll` resource called `default`:
+
-----
$ oc get ServiceMeshMemberRoll default -o template --template='{{"\n"}}{{.spec}}{{"\n\n"}}' -n $SM_CP_NS
-----
+
.Sample Output
-----
map[]
-----

* In a later lab, you (as the service mesh control plane admin) add the namespace where your Emergency Response Demo application resides to this currently empty `ServiceMeshMemberRoll` resource.


== Review Emergency Response Demo: `namespace-admin`

For the purposes of this course, your service mesh control plane manages your instance of the link:https://www.erdemo.io[Emergency Response Demo application].

The intent of the Emergency Response Demo application is to showcase the breadth of the Red Hat Middleware portfolio running on Red Hat OpenShift.

The services of the Emergency Response Demo application use a mix of HTTP-based synchronous communication and AMQ Streams-based asynchronous communication.
Because of this, the demo is an excellent application to highlight the existing capabilities and limitations of a service mesh.

=== Explore Resources

. Switch to the `$ERDEMO_USER` user:
+
-----
$ oc login -u $ERDEMO_USER -p $OCP_PASSWD
-----
* Your `$ERDEMO_USER` account has admin privileges to your dedicated Emergency Response Demo application.

. View the OpenShift DeploymentConfigs in the `$ERDEMO_USER-er-demo` namespace:
+
-----
$ oc get dc -n $ERDEMO_NS
-----
+
.Sample Output
-----
NAME                              REVISION   DESIRED   CURRENT   TRIGGERED BY
postgresql                        1          1         1         config,image(postgresql:9.6)
process-service-postgresql        1          1         1         config,image(postgresql:9.6)
user50-disaster-simulator         1          1         1         config,image(user50-disaster-simulator:c6818a7a8)
user50-emergency-console          1          1         1         config,image(user50-emergency-console:1fa91c80b)
user50-incident-priority-service  1          1         1         config,image(user50-incident-priority-service:1f5b136f5)
user50-incident-service           1          1         1         config,image(user50-incident-service:ba3e9bca3)
user50-mission-service            1          1         1         config,image(user50-mission-service:803e6420f)
user50-process-service            1          1         1         config,image(user50-process-service:cbfad8cf2)
user50-process-viewer             1          1         1         config,image(user50-process-viewer:8eac584e7)
user50-responder-client-app       1          1         1         config,image(user50-responder-client-app:f2394c06c)
user50-responder-service          1          1         1         config,image(user50-responder-service:4ca7b4bab)
user50-responder-simulator        1          1         1         config,image(user50-responder-simulator:66105af21)
-----

. Observe that the Emergency Response Demo application also makes use of Red Hat AMQ streams for event-driven, streams-based communication between many of its services:
+
-----
$ oc get deploy -n $ERDEMO_NS
-----
+
.Sample Output
-----
NAME                            READY   UP-TO-DATE   AVAILABLE   AGE
kafka-cluster-entity-operator   1/1     1            1           3d5h
-----
+
-----
$ oc get statefulset -l strimzi.io/kind=Kafka -n $ERDEMO_NS
-----
+
.Sample Output
-----
kafka-cluster-kafka       3/3     3d5h
kafka-cluster-zookeeper   3/3     3d5h
-----

. Also, observe that the Emergency Response Demo application makes use of Red Hat Data Grid:
+
-----
$ oc get statefulset -l application=datagrid-service -n $ERDEMO_NS
-----
+
.Sample Output
-----
datagrid-service   3/3     3d5h
-----

=== Get Started

The Emergency Response Demo application includes a web console where you can view emergency _incidents_ being accepted by volunteer _responders_.

==== Visit ER Demo App
. Access the web console at the following URL:
+
-----
echo -en "\n\nhttps://$(oc get route $ERDEMO_USER-emergency-console -o template --template={{.spec.host}} -n $ERDEMO_NS)\n\n"
-----

. On the login page, click the *Register Now* button
+
image::images/er-demo-register-now-button.png[]

. Register a new test user
.. *Boat Capacity*: How many rescuees the responder can carry in their boat. Setting a boat-capability of 10-12 is typically sufficient to be assigned an incident
.. *Medical Support*: Set to true if the responder can provide medical/first-aid support.
+
image::images/er-demo-register-now-responder.png[]

.. Once completed, click: *Register*


. Click the *Dashboard* link in the left panel. You will see the main application screen.
+
image::images/er-demo-main-screen.png[]



===== Incident Status
The Incident Status section tracks the data for number of incidents requested, assigned, picked up and rescued. These values update in real-time based on application events.

===== Responder Utilization
The Responder Utilization section monitors the total number of responders, active and idle responders. This section is also updated in real-time based on application events.

===== Map
The map shows the location of the incidents, responders and their associated routes. 

==== Mission

When a Responder is assigned an Incident, a Mission is created. The Mission defines where the Responder needs to go to collect the victims of the Incident (the Way Point) and what shelter the victims should be dropped off at (the Target Location). The mission also has details of the responders location history.

. Select the link for *Mission*

** This screen shows the view for an individual responder which shows their current mission, including the router to the Incident and onward route to the shelter.

. Add yourself to the map as a responder
.. Click any location on the map.
+
image::images/er-demo-add-as-responder-to-the-map.png[width="80%"]

.. Click your boat icon. It will show the details of your boat profile.
+
image::images/er-demo-responder-boat-details.png[width="80%"]

.. Click the *Available* button. 

NOTE: Scroll to the bottom of the screen to see the *Available* button.

image::images/er-demo-mission_click_available_button.png[width="80%"]

NOTE: After you click the *Available* button, the button may continue to spin. You don't have to wait for the button to finish spinning. You can safely proceed to the next section.

==== View the Dashboard

. Click the *Dashboard* link
** In the *Responder Utilization* section, verify that there are 1 total responders. This is based on your recent action.
+
image::images/er-demo-1-total-responders.png[width="80%"]

==== Incidents

An incident is a request for help from an individual (or group of individuals) that are in need of rescue. Details of an Incident include the location (Lat, Long), the number of people stranded and whether medical assistance is required.

. Click the *Incidents* link

** This screen shows a list of incidents. At the moment, this screen is empty, but we will create incidents in the next section.
+
image::images/er-demo-incidents-empty.png[width="80%"]

==== Disaster Simulator
Let's get the web URL for the Disaster Simulator

. In your terminal window, type the following
+
-----
echo -en "\n\nhttp://$(oc get route $ERDEMO_USER-disaster-simulator -o template --template={{.spec.host}} -n $ERDEMO_NS)\n\n"
-----

. In your web browser, visit the above URL.

===== Create Incidents

. In the section for *Create Incidents*, move to the field for *Number of Incidents* and enter `50`.
. Click *Submit*
+
image::images/er-demo-create-incidents.png[width="60%"]

** At the top of the screen, you will see the result of the command.
+
image::images/er-demo-simulator-create-incidents-result.png[]

===== Create Responders
. In the section for *Create Responders*, move to the field for *Number of Responders* and enter `3`.
. Click *Submit*
+
image::images/er-demo-create-responders.png[width="60%"]

** At the top of the screen, you will see the result of the command.
+
image::images/er-demo-simulator-create-responders-result.png[]

. Move back to the *Emergency Response Demo Web Console* window

. Click the *Dashboard* link.

. Confirm that you have incidents and responders. 

** You will see activity as the responders are assigned to missions. The responders will start moving to rescue the stranded victims.

==== View Your Mission

By this time, your boat should have been assigned to a mission.

. Click the *Mission* link.

* You will see your boat moving towards an incident.

* Once your boat makes it to the incident location, click the *Picked up* button. 

NOTE: Scroll to the bottom of the screen to see the *Picked up* button.

image::images/er-demo-mission-picked-up.png[width="80%"]

* This confirms that you have picked up the passengers and your boat will proceed to the shelter.

==== View Incidents

You can view a list of all incidents and check their status.

. Click the *Incidents* link.
+
image::images/er-demo-view-all-incidents.png[width="80%"]

==== Process Automation

The Process Service is responsible for managing the overall process flow of the system. The Process Service operates purely on Kafka messages and does not expose any HTTP API - although it does invoke HTTP APIs in the Responder and Incident Priority Services. 

When a new Incident is reported on the topic-incident-event Topic, the process Service kicks off a new BPM process to manage the new Incident. When a Responder is shown as available (via the topic-responder-event Topic), the BPM process is updated to reflect this. As the Mission progresses and additional messages are received on the topic-mission-event Topic, the BPM process is updated to reflect the latest state.

The Process Service sends out multiple types of messages on various Topics in response to the Incident progressing through the Business Process.

Let's view the process diagram for an incident.

. Click the *Dashboard* link.

. Click an Incident on the map.
** This will show a pop-up for the incident.
+
image::images/er-demo-incident-popup.png[width="80%"]

. Click the link for *Process Diagram*
** This will open new tab to view the Process Diagram for this incident.
+
image::images/er-demo-view-process-diagram.png[width="80%"]

. Review the process diagram for this incident.

== Appendix

=== Reference

. link:https://servicemesh.io/[The Service Mesh: What Every Software Engineer Needs to Know about the World's Most Over-Hyped Technology]
. link:https://issues.jboss.org/projects/OSSM/issues/OSSM-11?filter=allopenissues[OpenShift Service Mesh Jiras]
. link:https://docs.openshift.com/container-platform/4.2/service_mesh/service_mesh_arch/understanding-ossm.html[OpenShift Service Mesh component documentation]

ifdef::showscript[]


endif::showscript[]
