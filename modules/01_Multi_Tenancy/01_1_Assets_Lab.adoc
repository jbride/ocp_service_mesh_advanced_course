:noaudio:
:scrollbar:
:toc2:
:linkattrs:
:data-uri:

== Assets and Tooling Lab

.Goals
* Review lab environment
* Review preinstalled Red Hat^(R)^ OpenShift^(R)^ Service Mesh _operator_
* Review preinstalled OpenShift Service Mesh _control plane_
* Review the three user roles typically involved in an OpenShift Service Mesh-managed environment
* Become familiar with Emergency Response Demo

:numbered:

== Set Up Local Workstation

=== Install Client Utilities

. Download and install the following utilities on your workstation:
.. `openssl` (Use the `yum install openssl` command on Red Hat Enterprise Linux^(R)^ or Fedora).
.. link:https://stedolan.github.io/jq/download/[jq] (Use the `dnf install jq` command on Fedora).
.. link:https://mirror.openshift.com/pub/openshift-v4/clients/oc/4.2/[`oc` utility].

... After installing the `oc` client, make sure it is on your `$PATH` and that its version is in the OpenShift 4.2 family:
+
-----
$ oc version
-----
+
.Sample Output
-----
Client Version: v4.2.0
-----

.. link:https://github.com/istio/istio/releases/tag/1.3.5[`istioctl` utility].
** This link provides a more recent version of the `istioctl` command line utility than the version of Istio that OpenShift Service Mesh 1.0 is based on. However, for the purposes of this course, this version of `istioctl` works well.
... Unarchive the download and place the `bin/istioctl` binary somewhere on your `$PATH`.

=== Set Environment Variables

. Ask your instructor to share the lab details spreadsheet:
+
image::images/lab_details_spreadsheet.png[]

. Open a terminal window on your local machine and download the script that sets the environment variables required for the labs in this course:
+
-----
$ mkdir -p $HOME/lab

$ curl https://raw.githubusercontent.com/gpe-mw-training/ocp_service_mesh_advanced/master/utils/set_env_vars.sh \
    -o $HOME/lab/set_env_vars.sh && \
    chmod 775 $HOME/lab/set_env_vars.sh
-----

. Using your favorite text editor, modify the environment variables in this downloaded shell script.
* Refer to the lab details spreadsheet shared by your instructor.

. Execute the shell script so that the environment variables are applied to your existing shell:
+
-----
$ $HOME/lab/set_env_vars.sh
-----

== Review OpenShift Container Platform Assets
Your lab is built on a shared OpenShift Container Platform v4 cluster in the cloud.

=== Explore OpenShift User: `$ERDEMO_USER`

This user is an administrator of your microservice-architected business application used for this course--the _Emergency Response Demo_ application.
In addition, this user has view access to a variety of other namespaces.

. At the terminal, authenticate into OpenShift Container Platform as the `$ERDEMO_USER` user:
+
-----
$ oc login $LAB_MASTER_API -u $ERDEMO_USER -p $OCP_PASSWD
-----

. View a listing of OpenShift projects that you have access to:
+
-----
$ oc get projects
-----
+
.Sample Output
-----
istio-operator       Service Mesh Operator         Active
kafka-operator-erd                                 Active
tools-erd                                          Active
user50-er-demo                                      Active
admin50-istio-system   admin5 Service Mesh System   Active
-----

* Subsequent sections of this lab introduce you to each of these namespaces.

=== Explore OpenShift User: `$SM_CP_ADMIN`

This user is your OpenShift Service Mesh control plane administrator.
In addition, this user has view access to a variety of other namespaces.

. At the terminal, authenticate into OpenShift Container Platform as the `$SM_CP_ADMIN` user:
+
-----
$ oc login $LAB_MASTER_API -u $SM_CP_ADMIN -p $OCP_PASSWD
-----

. View a listing of OpenShift projects that you have access to:
+
-----
$ oc get projects
-----
+
.Sample Output
-----
3scale-mt-api0         3scale-mt-api0                Active
admin50-istio-system   admin50 Service Mesh System   Active
istio-operator         Service Mesh Operator         Active
user50-er-demo                                       Active
-----

=== Access OpenShift Container Platform Web Console

. At the terminal, determine the URL of the OpenShift Container Platform web console:
+
-----
$ oc whoami --show-console
-----

. Open a browser and navigate to the URL revealed in the previous step.
. Authenticate using the values of `$ERDEMO_USER` and `$OCP_PASSWD`.

== Review OpenShift Service Mesh Resources: `cluster-admin`

OpenShift Service Mesh implements _soft multi-tenancy_ that provides a three-tier RBAC model comprising the roles of `cluster-admin`, `mesh-admin`, and `namespace-admin`.

In this section of the lab, you review the OpenShift Service Mesh resources owned by an OpenShift user with the `cluster-admin` role--in this case, your instructor.

=== Explore Operator

. Switch to the administrator of your Service Mesh control plane (who also has view access to the `istio-operator` namespace):
+
-----
$ oc login -u $SM_CP_ADMIN -p $OCP_PASSWD
-----

. View the previously installed Service Mesh operator found in the `istio-operator` namespace:
+
-----
$ oc get deploy istio-operator -n istio-operator
-----
+
.Sample Output
-----
istio-operator   1/1     1            1           15h
-----

* This operator is global in scope to the OpenShift cluster.
* The administrator of the `istio-operator` namespace is the OpenShift user with `cluster-admin` rights.


=== Explore Istio CNI Plug-in

When the link:https://istio.io/[Istio] community project injects the _Envoy_ service proxy sidecar into an application pod, it typically uses link:https://kubernetes.io/docs/concepts/workloads/pods/init-containers/[init containers] to manipulate the iptables rules of the OpenShift node where the application pod runs.
It modifies these iptables in order to intercept requests to application containers.
Although the Envoy service proxy sidecar does not require `root` to run, this short-lived `init-container` container does require link:https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities[`cap_net_admin` privileges].
This use of `init container` in each application pod with elevated `cap_net_admin` privileges is a security vulnerability.

OpenShift Service Mesh avoids this approach.
Instead, it makes use of the link:https://istio.io/docs/setup/additional-setup/cni/[`istio-cni` plug-in].
This plug-in is an implementation of the link:https://github.com/containernetworking/cni[Linux container network interface] specification.
It is responsible for manipulating iptables routing rules on a pod injected with the Envoy sidecar container.

The `istio-cni` plug-in still runs with elevated privileges.
Subsequently, it is implemented as a DaemonSet in the `istio-operator` namespace, which is typically owned by the OpenShift user with `cluster-admin` privileges.


. View the previously installed `istio-cni` pods implemented as an OpenShift DaemonSet in the `istio-operator` namespace:
+
-----
oc get daemonset istio-node -n istio-operator
-----
+
.Sample Output
-----
NAME         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                 AGE

istio-node   12        12        12      12           12          beta.kubernetes.io/os=linux   4d21h
-----

* As a DaemonSet, an Istio CNI pod runs on every node of the OpenShift cluster.

. Examine the use of the Red Hat-supported `istio-cni` Linux container image:
+
-----
$ oc describe daemonset istio-node -n istio-operator | grep Image
-----
+
.Sample Output
-----
registry.redhat.io/openshift-service-mesh/istio-cni-rhel8:1.0.1
-----


=== Explore CRDs
Custom Resource Definitions (CRDs) facilitate domain-specific extensions to the OpenShift master API.
OpenShift Service Mesh defines several CRDs to facilitate the provisioning and life cycle of a service mesh.


. View the service mesh-related CRDs that extend the OpenShift master API:
+
-----
$ oc get crd --as=system:admin | grep 'maistra\|istio'
-----
+
[NOTE]
You need to impersonate an OpenShift cluster admin to do this because a service mesh control plane administrator does not have access to this in a production environment.

* Expect to see about 55 CRDs.

. Review some of the more prominent service mesh-related CRD extensions to the OpenShift master API including those in the following table:
+
[cols="2",options=header]
|======
|CRD | Description
| `adapters.config.istio.io`
|link:https://istio.io/docs/reference/config/policy-and-telemetry/adapters/[Mixer adapters] allow Istio to interface to a variety of infrastructure back ends for things such as metrics and logs.
| `destinationrules.networking.istio.io`
|link:https://istio.io/docs/reference/config/networking/destination-rule/[`DestinationRule`] defines policies that apply to traffic intended for a service after routing has occurred.
| `gateways.networking.istio.io`
|link:https://istio.io/docs/reference/config/networking/gateway/[Gateway] describes a load balancer operating at the edge of the mesh receiving incoming or outgoing HTTP/TCP connections.
| `servicemeshcontrolplanes.maistra.io`
|Defines the details of a service mesh control plane.
| `servicemeshmemberrolls.maistra.io`
|The `ServiceMeshMemberRoll` resource lists the projects belonging to the control plane.
| `servicemeshpolicies.authentication.maistra.io`
|Allows for overriding of the `ServiceMeshControlPlane` settings with either namespace-scoped or service-scoped policies.
| `virtualservices.networking.istio.io`
|link:https://istio.io/docs/reference/config/networking/virtual-service/[Virtual Service] defines traffic routing.
|======

=== Explore Mutating Webhooks

You can inject the Envoy sidecar container into a deployment manually or automatically.
Most of the time, you use automatic injection, which requires an OpenShift _admission controller_.

An OpenShift admission controller is a piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object, but after the request is authenticated and authorized.
You can define two types of admission webhooks:

* *Validating admission webhook*: Allows you to reject requests to enforce custom admission policies.

* *Mutating admission webhook*: Allows you to change requests to enforce custom defaults.

In this section, you explore the mutating admission webhook.

. Impersonate `cluster-admin` and get a list of `mutatingwebhookconfiguration` resources on the OpenShift cluster:
+
-----
$ oc get mutatingwebhookconfiguration --as=system:admin | grep $SM_CP_NS
-----
+
.Sample Output
-----
[...]

istio-sidecar-injector-admin25-istio-system   2019-11-12T15:15:18Z
-----

. Extract the details of `mutatingwebhookconfiguration` specific to your OpenShift Service Mesh installation:
+
-----
$ oc get mutatingwebhookconfiguration istio-sidecar-injector-$SM_CP_NS \
       -o yaml \
       --as=system:admin \
       > $HOME/lab/$SM_CP_NS-mutatingwebhookconfiguration.yaml
-----

. Study the content of the `$HOME/lab/$SM_CP_NS-mutatingwebhookconfiguration.yaml` output file and note the following:

* The `/inject` endpoint of the `istio-sidecar-injector` service from your service mesh control plane is invoked when auto-injecting the Envoy service proxy into an application pod:
+
-----
    service:
      name: istio-sidecar-injector
      namespace: admin25-istio-system
      path: /inject
-----

* The scope of your mutating webhook is the namespace of your service mesh control plane:
+
-----
  namespaceSelector:
    matchExpressions:
    - key: maistra.io/member-of
      operator: In
      values:
      - admin25-istio-system
    - key: maistra.io/ignore-namespace
      operator: DoesNotExist
-----


== Review OpenShift Service Mesh Resources: `mesh-admin`

Your lab environment includes a dedicated OpenShift Service Mesh control plane.
This dedicated control plane is owned by your `mesh-admin` user.
The life cycle of your service mesh control plane is managed by the cluster-scoped OpenShift Service Mesh operator.

=== Explore Resources

. Make sure you are still authenticated into OpenShift as the `mesh-admin` user:
+
-----
$ oc login -u $SM_CP_ADMIN -p $OCP_PASSWD
-----

. Determine the version of OpenShift Service Mesh being used in your lab environment:
+
-----
$ istioctl version --remote=true -i $SM_CP_ADMIN-istio-system
-----
+
.Sample Output
-----
client version: 1.3.5
control plane version: 1.0.1-1
-----

. Examine the service mesh control plane deployments:
+
-----
$ oc get deployments -n $SM_CP_NS
-----
+
.Sample Output
-----
NAME                     READY   UP-TO-DATE   AVAILABLE   AGE
grafana                  1/1     1            1           24h
istio-citadel            1/1     1            1           24h
istio-egressgateway      1/1     1            1           24h
istio-galley             1/1     1            1           24h
istio-ingressgateway     1/1     1            1           24h
istio-pilot              1/1     1            1           24h
istio-policy             1/1     1            1           24h
istio-sidecar-injector   1/1     1            1           24h
istio-telemetry          1/1     1            1           24h
jaeger                   1/1     1            1           24h
kiali                    1/1     1            1           24h
prometheus               1/1     1            1           24h
-----

. Examine the `ServiceMeshControlPlane` custom resource:
+
-----
$ oc get ServiceMeshControlPlane -n $SM_CP_NS
-----
+
.Sample Output
-----
NAME           READY
full-install   True
-----

* During the installation of your lab environment, the `$SM_CP_NS` namespace was created and this link:https://github.com/gpe-mw-training/ocp_service_mesh_advanced/blob/master/ansible/roles/maistra_control_plane/tasks/main.yml#L3-L5[`ServiceMeshControlPlane` custom resource] was applied to it.
The OpenShift Service Mesh operator detected the presence of this new custom resource and subsequently provisioned the service mesh control plane.
+
[NOTE]
If the status in the `READY` column is not `True`, this means that the service mesh control plane did not install correctly. If this occurs, examine the status of the `ServiceMeshControlPlane` custom resource to determine the issue.

. Note the existence of a _ServiceMeshPolicy_:
+
-----
$ oc get ServiceMeshPolicies -n $SM_CP_NS
-----
+
.Sample Output
-----
NAME      AGE
default   24h
-----
+
NOTE: This OpenShift Service Mesh-specific resource replaces the upstream Istio community `MeshPolicy` resource, which is cluster-scoped and not compatible with multi-tenancy. In a later lab of this course, this resource is used to configure mutual Transport Layer Security (mTLS) within a single service mesh.

. Examine the various namespace-scoped `RoleBinding` resources in the service mesh control plane:
+
-----
$ oc get RoleBinding -n $SM_CP_NS
-----
+
.Sample Output
-----
istio-citadel-admin50-istio-system                               24h
istio-citadel-admin50-istio-system                               24h
istio-egressgateway-admin50-istio-system                         24h
istio-galley-admin-role-binding-admin50-istio-system             24h
istio-ingressgateway-admin50-istio-system                        24h
istio-ingressgateway-sds                                         24h
istio-mixer-admin-role-binding-admin50-istio-system              24h
istio-pilot-admin50-istio-system                                 24h
istio-sidecar-injector-admin-role-binding-admin50-istio-system   24h
kiali                                                            24h
prometheus-admin50-istio-system                                  24h
-----
+
NOTE: The use of a project-scoped `RoleBinding` resource rather than a cluster-scoped `ClusterRoleBinding` resource is a key enabler of the multi-tenant capabilities of the OpenShift Service Mesh component.

. Note the existence of an empty `ServiceMeshMemberRoll` resource called `default`:
+
-----
$ oc get ServiceMeshMemberRoll default -o template --template='{{"\n"}}{{.spec}}{{"\n\n"}}' -n $SM_CP_NS
-----
+
.Sample Output
-----
map[]
-----

* In a later lab, you (as the service mesh control plane admin) add the namespace where your Emergency Response Demo application resides to this currently empty `ServiceMeshMemberRoll` resource.


== Review Emergency Response Demo: `namespace-admin`

For the purposes of this course, your service mesh control plane manages your instance of the link:https://www.erdemo.io[Emergency Response Demo application].

The intent of the Emergency Response Demo application is to showcase the breadth of the Red Hat Middleware portfolio running on Red Hat OpenShift.

The services of the Emergency Response Demo application use a mix of HTTP-based synchronous communication and AMQ streams-based asynchronous communication.
Because of this, the demo is an excellent application to highlight the existing capabilities and limitations of a service mesh.

=== Explore Resources

. Switch to the `$ERDEMO_USER` user:
+
-----
$ oc login -u $ERDEMO_USER -p $OCP_PASSWD
-----
* Your `$ERDEMO_USER` account has admin privileges to your dedicated Emergency Response Demo application.

. View the OpenShift DeploymentConfigs in the `$ERDEMO_USER-er-demo` namespace:
+
-----
$ oc get dc -n $ERDEMO_NS
-----
+
.Sample Output
-----
NAME                                REVISION   DESIRED   CURRENT
postgresql                          1          1         1
user50-disaster-simulator           1          1         1
user50-emergency-console            1          1         1
user50-incident-priority-service    1          1         1
user50-incident-service             1          1         1
user50-mission-service              1          1         1
user50-process-service              1          1         1
user50-process-service-postgresql   1          1         1
user50-process-viewer               1          1         1
user50-responder-service            1          1         1
user50-responder-simulator          1          1         1
-----

. Observe that the Emergency Response Demo application also makes use of Red Hat AMQ streams for event-driven, streams-based communication between many of its services:
+
-----
$ oc get deploy -n $ERDEMO_NS
-----
+
.Sample Output
-----
NAME                            READY   UP-TO-DATE   AVAILABLE   AGE
kafka-cluster-entity-operator   1/1     1            1           3d5h
-----
+
-----
$ oc get statefulset -l strimzi.io/kind=Kafka -n $ERDEMO_NS
-----
+
.Sample Output
-----
kafka-cluster-kafka       3/3     3d5h
kafka-cluster-zookeeper   0/3     3d5h
-----

. Also, observe that the Emergency Response Demo application makes use of Red Hat Data Grid:
+
-----
$ oc get statefulset -l application=datagrid-service -n $ERDEMO_NS
-----
+
.Sample Output
-----
datagrid-service   3/3     3d5h
-----

=== Get Started

The Emergency Response Demo application includes a web console where you can view emergency _incidents_ being accepted by volunteer _responders_.

. Access the web console at the following URL:
+
-----
echo -en "\n\nhttps://$(oc get route $ERDEMO_USER-emergency-console -o template --template={{.spec.host}} -n $ERDEMO_NS)\n\n"
-----

. Log in to the web console and simulate an emergency by following the instructions in the link:https://www.erdemo.io/gettingstarted/[Getting Started guide].

. Attempt to create one or more emergency incidents that trigger responses from volunteer community responders.


== Appendix

=== Reference

. link:https://servicemesh.io/[The Service Mesh: What Every Software Engineer Needs to Know about the World's Most Over-Hyped Technology]
. link:https://issues.jboss.org/projects/OSSM/issues/OSSM-11?filter=allopenissues[OpenShift Service Mesh Jiras]
. link:https://docs.openshift.com/container-platform/4.2/service_mesh/service_mesh_arch/understanding-ossm.html[OpenShift Service Mesh component documentation]

ifdef::showscript[]


endif::showscript[]
