:noaudio:
:scrollbar:
:toc2:
:linkattrs:
:data-uri:

= Service Mesh Multi Tenancy Lab

.Goals
** Understand importance of Service Mesh multi-tenancy
** Understand ServiceMeshMemberRoll
** Understand Envoy _Data Plane_

:numbered:

== Multi-tenancy Overview

Faulty or compromised control planes should not be able affect any other tenants on the cluster. 
This means that control plane components and service accounts have to be locked down using appropriate Kubernetes Roles and network access limited by NetworkPolicies.

.References:
* link:https://docs.google.com/document/d/1eMnLBpcJNMahoE6cYKcECp_Jcy4Haj3qc36RBAO9J-U/edit#[Operator-Based Soft Multi-Tenancy]
* link:https://maistra.io/docs/comparison-with-istio/#_cluster_scoped_custom_resources[Comparison with Istio]


== ServiceMeshMemberRoll

. Switch to your service mesh control plane administrator user:
+
-----
oc login -u $SM_CP_ADMIN -p $OCP_PASSWD
-----

. Register the $ER_DEMO_NS namespace as namespace _member_ to be monitored and manged by your service mesh control plane.
+
-----
echo "apiVersion: maistra.io/v1
kind: ServiceMeshMemberRoll
metadata:
  name: default
spec:
  members:
  - $ER_DEMO_NS" | oc apply -n $RHSM_CONTROL_PLANE_NS -f -
-----
+
Recall that the _ServiceMeshMemberRoll_ exists in the service mesh control plane (which is owned by the admin: $SM_CP_ADMIN )

. Notice that your $ER_DEMO_NS namespace now includes _kiali_ and _maistra_ annotations:
+
-----
echo -en "\n\n$(oc get project $ER_DEMO_NS -o template --template='{{.metadata.labels}}')\n\n"


map[kiali.io/member-of:admin50-istio-system maistra.io/member-of:admin50-istio-system]
-----

. Notice also that your $ER_DEMO_NS namespace now includes namespace scoped _RoleBindings_ associated with the Istio related service accounts from your specific service mesh control plane:
+
-----
oc get RoleBinding  -n $ER_DEMO_NS -l release=istio

istio-citadel-admin50-istio-system                               24h
istio-egressgateway-admin50-istio-system                         24h
istio-galley-admin-role-binding-admin50-istio-system             24h
istio-ingressgateway-admin50-istio-system                        24h
istio-mixer-admin-role-binding-admin50-istio-system              24h
istio-pilot-admin50-istio-system                                 24h
istio-sidecar-injector-admin-role-binding-admin50-istio-system   24h
prometheus-admin50-istio-system                                  24h
-----
+
The use of a project scoped _RoleBinding_ rather than a cluster-scoped _ClusterRoleBinding_ is a key enabler of _multi-tenant_ capabilities of the Red Hat Service Mesh product.

. Red Hat OpenShift Service Mesh configures each member project to ensure network access between itself, the control plane, and other member projects.
+
Notice that your $ER_DEMO_NS namespace now also includes a _NetworkPolicy_ called: _istio-mesh_
+
-----
oc get NetworkPolicy istio-mesh -n $ER_DEMO_NS

istio-mesh   <none>         26m
-----
+
This _NetworkPolicy_ allows ingress to all pods specific to this namespace from the corresponding Red Hat Service Mesh control plane all other other registered _members_.

. Notice also that your $ER_DEMO_NS namespace now includes _istio_ related secrets:
+
-----
oc get secrets -n $ER_DEMO_NS | grep istio

...

stio.builder                                   istio.io/key-and-cert                 3      9m58s
istio.default                                   istio.io/key-and-cert                 3      9m55s
istio.deployer                                  istio.io/key-and-cert                 3      9m57s
istio.disaster-simulator-service                istio.io/key-and-cert                 3      9m58s
istio.emergency-console                         istio.io/key-and-cert                 3      9m54s
istio.incident-priority-service                 istio.io/key-and-cert                 3      9m57s
istio.incident-service                          istio.io/key-and-cert                 3      9m54s
istio.kafka-cluster-entity-operator             istio.io/key-and-cert                 3      9m54s
istio.kafka-cluster-kafka                       istio.io/key-and-cert                 3      9m55s
istio.kafka-cluster-zookeeper                   istio.io/key-and-cert                 3      9m55s
istio.mission-service                           istio.io/key-and-cert                 3      9m56s
istio.postgresql                                istio.io/key-and-cert                 3      9m56s
istio.process-service                           istio.io/key-and-cert                 3      9m57s
istio.process-viewer                            istio.io/key-and-cert                 3      9m54s
istio.responder-service                         istio.io/key-and-cert                 3      9m54s
istio.responder-simulator-service               istio.io/key-and-cert                 3      9m53s
-----
+
These secrets support mutual TLS functionality provided by the Red Hat Service Mesh.
More information about these secrets is provided in the next module of this course.

== Opt-in Auto-Injection Annotations

When deploying an application into the Red Hat OpenShift Service Mesh you must opt in to injection of the Envoy _data-plane_ by specifying the following annotation: _sidecar.istio.io/inject=true_ . 

Opting in ensures that the sidecar injection does not interfere with other OpenShift features such as builder pods used by numerous frameworks within the OpenShift ecosystem.

In this section of this lab you (as the owner of the Emergency Response application) opt in a selective list of services for auto injection of a sidecar.

. Switch to the $ERDEMO_USER:
+
-----
oc login -u $ERDEMO_USER
-----
+
The $ERDEMO_USER is the admin of the $ER_DEMO_NS namespace where the Emergency Response application resides.

. Review the contents of link:https://github.com/gpe-mw-training/ocp_service_mesh_advanced/blob/master/utils/inject_istio_annotation.sh[this script].


. Execute script that adds Envoy auto-injection annotations to Emergency Response services:
+
-----
curl https://raw.githubusercontent.com/gpe-mw-training/ocp_service_mesh_advanced/master/utils/inject_istio_annotation.sh \
    -o $HOME/lab/inject_istio_annotation.sh && \
    chmod 775 $HOME/lab/inject_istio_annotation.sh && \
    $HOME/lab/inject_istio_annotation.sh
-----

. After completion of the script, review the list Emergency Response related pods:
+
-----
oc get pods -l group=erd-services -n $ER_DEMO_NS

user50-disaster-simulator-1-p9gfl          2/2     Running   7          9h
user50-incident-priority-service-1-hgmdn   2/2     Running   4          9h
user50-incident-service-1-sz4dk            2/2     Running   3          9h
user50-mission-service-1-jz2r8             2/2     Running   9          9h
user50-process-service-4-cz5sz             2/2     Running   5          7h17m
user50-responder-service-1-qm5gn           2/2     Running   3          7h14m
user50-responder-simulator-1-tdrz2         2/2     Running   6          7h13m
-----
+
Notice that each of these pods indicates that two containers have started.

. You could use a script such as the following to identify a list of container names for each of the pods:
+
-----

for POD_NAME in $(oc get pods -n $ER_DEMO_NS -l group=erd-services -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}')
do
    oc get pod $POD_NAME  -n $ER_DEMO_NS -o jsonpath='{.metadata.name}{"    :\t\t"}{.spec.containers[*].name}{"\n"}'
done


...

user50-disaster-simulator-1-p9gfl    :          user50-disaster-simulator        istio-proxy
user50-incident-priority-service-1-hgmdn    :   user50-incident-priority-service istio-proxy
user50-incident-service-1-sz4dk    :            user50-incident-service          istio-proxy
user50-mission-service-1-jz2r8    :             user50-mission-service           istio-proxy
user50-process-service-4-cz5sz    :             user50-process-service           istio-proxy
user50-responder-service-1-qm5gn    :           user50-responder-service         istio-proxy
user50-responder-simulator-1-tdrz2    :         user50-responder-simulator       istio-proxy
-----

.. Notice that each pod now contains an _istio-proxy_ container co-located with the primary business service container.
.. Istio uses Kubernetes' link:https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook[MutatingAdmissionWebhook] for automatically injecting the sidecar proxy into user pods.

. The two databases leveraged by the Emergency Response demo ( _postgresql_ and _$ERDEMO_USER-process-service-postgresql_ ) are also now injected with an envoy proxy.
+
Verify that this is infact the case either through the OpenShift web console or the oc utility.

== Envoy _Data Plane_

=== Modified iptables 

TO-DO:  https://github.com/istio/cni#validate-the-iptables-are-modified


=== Service proxy container configuration

. Capture the details of the _istio-proxy_ container configuration from the _responder-service_ pod of the Emergency Response demo :
+
-----
oc get pod -n $ER_DEMO_NS \
       $(oc get pod -n $ER_DEMO_NS | grep "^$ERDEMO_USER-responder-service" | awk '{print $1}') \
       -o json \
       | jq .spec.containers[1] \
        > $HOME/lab/responder_envoy.json
-----

. Study the details of the _istio-proxy_ container:
+
-----
less $HOME/lab/responder_envoy.json
-----

. Answer the following questions pertaining to this _istio-proxy_ container:

.. What URL does OpenShift use to pull the remote Envoy proxy image that serves as the basis of this Envoy proxy sidecar?
.. What is the maximum amount of RAM and CPU dedicated to this Envoy proxy sidecar container ?
.. What is the URL that the Envoy proxy sidecar uses to communicate with _Pilot_ component of Red Hat Service Mesh ?


ifdef::showscript[]

1) registry.redhat.io/openshift-service-mesh/proxyv2-rhel8:1.0.1
2) cpu: 500m,  memory: 128Mi
3) istio-pilot.admin50-istio-system:15010

endif::showscript[]

=== Administration API

link:https://www.envoyproxy.io/docs/envoy/v1.12.0/operations/admin#operations-admin-interface[Envoy Administration API]

-----
oc rsh `oc get pod -n $ER_DEMO_NS | grep "responder-service" | awk '{print $1}'` \
    curl http://localhost:15000/help
-----

-----
oc rsh `oc get pod -n $ER_DEMO_NS | grep "responder-service" | awk '{print $1}'` \
   curl http://localhost:15000/clusters
-----

. Inspect the configuration sent by Pilot to your pod's sidecar using _istioctl_:
+
-----
istioctl proxy-config cluster -n <POD NAMESPACE> <PODNAME> -o json
-----
+
if you search for the destination service name you will see an embedded metadata JSON element that names the specific DestinationRule that pod is currently using to communicate with the external service.


-----
oc rsh `oc get pod -n $ER_DEMO_NS | grep "responder-service" | awk '{print $1}'` \
         curl http://localhost:15000/config_dump \
         > $HOME/lab/config_dump \
         && less $HOME/lab/config_dump \
         | jq ".configs | last | .dynamic_route_configs"
-----

=== Envoy Access Log File

.TO-DO:
* https://aspenmesh.io/how-to-debug-istio-mutual-tls-mtls-policy-issues-using-aspen-mesh/
* global.proxy.accessLogFile
* Is this log file any different than what is already being logged from Envoy in Red Hat Service Mesh ?
* What is a good example of using it to debug Istio configuration and policy issues ?

=== Debugging Envoy and Pilot

The source of truth for a given moment is always found in your pod’s Envoy sidecar configuration.
In this section of the lab, you link:https://istio.io/docs/ops/troubleshooting/proxy-cmd/[debug Envoy and Pilot].


link:https://www.erdemo.io/gettingstarted/[Getting Started]

ifdef::showscript[]

-----
oc project $RHSM_CONTROL_PLANE_NS && \
         oc rsh `oc get pod | grep "istio-ingressgateway" | awk '{print $1}'` \
         curl http://localhost:15000/config_dump \
         > $HOME/lab/config_dump \
         && less $HOME/lab/config_dump \
         | /usr/local/bin/jq ".configs | last | .dynamic_route_configs"
-----

endif::showscript[]
