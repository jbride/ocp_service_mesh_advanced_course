:noaudio:
:scrollbar:
:toc2:
:linkattrs:
:data-uri:

== Service Mesh Multi-Tenancy Lab

.Goals
** Understand the importance of service mesh multi-tenancy
** Understand `ServiceMeshMemberRoll`
** Understand Envoy _data plane_

:numbered:

== Review Multi-Tenancy

It is important to prevent faulty or compromised control planes from affecting any other tenants on the cluster.
This means control plane components and service accounts have to be locked down using appropriate Kubernetes roles and network access limited by NetworkPolicies.

== Explore `ServiceMeshMemberRoll`

. Switch to your service mesh control plane admin user:
+
-----
$ oc login -u $SM_CP_ADMIN -p $OCP_PASSWD
-----

. Register the `$ERDEMO_NS` namespace as a namespace member to be monitored and managed by your service mesh control plane:
+
-----
$ echo "apiVersion: maistra.io/v1
kind: ServiceMeshMemberRoll
metadata:
  name: default
spec:
  members:
  - $ERDEMO_NS" | oc apply -n $SM_CP_NS -f -
-----

* Recall that the `ServiceMeshMemberRoll` resource exists in the service mesh control plane, which is owned by the `$SM_CP_ADMIN` admin.

. Verify that your `$ERDEMO_NS` namespace now includes `kiali` and `maistra` annotations:
+
-----
$ echo -en "\n\n$(oc get project $ERDEMO_NS -o template --template='{{.metadata.labels}}')\n\n"
-----
+
.Sample Output
-----
map[kiali.io/member-of:admin50-istio-system maistra.io/member-of:admin50-istio-system]
-----

. Verify that your `$ERDEMO_NS` namespace now includes namespace-scoped `RoleBinding` resources associated with the Istio-related service accounts from your specific service mesh control plane:
+
-----
$ oc get RoleBinding  -n $ERDEMO_NS -l release=istio
-----
+
.Sample Output
-----
NAME                                                             AGE
istio-citadel-admin50-istio-system                               24s
istio-galley-admin-role-binding-admin50-istio-system             24s
istio-mixer-admin-role-binding-admin50-istio-system              24s
istio-pilot-admin50-istio-system                                 24s
istio-sidecar-injector-admin-role-binding-admin50-istio-system   24s
prometheus-admin50-istio-system                                  24s
-----
* The use of a project-scoped `RoleBinding` resource, rather than a cluster-scoped `ClusterRoleBinding` resource, is a key enabler of the _multi-tenant_ capabilities of Red Hat^(R)^ OpenShift^(R)^ Service Mesh.

* OpenShift Service Mesh configures each member project to ensure network access between itself, the control plane, and other member projects.

. Verify that your `$ERDEMO_NS` namespace now also includes a `NetworkPolicy` resource called `istio-mesh`:
+
-----
$ oc get NetworkPolicy istio-mesh -n $ERDEMO_NS
-----
+
.Sample Output
-----
NAME         POD-SELECTOR   AGE
istio-mesh   <none>         26m
-----
* This `NetworkPolicy` resource allows ingress to all pods specific to this namespace from all other registered members of the same OpenShift Service Mesh control plane.

. Verify that your `$ERDEMO_NS` namespace now includes `istio` related secrets:
+
-----
$ oc get secrets -n $ERDEMO_NS | grep istio
-----
+
.Sample Output
-----
[...]
istio.builder                                   istio.io/key-and-cert                 3      9m58s
istio.default                                   istio.io/key-and-cert                 3      9m55s
istio.deployer                                  istio.io/key-and-cert                 3      9m57s
istio.disaster-simulator-service                istio.io/key-and-cert                 3      9m58s
istio.emergency-console                         istio.io/key-and-cert                 3      9m54s
istio.incident-priority-service                 istio.io/key-and-cert                 3      9m57s
istio.incident-service                          istio.io/key-and-cert                 3      9m54s
istio.kafka-cluster-entity-operator             istio.io/key-and-cert                 3      9m54s
istio.kafka-cluster-kafka                       istio.io/key-and-cert                 3      9m55s
istio.kafka-cluster-zookeeper                   istio.io/key-and-cert                 3      9m55s
istio.mission-service                           istio.io/key-and-cert                 3      9m56s
istio.postgresql                                istio.io/key-and-cert                 3      9m56s
istio.process-service                           istio.io/key-and-cert                 3      9m57s
istio.process-viewer                            istio.io/key-and-cert                 3      9m54s
istio.responder-service                         istio.io/key-and-cert                 3      9m54s
istio.responder-simulator-service               istio.io/key-and-cert                 3      9m53s
-----
* These secrets support mutual TLS functionality provided by OpenShift Service Mesh.
You learn more about these secrets in the next module, which pertains to mTLS.

== Opt In Auto-Injection Annotations

When deploying an application into Red Hat OpenShift Service Mesh, you must opt in to injection of the Envoy _data plane_ for each deployment.
You do so by specifying the `sidecar.istio.io/inject=true` annotation in your deployment.

Opting in ensures that the sidecar injection does not interfere with other OpenShift capabilities (such as S2I builder pods) that likely do not need to be managed by the service mesh.

In this section of the lab, you, as the owner of the Emergency Response Demo application, opt in a selective list of deployments for auto injection of a sidecar.

. Switch to the `$ERDEMO_USER` user:
+
-----
$ oc login -u $ERDEMO_USER
-----
* `$ERDEMO_USER` is the admin of the `$ERDEMO_NS` namespace where your Emergency Response Demo application resides.

. Review the contents of link:https://github.com/gpe-mw-training/ocp_service_mesh_advanced/blob/master/utils/inject_istio_annotation.sh[this script], which iterates through the DeploymentConfig of your Emergency Response Demo application and adds the `sidecar.istio/inject=true` annotation.
+
****
*Questions*:

* Which DeploymentConfig resources of the Emergency Response Demo application are to be opted into your service mesh?
* Which resources of the Emergency Response Demo application will not be managed by your service mesh?
****

. Execute the shell script that adds Envoy auto-injection annotations to Emergency Response Demo deployments:
+
-----
$ curl https://raw.githubusercontent.com/gpe-mw-training/ocp_service_mesh_advanced/master/utils/inject_istio_annotation.sh \
    -o $HOME/lab/inject_istio_annotation.sh && \
    chmod 775 $HOME/lab/inject_istio_annotation.sh && \
    $HOME/lab/inject_istio_annotation.sh
-----

. After completion of the script, review the list of Emergency Response-related pods:
+
-----
$ oc get pods -l group=erd-services -n $ERDEMO_NS
-----
+
.Sample Output
-----
user50-disaster-simulator-1-p9gfl          2/2     Running   7          9h
user50-incident-priority-service-1-hgmdn   2/2     Running   4          9h
user50-incident-service-1-sz4dk            2/2     Running   3          9h
user50-mission-service-1-jz2r8             2/2     Running   9          9h
user50-process-service-4-cz5sz             2/2     Running   5          7h17m
user50-responder-service-1-qm5gn           2/2     Running   3          7h14m
user50-responder-simulator-1-tdrz2         2/2     Running   6          7h13m
-----
* Note that each of these pods indicates that two containers have started.

. Use a script similar to this to identify a list of container names for each of the pods:
+
-----
$ for POD_NAME in $(oc get pods -n $ERDEMO_NS -l group=erd-services -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}')
do
    oc get pod $POD_NAME  -n $ERDEMO_NS -o jsonpath='{.metadata.name}{"    :\t\t"}{.spec.containers[*].name}{"\n"}'
done
-----
+
.Sample Output
-----
[...]
user50-disaster-simulator-1-p9gfl    :          user50-disaster-simulator        istio-proxy
user50-incident-priority-service-1-hgmdn    :   user50-incident-priority-service istio-proxy
user50-incident-service-1-sz4dk    :            user50-incident-service          istio-proxy
user50-mission-service-1-jz2r8    :             user50-mission-service           istio-proxy
user50-process-service-4-cz5sz    :             user50-process-service           istio-proxy
user50-responder-service-1-qm5gn    :           user50-responder-service         istio-proxy
user50-responder-simulator-1-tdrz2    :         user50-responder-simulator       istio-proxy
-----

* Note that each pod now contains an additional `istio-proxy` container colocated with the primary business service container.
* Recall from a previous lab that OpenShift Service Mesh uses a Kubernetes link:https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook[`MutatingAdmissionWebhook`] for automatically injecting the sidecar proxy into user pods.



== Explore Envoy Data Plane

=== Review Architecture
Envoy has many features useful for inter-service communication.
To help understand Envoy's features and capabilities, you need to be familiar with the following terminology:

* *Listeners*: Listeners expose a port to the outside world into which an application can connect--for example, a listener on port 8080 accepts traffic and applies any configured behavior to that traffic.

* *Routes*: Routes are rules for how to handle traffic that comes in on listeners--for example, if a request comes in and matches `/incident`, the route directs that traffic to the incident _cluster_.

* *Clusters*: Clusters are specific upstream services to which Envoy can direct traffic--for example, if `incident-v1` and `incident-v2` are separate clusters, _routes_ can specify rules about how traffic is directed to either `v1` or `v2` of the incident service.


Traffic comes from a downstream system into Envoy via a listener.
This traffic is routed to one of Envoy's clusters, which is responsible for sending that traffic to an upstream system.
Downstream to upstream is how traffic always flows through Envoy.

{nbsp}
{nbsp}

image::images/envoy_architecture.png[]


=== Configure Service Proxy Container

. Delete any `deploy` pods that are in a `completed` status and have not yet been deleted:
+
-----
$ curl https://raw.githubusercontent.com/gpe-mw-training/ocp_service_mesh_advanced/master/utils/delete_pod_deploys.sh \
    -o $HOME/lab/delete_pod_deploys.sh && \
    chmod 775 $HOME/lab/delete_pod_deploys.sh

$HOME/lab/delete_pod_deploys.sh
-----

. Capture the details of the `istio-proxy` container configuration from the `responder-service` pod of the Emergency Response Demo application:
+
-----
$ oc get pod -n $ERDEMO_NS \
       $(oc get pod -n $ERDEMO_NS | grep "^$ERDEMO_USER-responder-service" | awk '{print $1}') \
       -o json \
       | jq .spec.containers[1] \
        > $HOME/lab/responder_envoy.json
-----

. Study the details of the `istio-proxy` container:
+
-----
$ less $HOME/lab/responder_envoy.json
-----

. Answer the following questions pertaining to this `istio-proxy` container:
+
****
*Questions*:

* What URL does OpenShift use to pull the remote Envoy proxy image that serves as the basis of this Envoy proxy sidecar?
* What is the maximum amount of RAM and CPU dedicated to this Envoy proxy sidecar container?
* What is the URL that the Envoy proxy sidecar uses to communicate with the Pilot component of OpenShift Service Mesh?
****

ifdef::showscript[]

*Answers*:

. `registry.redhat.io/openshift-service-mesh/proxyv2-rhel8:1.0.1`
. CPU: 500m,  memory: 128Mi
. `istio-pilot.admin50-istio-system:15010`

endif::showscript[]

=== Demonstrate Modified iptables

When an Envoy service proxy is injected into an application pod, the `istio-cni` resource modifies iptables on the node that the pod lands on.
Recall from a previous lab that the `istio-cni` resource is deployed as a DaemonSet and subsequently runs one pod for all of the nodes in an OpenShift cluster.

In particular, the `istio-cni` resource creates iptable rules so that all ingress to and egress from the application container is redirected to port 15001 of the pod.
The Envoy service proxy has its listener bound to port 15001.


Have your instructor demonstrate these modified iptable rules in a manner similar to the following:

. Identify the OpenShift Container Platform worker node that one of the Emergency Response Demo application pods is running on:
+
-----
$ oc get pod user50-responder-service-6-5xr86 -o json | jq .spec.nodeName
-----
+
.Sample Output
-----
[...]
ip-10-0-136-113.eu-central-1.compute.internal
-----


. Identify the ID of either container (application container or `envoy-proxy`) in that pod:
+
-----
$ oc describe pod user50-responder-service-6-5xr86 | grep cri-o
-----
+
.Sample Output
-----
[...]
Container ID:  cri-o://397fea50eb8ecd03db9fe8c9a7657c7980f23c8462e9cf2554e9a4493308e651
Container ID:  cri-o://90260d3d7ece810bb4c44a8aee3e23ebe50fd6b1225d48e6e103da070194c53a
-----


. Set up a debug session into the node where the target Emergency Response pod runs:
+
-----
$ oc debug node/ip-10-0-136-113.eu-central-1.compute.internal
-----

. On that OpenShift node, switch to the host operating system shell that runs host operating system binaries:
+
-----
sh-4.4# chroot /host
-----

. Using the previously determined container ID, determine the operating system process ID of the container on the OpenShift node:
+
-----
sh-4.4# crictl inspect --output json  90260d3d7ece810bb4c44a8aee3e23ebe50fd6b1225d48e6e103da070194c53a | grep pid
-----
+
.Sample Output
-----
45315
-----

. Using the process ID of the container, view the iptable rules on that host machine:
+
-----
sh-4.4# nsenter -t 45315 -n iptables -t nat -S
-----
+
.Sample Output
-----
-P PREROUTING ACCEPT
-P INPUT ACCEPT
-P POSTROUTING ACCEPT
-P OUTPUT ACCEPT
-N ISTIO_REDIRECT
-N ISTIO_IN_REDIRECT
-N ISTIO_INBOUND
-N ISTIO_OUTPUT
-A PREROUTING -p tcp -j ISTIO_INBOUND
-A OUTPUT -p tcp -j ISTIO_OUTPUT
-A ISTIO_REDIRECT -p tcp -j REDIRECT --to-ports 15001
-A ISTIO_IN_REDIRECT -p tcp -j REDIRECT --to-ports 15001
-A ISTIO_INBOUND -p tcp -m tcp --dport 8080 -j ISTIO_IN_REDIRECT
-A ISTIO_OUTPUT ! -d 127.0.0.1/32 -o lo -j ISTIO_REDIRECT
-A ISTIO_OUTPUT -m owner --uid-owner 1000710001 -j RETURN
-A ISTIO_OUTPUT -m owner --gid-owner 1000710001 -j RETURN
-A ISTIO_OUTPUT -d 127.0.0.1/32 -j RETURN
-A ISTIO_OUTPUT -j ISTIO_REDIRECT
-----
* Note that all of the incoming traffic for this operating system process to port 8080--the port on which the Emergency Response `response-service` is listening--is being redirected to port 15001--the port on which the `istio-proxy` is listening.
The same holds true for the outgoing traffic.


=== Explore Administration API

The Envoy data plane API provides an open standard for centralized management of a large fleet of Envoys.
Instead of copying configuration files to the many Envoy proxies in a typical microservice-architected application, a central point of control is available.

The administration API of each envoy container is available using the `curl` utility from within any application pod enabled with Envoy.

. Log in to OpenShift as the owner of the Emergency Response Demo application:
+
-----
$ oc login -u $ERDEMO_USER -p $OCP_PASSWD
-----

. Retrieve the help documentation provided by the Envoy administration API:
+
-----
$ oc -n $ERDEMO_NS rsh -c $ERDEMO_USER-responder-service \
    `oc get pod -n $ERDEMO_NS | grep "responder-service" | grep "Running" | awk '{print $1}'` \
    curl http://127.0.0.1:15000/help
-----

. Retrieve the status of all of the clusters visible to this Envoy service proxy:
+
-----
$ oc -n $ERDEMO_NS rsh -c $ERDEMO_USER-responder-service \
    `oc get pod -n $ERDEMO_NS | grep "responder-service" | grep "Running" | awk '{print $1}'` \
    curl http://127.0.0.1:15000/clusters?format=json \
     > $HOME/lab/responder-service-clusters.json
-----

. Skim through the contents of `$HOME/lab/responder-service-clusters.json`.
* Note that this information includes all discovered upstream hosts in each cluster along with per-host statistics. This is useful for debugging service discovery issues.
* Also note that absolutely all `cluster_statuses` reference services to your specific Emergency Response Demo application or your specific service mesh control plane.
+
IMPORTANT: Under no circumstances do any of your Envoy proxies have visibility to services that you do not own. This is critical from performance, scalability, and security perspectives.

. Inspect the configuration sent by Pilot to your pod's sidecar using `istioctl`:
+
-----
$ istioctl proxy-config cluster -n $ERDEMO_NS \
    `oc get pod -n $ERDEMO_NS | grep "responder-service" | awk '{print $1}'` -o json | less
-----

. Search for the destination service name to see an embedded metadata JSON element that names the specific DestinationRule that pod is currently using to communicate with the external service:
+
-----
$ oc -n $ERDEMO_NS rsh -c $ERDEMO_USER-responder-service \
  `oc get pod -n $ERDEMO_NS | grep "responder-service" | awk '{print $1}'` \
  curl http://127.0.0.1:15000/config_dump \
  | jq ".configs | .[] | select(.dynamic_route_configs) | .dynamic_route_configs" | less
-----

== Explore Network Policies

In multi-tenancy mode, OpenShift Service Mesh creates an isolated network for each mesh instance using `NetworkPolicy` resources. Pods within the mesh can communicate with each other and with pods in the data plane. Communication between pods in different meshes is not allowed.
Note that during service mesh installation, existing `NetworkPolicy` objects are not touched or deleted.

. Review the `NetworkPolicy` resources created by the service mesh:
.. Log in to OpenShift as the owner of the Emergency Response Demo application:
+
-----
$ oc login -u $ERDEMO_USER -p $OCP_PASSWD
-----
.. List the `NetworkPolicy` resources in the Emergency Response namespace:
+
----
$ oc get networkpolicy -n $ERDEMO_NS
----
+
.Sample Output
----
allow-from-all-namespaces                <none>                                    17h
allow-from-ingress-namespace             <none>                                    17h
istio-expose-route                       maistra.io/expose-route=true              19h
istio-mesh                               <none>                                    19h
kafka-cluster-network-policy-kafka       strimzi.io/name=kafka-cluster-kafka       32h
kafka-cluster-network-policy-zookeeper   strimzi.io/name=kafka-cluster-zookeeper   32h
----
** `allow-from-all-namespaces` and `allow-from-ingress-namespace` are installed by default in every namespace when using OpenShift SDN mode. The policies allow traffic between all pods in all namespaces, as well as ingress to pods through the router.
** `kafka-cluster-network-policy-kafka` and `kafka-cluster-network-policy-zookeeper` are created when installing the Kafka cluster, and define ingress rules for the Kafka and Zookeeper pods.
** `istio-mesh` and `istio-expose-route` are created when adding the namespace to the service mesh.
.. Review the `istio-mesh` network policy:
+
----
$ oc get networkpolicy istio-mesh -n $ERDEMO_NS -o yaml
----
+
.Sample Output
----
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  annotations:
    [...]
  name: istio-mesh
  [...]
  namespace: user1-er-demo
  labels:
    [...]
spec:
  podSelector: {}
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              maistra.io/member-of: admin1-istio-system
  policyTypes:
    - Ingress
----
** This policy allows all ingress traffic between namespaces that are labeled with `maistra.io/member-of: $SM_CP_ADMIN-istio-system`. This includes the service mesh data plane namespace as well as the mesh member namespaces as defined in `ServiceMeshMemberRoll`.

.. Review the `istio-expose-route` network policy:
+
----
$ oc get networkpolicy istio-expose-route -n $ERDEMO_NS -o yaml
----
+
.Sample Output
----
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  annotations:
  [...]
  name: istio-expose-route
  [...]
  namespace: user1-er-demo
  labels:
    [...]
spec:
  podSelector:
    matchLabels:
      maistra.io/expose-route: 'true'
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              network.openshift.io/policy-group: ingress
  policyTypes:
    - Ingress
----
** This policy allows ingress traffic between namespaces that are labeled with `network.openshift.io/policy-group: ingress` and pods with the `maistra.io/expose-route: 'true'` label. The `openshift-ingress` namespace--in which the OpenShift router pods run--has the `network.openshift.io/policy-group: ingress` label so annotated pods can be reached through a route.

. Check if pods in the Emergency Response Demo application are reachable from outside the service mesh:
.. Obtain a remote shell into the `stage-apicast` pod in the `apicast` namespace:
+
----
$ oc rsh -n $GW_PROJECT stage-apicast-1-xxxxx
----
.. In the remote shell, `curl` the incident service through its service name:
+
----
sh-4.2$ curl http://$ERDEMO_USER-incident-service.$ERDEMO_NS.svc:8080/incidents
----

.. Exit the remote shell.
.. In a normal shell, call the incident service through its exposed URL:
+
----
$ curl http://$ERDEMO_USER-incident-service.apps.$SUBDOMAIN_BASE/incidents
----
** Expect both `curl` requests to succeed.
+
****
*Question*:

* This seems to violate the concept of an isolated service mesh network. Can you explain?
****
. Delete the `allow-from-all-namespaces` and `allow-from-ingress-namespace` `NetworkPolicy` objects from the Emergency Response namespace:
+
----
$ oc delete networkpolicy allow-from-all-namespaces -n $ERDEMO_NS
$ oc delete networkpolicy allow-from-ingress-namespace -n $ERDEMO_NS
----

. Try to reach the incident service from within another namespace or from outside the cluster using `curl`.
+
****
*Question*:

* What do you observe?
****
. Add the `maistra.io/expose-route: 'true'` label to the template section of the DeploymentConfig of the incident service:
+
----
kind: DeploymentConfig
apiVersion: apps.openshift.io/v1
metadata:
 [...]
spec:
  [...]
  template:
    metadata:
      labels:
        app: user1-incident-service
        group: erd-services
        maistra.io/expose-route: 'true'
      annotations:
        sidecar.istio.io/inject: 'true'
    spec:
    [...]
----
. Save the DeploymentConfig.
. Once the incident service pod is redeployed, try to call the incident service from outside the cluster again.
+
****
*Question*:

* What do you observe?
****

. (Optional) Execute the following commands to restore the network policies to their original state:
+
----
$ echo "---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-from-all-namespaces
spec:
  podSelector: {}
  ingress:
    - from:
        - namespaceSelector: {}
  policyTypes:
    - Ingress
" | oc create -f - -n $ERDEMO_NS
----
+
----
$ echo "---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-from-ingress-namespace
spec:
  podSelector: {}
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              network-policy: global
  policyTypes:
    - Ingress
" | oc create -f - -n $ERDEMO_NS
----


== Appendix

.References

* link:https://docs.google.com/document/d/1eMnLBpcJNMahoE6cYKcECp_Jcy4Haj3qc36RBAO9J-U/edit#[Operator-Based Soft Multi-Tenancy]
* link:https://maistra.io/docs/comparison-with-istio/#_cluster_scoped_custom_resources[Comparison between Red Hat OpenShift Service Mesh and Istio]
* link:https://istio.io/blog/2019/data-plane-setup/[Demystifying Istio's Sidecar Injection Model]


ifdef::showscript[]
== Instructor Notes

. The two databases leveraged by the Emergency Response Demo application (`postgresql` and `$ERDEMO_USER-process-service-postgresql`) are also now injected with an Envoy proxy.
.. Verify that this is in fact the case either through the OpenShift Container Platform web console or the `oc` utility.

=== Envoy Access Log File

.TO-DO:
* https://aspenmesh.io/how-to-debug-istio-mutual-tls-mtls-policy-issues-using-aspen-mesh/
* global.proxy.accessLogFile
* Is this log file any different than what is already being logged from Envoy in OpenShift Service Mesh?
* What is a good example of using it to debug Istio configuration and policy issues?

=== Debug Envoy and Pilot

The source of truth for a given moment is always found in your pod’s Envoy sidecar configuration.
In this section of the lab, you link:https://istio.io/docs/ops/troubleshooting/proxy-cmd/[debug Envoy and Pilot].


link:https://www.erdemo.io/gettingstarted/[Getting Started]


endif::showscript[]
