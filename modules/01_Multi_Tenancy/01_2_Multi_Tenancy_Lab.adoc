:noaudio:
:scrollbar:
:toc2:
:linkattrs:
:data-uri:

= Service Mesh Multi Tenancy Lab

.Goals
** Understand ServiceMeshMemberRoll
** Understand Envoy _Data Plane_

:numbered:

== ServiceMeshMemberRoll

. Switch to your service mesh control plane administrator user:
+
-----
oc login -u $SM_CP_ADMIN
-----

. Register the $ERDEMO_USER-er-demo namespace as namespace _member_ to be monitored and manged by your service mesh control plane.
+
-----
echo "apiVersion: maistra.io/v1
kind: ServiceMeshMemberRoll
metadata:
  name: default
spec:
  members:
  - $ERDEMO_USER-er-demo" | oc apply -n $SM_CP_ADMIN-istio-system -f -
-----
+
Recall that the _ServiceMeshMemberRoll_ exists in the service mesh control plane (which is owned by the admin: $SM_CP_ADMIN )

. Notice that your $ERDEMO_USER-er-demo namespace now includes _kiali_ and _maistra_ annotations:
+
-----
echo -en "\n\n$(oc get project $ERDEMO_USER-er-demo -o template --template='{{.metadata.labels}}')\n\n"


map[kiali.io/member-of:admin50-istio-system maistra.io/member-of:admin50-istio-system]
-----

== Opt-in Auto-Injection Annotations

When deploying an application into the Red Hat OpenShift Service Mesh you must opt in to injection of the Envoy _data-plane_ by specifying the following annotation: _sidecar.istio.io/inject=true_ . 

Opting in ensures that the sidecar injection does not interfere with other OpenShift features such as builder pods used by numerous frameworks within the OpenShift ecosystem.

In this section of this lab you (as the owner of the Emergency Response application) opt in a selective list of services for auto injection of a sidecar.

. Switch to the $ERDEMO_USER:
+
-----
oc login -u $ERDEMO_USER
-----
+
The $ERDEMO_USER is the admin of the $ERDEMO_USER-er-demo namespace where the Emergency Response application resides.

. Review the contents of link:https://github.com/gpe-mw-training/ocp_service_mesh_advanced/blob/master/utils/inject_istio_annotation.sh[this script].


. Execute script that adds Envoy auto-injection annotations to Emergency Response services:
+
-----
curl https://raw.githubusercontent.com/gpe-mw-training/ocp_service_mesh_advanced/master/utils/inject_istio_annotation.sh \
    -o /tmp/inject_istio_annotation.sh && \
    chmod 775 /tmp/inject_istio_annotation.sh && \
    /tmp/inject_istio_annotation.sh
------

. After completion of the script, review the list Emergency Response related pods:
+
-----
oc get pods -l group=erd-services -n $ERDEMO_USER-er-demo

user50-disaster-simulator-1-p9gfl          2/2     Running   7          9h
user50-incident-priority-service-1-hgmdn   2/2     Running   4          9h
user50-incident-service-1-sz4dk            2/2     Running   3          9h
user50-mission-service-1-jz2r8             2/2     Running   9          9h
user50-process-service-4-cz5sz             2/2     Running   5          7h17m
user50-responder-service-1-qm5gn           2/2     Running   3          7h14m
user50-responder-simulator-1-tdrz2         2/2     Running   6          7h13m
-----
+
Notice that each of these pods indicates that two containers have started.

. You could use a script such as the following to identify a list of container names for each of the pods:
+
-----
for POD_NAME in $(oc get pods -n $ERDEMO_USER-er-demo -l group=erd-services -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}')
do
    oc get pod $POD_NAME  -n $ERDEMO_USER-er-demo -o jsonpath='{.metadata.name}{"    :\t\t"}{.spec.containers[*].name}{"\n"}'
done

...


user50-disaster-simulator-1-p9gfl    :          user50-disaster-simulator istio-proxy
user50-incident-priority-service-1-hgmdn    :           user50-incident-priority-service istio-proxy
user50-incident-service-1-sz4dk    :            user50-incident-service istio-proxy
user50-mission-service-1-jz2r8    :             user50-mission-service istio-proxy
user50-process-service-4-cz5sz    :             user50-process-service istio-proxy
user50-responder-service-1-qm5gn    :           user50-responder-service istio-proxy
user50-responder-simulator-1-tdrz2    :         user50-responder-simulator istio-proxy
-----
 
.. Notice that the output indicates the existence of an _istio-proxy_ container co-located with the primary business service containers for each pod.
.. Istio uses Kubernetes' link:https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook[MutatingAdmissionWebhook] for automatically injecting the sidecar proxy into user pods.

. The two databases leveraged by the Emergency Response demo ( _postgresql_ and _user50-process-service-postgresql_ ) are also now injected with an envoy proxy.
+
Verify that this is infact the case either through the OpenShift web console or the oc utility.

== Envoy _Data Plane_

. Capture the details of the _istio-proxy_ container from the _responser-service_ pod:
+
-----
oc get pod -n $ERDEMO_USER-er-demo \
       $(oc get pod -n $ERDEMO_USER-er-demo | grep "^$ERDEMO_USER-responder-service" | awk '{print $1}') \
       -o json \
       | jq .spec.containers[1] \
        > /tmp/responder_envoy.json
-----

. Study the details of the _istio-proxy_ container:
+
-----
less /tmp/responder_envoy.json
-----

. Answer the following questions pertaining to this _istio-proxy_ container:

.. What URL does OpenShift use to pull the remote Envoy proxy image that serves as the basis of this Envoy proxy sidecar?
.. What is the maximum amount of RAM and CPU dedicated to this Envoy proxy sidecar container ?
.. What is the URL that the Envoy proxy sidecar uses to communicate with _Pilot_ component of Red Hat Service Mesh ?


ifdef::showscript[]

1) registry.redhat.io/openshift-service-mesh/proxyv2-rhel8:1.0.1
2) cpu: 500m,  memory: 128Mi
3) istio-pilot.admin50-istio-system:15010

endif::showscript[]

=== Administration API

link:https://www.envoyproxy.io/docs/envoy/v1.12.0/operations/admin#operations-admin-interface[Envoy Administration API]

-----
oc rsh `oc get pod -n $ERDEMO_USER-er-demo | grep "responder-service" | awk '{print $1}'` \
    curl http://localhost:15000/help
-----

-----
oc rsh `oc get pod -n $ERDEMO_USER-er-demo | grep "responder-service" | awk '{print $1}'` \
   curl http://localhost:15000/clusters
-----


-----
oc rsh `oc get pod -n $ERDEMO_USER-er-demo | grep "responder-service" | awk '{print $1}'` \
         curl http://localhost:15000/config_dump \
         > /tmp/config_dump \
         && less /tmp/config_dump \
         | /usr/local/bin/jq ".configs | last | .dynamic_route_configs"
-----

== Getting Started with the Emergency Response demo

-----
echo -en "\n\n$(oc get route $ERDEMO_USER-emergency-console -n $ERDEMO_USER-er-demo -o --template={{.spec.host}}  -n  $ERDEMO_USER-er-demo)\n"
-----

link:https://www.erdemo.io/gettingstarted/[Getting Started]

ifdef::showscript[]

-----
oc project istio-system && \
         oc rsh `oc get pod | grep "istio-ingressgateway" | awk '{print $1}'` \
         curl http://localhost:15000/config_dump \
         > /tmp/config_dump \
         && less /tmp/config_dump \
         | /usr/local/bin/jq ".configs | last | .dynamic_route_configs"
-----

endif::showscript[]
