:noaudio:
:scrollbar:
:toc2:
:linkattrs:
:data-uri:

= Service Mesh Multi Tenancy Lab

.Goals
** Understand importance of Service Mesh multi-tenancy
** Understand ServiceMeshMemberRoll
** Understand Envoy _Data Plane_

:numbered:

== Overview

Faulty or compromised control planes should not be able affect any other tenants on the cluster. 
This means that control plane components and service accounts have to be locked down using appropriate Kubernetes Roles and network access limited by NetworkPolicies.

link:https://docs.google.com/document/d/1eMnLBpcJNMahoE6cYKcECp_Jcy4Haj3qc36RBAO9J-U/edit#[Operator-Based Soft Multi-Tenancy]


== ServiceMeshMemberRoll

. Switch to your service mesh control plane administrator user:
+
-----
oc login -u $SM_CP_ADMIN
-----

. Register the $ERDEMO_USER-er-demo namespace as namespace _member_ to be monitored and manged by your service mesh control plane.
+
-----
echo "apiVersion: maistra.io/v1
kind: ServiceMeshMemberRoll
metadata:
  name: default
spec:
  members:
  - $ERDEMO_USER-er-demo" | oc apply -n $SM_CP_ADMIN-istio-system -f -
-----
+
Recall that the _ServiceMeshMemberRoll_ exists in the service mesh control plane (which is owned by the admin: $SM_CP_ADMIN )

. Notice that your $ERDEMO_USER-er-demo namespace now includes _kiali_ and _maistra_ annotations:
+
-----
echo -en "\n\n$(oc get project $ERDEMO_USER-er-demo -o template --template='{{.metadata.labels}}')\n\n"


map[kiali.io/member-of:admin50-istio-system maistra.io/member-of:admin50-istio-system]
-----

. Notice also that your $ERDEMO_USER-er-demo namespace now includes namespace scoped _RoleBindings_ associated with the Istio related service accounts from your specific service mesh control plane:
+
-----
oc get RoleBinding  -n user50-er-demo -l release=istio

istio-citadel-admin50-istio-system                               24h
istio-egressgateway-admin50-istio-system                         24h
istio-galley-admin-role-binding-admin50-istio-system             24h
istio-ingressgateway-admin50-istio-system                        24h
istio-mixer-admin-role-binding-admin50-istio-system              24h
istio-pilot-admin50-istio-system                                 24h
istio-sidecar-injector-admin-role-binding-admin50-istio-system   24h
prometheus-admin50-istio-system                                  24h
-----
+
The use of a project scoped _RoleBinding_ rather than a cluster-scoped _ClusterRoleBinding_ is a key enabler of _multi-tenant_ capabilities of the Red Hat Service Mesh product.+

. Red Hat OpenShift Service Mesh configures each member project to ensure network access between itself, the control plane, and other member projects.
+
Notice that your $ERDEMO_USER-er-demo namespace now also includes a _NetworkPolicy_ called: _istio-mesh_
+
-----
oc get NetworkPolicy istio-mesh -n $ERDEMO_USER-er-demo

istio-mesh   <none>         26m
-----
+
This _NetworkPolicy_ allows ingress to all pods specific to this namespace from the corresponding Red Hat Service Mesh control plane all other other registered _members_.

== Opt-in Auto-Injection Annotations

When deploying an application into the Red Hat OpenShift Service Mesh you must opt in to injection of the Envoy _data-plane_ by specifying the following annotation: _sidecar.istio.io/inject=true_ . 

Opting in ensures that the sidecar injection does not interfere with other OpenShift features such as builder pods used by numerous frameworks within the OpenShift ecosystem.

In this section of this lab you (as the owner of the Emergency Response application) opt in a selective list of services for auto injection of a sidecar.

. Switch to the $ERDEMO_USER:
+
-----
oc login -u $ERDEMO_USER
-----
+
The $ERDEMO_USER is the admin of the $ERDEMO_USER-er-demo namespace where the Emergency Response application resides.

. Review the contents of link:https://github.com/gpe-mw-training/ocp_service_mesh_advanced/blob/master/utils/inject_istio_annotation.sh[this script].


. Execute script that adds Envoy auto-injection annotations to Emergency Response services:
+
-----
curl https://raw.githubusercontent.com/gpe-mw-training/ocp_service_mesh_advanced/master/utils/inject_istio_annotation.sh \
    -o /tmp/inject_istio_annotation.sh && \
    chmod 775 /tmp/inject_istio_annotation.sh && \
    /tmp/inject_istio_annotation.sh
------

. After completion of the script, review the list Emergency Response related pods:
+
-----
oc get pods -l group=erd-services -n $ERDEMO_USER-er-demo

user50-disaster-simulator-1-p9gfl          2/2     Running   7          9h
user50-incident-priority-service-1-hgmdn   2/2     Running   4          9h
user50-incident-service-1-sz4dk            2/2     Running   3          9h
user50-mission-service-1-jz2r8             2/2     Running   9          9h
user50-process-service-4-cz5sz             2/2     Running   5          7h17m
user50-responder-service-1-qm5gn           2/2     Running   3          7h14m
user50-responder-simulator-1-tdrz2         2/2     Running   6          7h13m
-----
+
Notice that each of these pods indicates that two containers have started.

. You could use a script such as the following to identify a list of container names for each of the pods:
+
-----
for POD_NAME in $(oc get pods -n $ERDEMO_USER-er-demo -l group=erd-services -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}')
do
    oc get pod $POD_NAME  -n $ERDEMO_USER-er-demo -o jsonpath='{.metadata.name}{"    :\t\t"}{.spec.containers[*].name}{"\n"}'
done

...


user50-disaster-simulator-1-p9gfl    :          user50-disaster-simulator istio-proxy
user50-incident-priority-service-1-hgmdn    :           user50-incident-priority-service istio-proxy
user50-incident-service-1-sz4dk    :            user50-incident-service istio-proxy
user50-mission-service-1-jz2r8    :             user50-mission-service istio-proxy
user50-process-service-4-cz5sz    :             user50-process-service istio-proxy
user50-responder-service-1-qm5gn    :           user50-responder-service istio-proxy
user50-responder-simulator-1-tdrz2    :         user50-responder-simulator istio-proxy
-----
 
.. Notice that the output indicates the existence of an _istio-proxy_ container co-located with the primary business service containers for each pod.
.. Istio uses Kubernetes' link:https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook[MutatingAdmissionWebhook] for automatically injecting the sidecar proxy into user pods.

. The two databases leveraged by the Emergency Response demo ( _postgresql_ and _user50-process-service-postgresql_ ) are also now injected with an envoy proxy.
+
Verify that this is infact the case either through the OpenShift web console or the oc utility.

== Envoy _Data Plane_


=== Service proxy container configuration

. Capture the details of the _istio-proxy_ container configuration from the _responder-service_ pod of the Emergency Response demo :
+
-----
oc get pod -n $ERDEMO_USER-er-demo \
       $(oc get pod -n $ERDEMO_USER-er-demo | grep "^$ERDEMO_USER-responder-service" | awk '{print $1}') \
       -o json \
       | jq .spec.containers[1] \
        > /tmp/responder_envoy.json
-----

. Study the details of the _istio-proxy_ container:
+
-----
less /tmp/responder_envoy.json
-----

. Answer the following questions pertaining to this _istio-proxy_ container:

.. What URL does OpenShift use to pull the remote Envoy proxy image that serves as the basis of this Envoy proxy sidecar?
.. What is the maximum amount of RAM and CPU dedicated to this Envoy proxy sidecar container ?
.. What is the URL that the Envoy proxy sidecar uses to communicate with _Pilot_ component of Red Hat Service Mesh ?


ifdef::showscript[]

1) registry.redhat.io/openshift-service-mesh/proxyv2-rhel8:1.0.1
2) cpu: 500m,  memory: 128Mi
3) istio-pilot.admin50-istio-system:15010

endif::showscript[]

=== Administration API

link:https://www.envoyproxy.io/docs/envoy/v1.12.0/operations/admin#operations-admin-interface[Envoy Administration API]

-----
oc rsh `oc get pod -n $ERDEMO_USER-er-demo | grep "responder-service" | awk '{print $1}'` \
    curl http://localhost:15000/help
-----

-----
oc rsh `oc get pod -n $ERDEMO_USER-er-demo | grep "responder-service" | awk '{print $1}'` \
   curl http://localhost:15000/clusters
-----

. Inspect the configuration sent by Pilot to your pod's sidecar using _istioctl_:
+
-----
istioctl proxy-config cluster -n <POD NAMESPACE> <PODNAME> -o json
-----
+
if you search for the destination service name you will see an embedded metadata JSON element that names the specific DestinationRule that pod is currently using to communicate with the external service.


-----
oc rsh `oc get pod -n $ERDEMO_USER-er-demo | grep "responder-service" | awk '{print $1}'` \
         curl http://localhost:15000/config_dump \
         > /tmp/config_dump \
         && less /tmp/config_dump \
         | /usr/local/bin/jq ".configs | last | .dynamic_route_configs"
-----

=== Envoy Access Log File

TO-DO:
* https://aspenmesh.io/how-to-debug-istio-mutual-tls-mtls-policy-issues-using-aspen-mesh/
* global.proxy.accessLogFile
* Is this log file any different than what is already being logged from Envoy in Red Hat Service Mesh ?
* What is a good example of using it to debug Istio configuration and policy issues ?

=== Debugging Envoy and Pilot

The source of truth for a given moment is always found in your podâ€™s Envoy sidecar configuration.
In this section of the lab, you link:https://istio.io/docs/ops/troubleshooting/proxy-cmd/[debug Envoy and Pilot].


link:https://www.erdemo.io/gettingstarted/[Getting Started]

ifdef::showscript[]

-----
oc project istio-system && \
         oc rsh `oc get pod | grep "istio-ingressgateway" | awk '{print $1}'` \
         curl http://localhost:15000/config_dump \
         > /tmp/config_dump \
         && less /tmp/config_dump \
         | /usr/local/bin/jq ".configs | last | .dynamic_route_configs"
-----

endif::showscript[]
