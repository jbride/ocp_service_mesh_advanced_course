:noaudio:
:scrollbar:
:toc2:
:linkattrs:
:data-uri:

== Istio Resiliency Lab

.Goals
* Understand Istio resiliency features
* Understand retries, timeouts, circuit breakers, and outlier detection


:numbered:
== Introduction

A resilient system is one that can maintain good performance for its users while coping with failures in the downstream systems on which it depends.
Red Hat^(R)^ OpenShift^(R)^ Service Mesh provides many features to help build more resilient applications--most importantly, client-side load balancing, circuit breaking via outlier detection, automatic retry, and request timeouts.
OpenShift Service Mesh also provides tools to inject faults into applications, allowing you to build programmatic and reproducible tests of your systemâ€™s resiliency.

Without a service mesh, it is typically up to each microservice application to provide resiliency by implementing circuit breakers and handling timeouts when calling upstream services. In the Java(TM) world, Hystrix--a Netflix OSS library--is popular for adding circuit-breaker functionality to an application. MicroProfile Fault Tolerance provides equivalent functionality for MicroProfile applications.

Having to explicitly build in circuit breakers and other resiliency-related functionality (like the handling of timeouts) places an extra burden on the application developer. The service mesh can take over a major part of that burden.

== Explore Retries

In this section of the lab, you become familiar with the service mesh automatic retry functionality.

For the following lab exercises, you use a slightly modified version of the incident priority service that can intentionally return failure responses.
This modified version exposes an API on port 9080.
This new API allows for an external client to configure the rate of failure responses.

=== Deploy Modified Incident Priority Service

. Make sure you are logged in as the Emergency Response Demo application owner:
+
----
$ oc login -u $ERDEMO_USER
----
. Redeploy the incident priority service using an image with the fault injection code:
+
----
$ oc patch dc $ERDEMO_USER-incident-priority-service -p "{\"spec\":{\"triggers\":[{\"type\": \"ConfigChange\"},{\"type\": \"ImageChange\",\"imageChangeParams\": {\"automatic\": true, \"containerNames\":[\"$ERDEMO_USER-incident-priority-service\"], \"from\": {\"kind\": \"ImageStreamTag\", \"namespace\": \"$ERDEMO_NS\", \"name\": \"$ERDEMO_USER-incident-priority-service:1.0.0-fault\"}}}]}}" -n $ERDEMO_NS
----
* This causes a redeployment of the incident priority service. Wait until the service redeploys successfully.
. Scale up the number pods of the incident priority service to three:
+
----
$ oc scale dc $ERDEMO_USER-incident-priority-service --replicas=3 -n $ERDEMO_NS
----

=== Inject Fault

. Find the name of one of the incident priority service pods:
+
----
$ INCIDENT_PRIORITY_SERVICE_POD=$(oc get pods -n $ERDEMO_NS|grep Running|grep $ERDEMO_USER-incident-priority-service|awk '{ print $1 }'|head -1)
----
. Using `oc exec`, obtain a bash prompt inside the service container:
+
----
$ oc exec -it $INCIDENT_PRIORITY_SERVICE_POD -n $ERDEMO_NS -c $ERDEMO_USER-incident-priority-service /bin/bash
----
. In the container, execute the following `curl` command to have the service return an HTTP 503 return code when called over its REST APIs:
+
----
$ curl -X POST -d '{"error":503,"percentage":100}' http://127.0.0.1:9080/inject
----
. Verify that a call to the `priority/{incidentId}` endpoint returns a 503 return code:
+
----
$ curl -s -w %{http_code} --output /dev/null http://127.0.0.1:8080/priority/qwerty
----
+
.Sample Output
----
503
----
. Type `exit` to quit the container.

=== Generate REST Calls

. Perform a number of REST calls to the `priority/{incidentId}` endpoint of the incident priority service:
.. Using `curl`, execute the following command:
+
----
$ while :; do curl -k -s -w %{http_code} --output /dev/null https://incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/priority/qwerty; echo "";sleep .1; done
----
+
.Sample Output
----
200
200
200
200
200
200
[...]
200
200
200
^C
----
.. If you have Siege installed on your system, execute the following command:
+
TIP: On Fedora, you can install Siege with the `# dnf install siege` command. On macOS, you can use the `$ brew install siege` command.
+
----
$ siege -r 10 -c 1 -v https://incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/priority/qwerty
----
+
.Sample Output
----
** SIEGE 4.0.4
** Preparing 1 concurrent users for battle.
The server is now under siege...
HTTP/1.1 200     0.12 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.13 secs:      65 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.12 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.12 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.11 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.12 secs:      65 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.13 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.12 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.12 secs:      65 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.13 secs:      64 bytes ==> GET  /priority/qwerty

Transactions:                     10 hits
Availability:                 100.00 %
Elapsed time:                  11.24 secs
Data transferred:               0.00 MB
Response time:                  0.12 secs
Transaction rate:               0.89 trans/sec
Throughput:                     0.00 MB/sec
Concurrency:                    0.11
Successful transactions:          10
Failed transactions:               0
Longest transaction:            0.13
Shortest transaction:           0.11
----

* Note that all of the calls succeed, even though 33 percent of the requests return a 503 return code.
This is a result of the automatic retry feature of the service mesh. Whenever a call returns a 503 code, the Envoy proxy executes a retry targeting one of the other pods of the target service.

. Verify that the faulty container has been called:
+
----
$ oc logs -f $INCIDENT_PRIORITY_SERVICE_POD -c $ERDEMO_USER-incident-priority-service -n $ERDEMO_NS
----
+
.Sample Output
----
2019-11-28 16:55:28.812  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Incoming Request
2019-11-28 16:55:28.812  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Returning error code 503
2019-11-28 16:55:29.283  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Incoming Request
2019-11-28 16:55:29.283  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Returning error code 503
2019-11-28 16:55:29.983  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Incoming Request
2019-11-28 16:55:29.983  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Returning error code 503
2019-11-28 16:55:30.460  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Incoming Request
2019-11-28 16:55:30.460  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Returning error code 503
2019-11-28 16:55:30.921  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Incoming Request
2019-11-28 16:55:30.921  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Returning error code 503
2019-11-28 16:55:31.169  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Incoming Request
2019-11-28 16:55:31.169  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Returning error code 503
----
. Repeat the fault-injection procedure for the second pod of the incident priority service, then call the incident priority service using `curl` or `siege`.
* Expect all of the calls to succeed.
. Repeat the fault-injection procedure for the third pod of the incident priority service, then call the incident priority service using `curl` or `siege`:
+
----
while :; do curl -k -s -w %{http_code} --output /dev/null https://incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/priority/qwerty; echo "";sleep .1; done
----
+
.Sample Output
----
503
503
[...]
503
503
503
503
^C
----
* Expect all of the calls to return a 503 error code.

=== Verify Default Behavior

By default, automatic retry is only enabled for 503 return codes, not for other 5xx codes.

. To reset the application behavior, log in to the pods and execute the following `curl` command:
+
----
$ curl -X POST http://127.0.0.1:9080/reset
----
+
[TIP]
====
To automatically reset all of the pods for the `incident-priority-service`, use these commands:

----
for pod in $(oc get pods -n $ERDEMO_NS -l app=$ERDEMO_USER-incident-priority-service | awk '/Running/ {print $1}'); do 
    echo $pod 
    oc exec -it -n $ERDEMO_NS $pod -c $ERDEMO_USER-incident-priority-service -- curl -X POST http://127.0.0.1:9080/reset;
done
----
====

. Log in to one of the pods of the incident priority service and have it return a 500 return code:
+
----
$ curl -X POST -d '{"error":500,"percentage":100}' http://127.0.0.1:9080/inject
----
. Call the incident priority service using `curl` or `siege`:
+
----
while :; do curl -k -s -w %{http_code} --output /dev/null https://incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/priority/qwerty; echo "";sleep .1; done
----
+
.Sample Output
----
500
200
200
500
200
200
500
500
200
200
200
500
^C
----

* Note that the 500 error code is returned to the caller.
+
****
*Question*:

Why is automatic retry only enabled for 503 error codes by default?
****

=== Modify Retry Behavior

The service mesh retry functionality can be extended to include error conditions other than a 503 return code. This requires additional configuration in the `VirtualService` resource associated with the target service.

. Extend the retry functionality to include all error codes in the 5xx range by adding the following to the `incident-priority-service-virtualservice` VirtualService:
.. Open the `VirtualService` resource of the incident priority service for editing:
+
----
$ oc edit virtualservice incident-priority-service-virtualservice -o yaml -n $ERDEMO_NS
----

.. Add the retry configuration to the route rules for HTTP traffic:
+
----
kind: VirtualService
apiVersion: networking.istio.io/v1alpha3
[...]
spec:
  hosts:
    - >-
      incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
    - erd-wildcard-gateway.$SM_CP_ADMIN-istio-system.svc.cluster.local
  http:
    - match:
        - uri:
            prefix: /priority
        - uri:
            exact: /reset
      route:
        - destination:
            host: $ERDEMO_USER-incident-priority-service.$ERDEMO_USER-er-demo.svc.cluster.local
            port:
              number: 8080
      retries:
        attempts: 2
        retryOn: 5xx
----
+
** `retryOn` determines the conditions for retry. In this case it includes all HTTP return codes in the 5xx range. Different conditions can be combined by separating them with a comma.
** `attempts` determines the number of retry attempts before giving up.

. Log in to one of the pods of the incident priority service and have it return a 500 return code.
. Call the incident priority service using `curl` or `siege`.
* Expect to see that the retry is now also working for 500 return codes.
+
[NOTE]
====
Other retry conditions include the following:

* `gateway-error`: This is similar to the 5xx policy, but only retries requests that result in a 502, 503, or 504 code.
* `reset`: A retry is attempted if the upstream server does not respond at all (disconnect/reset/read timeout).
* `retriable-4xx`: A retry is attempted if the upstream server responds with a retriable 4xx response code. Currently, the only response code in this category is 409.
* See the complete list in the link:https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/router_filter#x-envoy-retry-on[Envoy documentation].
====

. When you are done with this exercise, reset the incident priority service pods to not return error codes. Use the reset steps from earlier in the exercise.

. Reset the `VirtualService` resource to its original state by removing the `retries` element.

== Explore Timeouts

Proper handling of timeouts is another aspect of building resilient systems. Without careful timeout handling, slow services can bring a complete system to a halt--for example, by saturating connection pools in downstream systems.

OpenShift Service Mesh allows you to define timeout settings at the mesh level, as well as configure behavior when service responses exceed the predefined timeouts.

=== Inject Delays
The incident priority service version you deployed in the beginning of the lab also allows you to inject delays, to mimic a slow service.

. Find the name of one of the incident priority service pods:
+
----
$ INCIDENT_PRIORITY_SERVICE_POD=$(oc get pods -n $ERDEMO_NS|grep Running|grep $ERDEMO_USER-incident-priority-service|awk '{ print $1 }'|head -1)
----
. Using `oc exec`, obtain a bash prompt inside the service container:
+
----
$ oc exec -it $INCIDENT_PRIORITY_SERVICE_POD -n $ERDEMO_NS -c $ERDEMO_USER-incident-priority-service /bin/bash
----
. In the container, execute the following `curl` command to have the service wait for two seconds before returning a response when called over its REST APIs:
+
----
$ curl -X POST -d '{"delay":2000,"percentage":100}' http://127.0.0.1:9080/inject
----
. Verify that a call to the `priority/{incidentId}` endpoint effectively takes two seconds:
+
----
$ curl -s -w %{http_code} --output /dev/null http://127.0.0.1:8080/priority/qwerty
----
+
.Expected Output After Two Seconds
----
200
----
. Type `exit` to quit the container.

. Call the incident priority service using `curl` or `siege`:
* Using `curl`:
+
----
while :; do curl -k -s -w %{http_code} --output /dev/null https://incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/priority/qwerty; echo "";sleep .1; done
----
+
.Sample Output
----
200
200
200
200
200
200
200
200
200
^C
----

* Using `siege`:
+
----
$ siege -r 5 -c 4 -d1 -v https://incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/priority/qwerty
----
+
.Sample Output
----
** SIEGE 4.0.4
** Preparing 4 concurrent users for battle.
The server is now under siege...
HTTP/1.1 200     0.13 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.13 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.12 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     2.12 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     2.13 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.13 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.11 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.11 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.12 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     2.12 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.12 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.12 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     2.12 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.11 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     2.12 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.12 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.13 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     2.12 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     2.11 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.12 secs:      64 bytes ==> GET  /priority/qwerty

Transactions:                     20 hits
Availability:                 100.00 %
Elapsed time:                  13.60 secs
Data transferred:               0.00 MB
Response time:                  0.82 secs
Transaction rate:               1.47 trans/sec
Throughput:                     0.00 MB/sec
Concurrency:                    1.21
Successful transactions:          20
Failed transactions:               0
Longest transaction:            2.13
Shortest transaction:           0.11
----

* Note that all calls succeed, but roughly 30 percent of the calls take two seconds.
* No handling of timeouts is the default behavior of the service mesh.

=== Define Timeouts

Timeouts can be defined in the `VirtualService` resource for the target service.

. Add a 500ms timeout to the VirtualService configuration:
.. Open the `VirtualService` resource of the incident priority service for editing:
+
----
$ oc edit virtualservice incident-priority-service-virtualservice -o yaml -n $ERDEMO_NS
----

.. Add the timeout configuration to the route rules for HTTP traffic:
+
----
kind: VirtualService
apiVersion: networking.istio.io/v1alpha3
[...]
spec:
  hosts:
    - >-
      incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
    - erd-wildcard-gateway.$SM_CP_ADMIN-istio-system.svc.cluster.local
  http:
    - match:
        - uri:
            prefix: /priority
        - uri:
            exact: /reset
      route:
        - destination:
            host: $ERDEMO_USER-incident-priority-service.$ERDEMO_USER-er-demo.svc.cluster.local
            port:
              number: 8080
      timeout: 500ms
----
. Call the incident priority service using `curl` or `siege`:
* Using `curl`:
+
----
$ while :; do curl -k -s -w %{http_code} --output /dev/null https://incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/priority/qwerty; echo "";sleep .1; done
----
+
.Sample Output
----
200
504
504
200
200
504
200
200
200
200
504
^C
----

* Using `siege`:
+
----
$ siege -r 5 -c 4 -d1 -v https://incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/priority/qwerty
----
+
.Sample Output
----
** SIEGE 4.0.4
** Preparing 4 concurrent users for battle.
The server is now under siege...
HTTP/1.1 200     0.12 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.13 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.14 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 504     0.63 secs:      24 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.11 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 504     0.62 secs:      24 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.12 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.12 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.13 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 504     0.63 secs:      24 bytes ==> GET  /priority/qwerty
HTTP/1.1 504     0.61 secs:      24 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.14 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.13 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 504     0.61 secs:      24 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.12 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.12 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 504     0.63 secs:      24 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.13 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 200     0.17 secs:      64 bytes ==> GET  /priority/qwerty
HTTP/1.1 504     0.61 secs:      24 bytes ==> GET  /priority/qwerty

Transactions:                     13 hits
Availability:                  65.00 %
Elapsed time:                  10.62 secs
Data transferred:               0.00 MB
Response time:                  0.46 secs
Transaction rate:               1.22 trans/sec
Throughput:                     0.00 MB/sec
Concurrency:                    0.57
Successful transactions:          13
Failed transactions:               7
Longest transaction:            0.63
Shortest transaction:           0.11
----

* Note that when calling the slow incident service pod, the Envoy proxy gives up after 500ms, and returns a 504 error code ("Gateway Timeout").
* It is up to the calling application to gracefully handle the error condition.

=== Combine Timeout and Retry
It is possible to combine timeout handling with retries.

. In the `VirtualService` resource of the incident priority service, make sure that `retryOn` is set to `5xx` in the `retries` element and that there is a `perTryTimeout` element equal to `200ms`:
+
----
kind: VirtualService
apiVersion: networking.istio.io/v1alpha3
[...]
spec:
  hosts:
    - >-
      incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE
  gateways:
    - erd-wildcard-gateway.$SM_CP_ADMIN-istio-system.svc.cluster.local
  http:
    - match:
        - uri:
            prefix: /priority
        - uri:
            exact: /reset
      route:
        - destination:
            host: $ERDEMO_USER-incident-priority-service.$ERDEMO_USER-er-demo.svc.cluster.local
            port:
              number: 8080
      retries:
        attempts: 2
        retryOn: 5xx
        perTryTimeout: 200ms
      timeout: 500ms
----
. Call the incident priority service using `curl` or `siege`.
* Expect all calls to return a 200 response code.

. When you are done with this exercise, reset the incident priority service pods to not return error codes. Also reset the `VirtualService` resource to its original state by removing the `timeout` element.

== Explore Circuit Breakers

From the previous exercises, you probably noted that failing service pods are still being called before the proxy attempts a retry to another pod. This is less than ideal, especially in the case of 503 errors. A 503 error often indicates a temporary situation from which the server may be able to recover--for example, an intermittent problem with a database connection or a saturated database connection pool. In these cases, continuing to hammer on the failing system does not help and may make things worse.

That is where circuit breakers come in. When a system is deemed unhealthy, it is temporarily removed from the pool to which requests are being sent--the circuit trips open. After a configurable amount of time, a request is sent to the unhealthy pod to check if the pod was able to recover. If so, it is brought back into the pool--the circuit is closed again. If not, it stays in quarantine until the next check.

OpenShift Service Mesh implements circuit breakers using outlier detection. As a service mesh administrator, you define the criteria that classifies a target pod as an outlier. If the criteria are met when calling the pod, the pod is evicted from the pool of healthy endpoints for the service.

=== Inject Fault

. Inject a 503 fault in one of the pods of the incident priority service:
.. Find the name of one of the incident priority service pods:
+
----
$ INCIDENT_PRIORITY_SERVICE_POD=$(oc get pods -n $ERDEMO_NS|grep Running|grep $ERDEMO_USER-incident-priority-service|awk '{ print $1 }'|head -1)
----
.. Using `oc exec`, obtain a bash prompt inside the service container:
+
----
$ oc exec -it $INCIDENT_PRIORITY_SERVICE_POD -n $ERDEMO_NS -c $ERDEMO_USER-incident-priority-service /bin/bash
----
.. In the container, execute the following `curl` command to have the service return an HTTP 503 return code when called over its REST APIs:
+
----
$ curl -X POST -d '{"error":503,"percentage":100}' http://127.0.0.1:9080/inject
----
. Call the incident priority service using `curl` or `siege`:
+
----
$ while :; do curl -k -s -w %{http_code} --output /dev/null https://incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/priority/qwerty; echo "";sleep .1; done
----
. In the logs of the faulty pod, verify that the service is still being called:
+
----
$ oc logs -f $INCIDENT_PRIORITY_SERVICE_POD -c $ERDEMO_USER-incident-priority-service -n $ERDEMO_NS
----
+
.Sample Output
----
2019-11-28 16:55:28.812  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Incoming Request
2019-11-28 16:55:28.812  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Returning error code 503
2019-11-28 16:55:29.283  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Incoming Request
2019-11-28 16:55:29.283  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Returning error code 503
2019-11-28 16:55:29.983  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Incoming Request
2019-11-28 16:55:29.983  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Returning error code 503
2019-11-28 16:55:30.460  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Incoming Request
2019-11-28 16:55:30.460  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Returning error code 503
2019-11-28 16:55:30.921  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Incoming Request
2019-11-28 16:55:30.921  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Returning error code 503
2019-11-28 16:55:31.169  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Incoming Request
2019-11-28 16:55:31.169  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Returning error code 503
----

=== Configure Outlier Detection

Service mesh outlier detection is configured in the `DestinationRule` resource for the service.

. Open the `DestinationRule` resource of the incident priority service for editing:
+
----
$ oc edit destinationrule incident-priority-service-client-mtls -n $ERDEMO_NS
----
. In the `DestinationRule` resource, add the outlier detection configuration:
+
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  [...]
spec:
  host: $ERDEMO_USER-incident-priority-service.$ERDEMO_USER-er-demo.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
    outlierDetection:
      baseEjectionTime: 2m
      consecutiveErrors: 1
      interval: 1s
      maxEjectionPercent: 100
----

* This setting has the net effect of ejecting a pod from the load-balancing pool if an error is detected. The outlier is ejected from the healthy pool for a period of time equal to the base ejection time (two minutes) multiplied by the number of times it has been ejected.
. Call the incident priority service using `curl` or `siege`:
+
----
$ while :; do curl -k -s -w %{http_code} --output /dev/null https://incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/priority/qwerty; echo "";sleep .1; done
----

. Verify the logs of the faulty pod.
+
.Sample Output
----
2019-11-29 16:20:04.142  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Incoming Request
2019-11-29 16:20:04.142  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Returning error code 503
2019-11-29 16:22:05.026  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Incoming Request
2019-11-29 16:22:05.026  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Returning error code 503
2019-11-29 16:22:05.458  INFO   --- [ntloop-thread-6] c.r.c.n.i.priority.RestApiVerticle       : Incoming Request
----

* Note that after the first call that returns a 503 code, the pod does not get requests for approximately two minutes. If after that period the pod still returns errors, it is ejected again from the pool.
* Service mesh circuit breaking functionality only works for error code 503.

. Reset the incident priority service pods to no longer respond with an error code.

=== Configure Concurrent HTTP Requests

Another use case for circuit breakers is preventing a service from being flooded with requests. In the `DestinationRule` resource, the service mesh administrator can configure the number of concurrent HTTP requests that are allowed to the target service pod before requests are short-circuited.

. Add connection pool settings to the `DestinationRule` resource of the incident priority service:
.. Open the `DestinationRule` resource of the incident priority service for editing:
+
----
$ oc edit destinationrule incident-priority-service-client-mtls -n $ERDEMO_NS
----
.. In the `DestinationRule` resource, add the connection pool configuration:
+
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  [...]
spec:
  host: $ERDEMO_USER-incident-priority-service.$ERDEMO_USER-er-demo.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
    outlierDetection:
      baseEjectionTime: 3m
      consecutiveErrors: 1
      interval: 1s
      maxEjectionPercent: 100
    connectionPool:
      http:
        http1MaxPendingRequests: 1
        maxRequestsPerConnection: 1
      tcp:
        maxConnections: 1
----

* Note that this is a fairly extreme setting because you are allowing only one concurrent connection to the incident priority service pod.

. Use Siege to exercise some load against the incident priority service. With Siege, you can easily simulate concurrent access to a host:
+
----
$ siege -r 100 -c 4 -d0 -v https://incident-priority-service.$ERDEMO_USER.apps.$SUBDOMAIN_BASE/priority/qwerty
----

* In this example, you make use of four concurrent users. Depending on the lab conditions, this setting may generate quite a few 503 responses. If that occurs, lower the number of concurrent users until you get only 200 responses.

. While the `siege` load test is still running, log in to one of the pods of the incident priority service and inject a delay of 2000ms.
. Go back to the `siege` load test and observe that you have a fair number of 503 responses.
Once the limits in the `connectionPool` are met, the circuit breaker functionality of the Envoy proxy short-circuits the call to the service and immediately returns a 503 response.
. Open the `DestinationRule` resource of the incident priority service for edit, and set `http1MaxPendingRequests` and `maxRequestsPerConnection` to `10`.
. Go back to the `siege` load test, and expect to see that you get only 200 responses.

This concludes the lab. You have explored the Istio resiliency functionality including retries, timeouts, circuit breakers, and outlier detection.
