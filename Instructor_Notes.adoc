= Red Hat OpenShift Service Mesh Advanced - Instuctor Notes

:numbered:

== Lab Assets

. link:https://docs.google.com/spreadsheets/d/1vazinjjbOSN-uDY8u_mmg-lXtrRlZtm1l5vJQucdKz8/edit#gid=959461386[Class Assignment Spreadsheet]

== *OCP 4.3 Workshop Deployer*
. Provisioned from https://labs.opentlc.com 
.. *Services -> Catalogs -> OPENTLC OpenShift 4 Labs -> OpenShift Workshop Deployer - 4.3*

. Instructor uses _MachineSet API_ to scale to appropriately sized cluster
.. Emergency Response (16GB RAM) + Istio Control Plane (6GB RAM) = 22GB RAM per student
.. Provision 1 worker node per 2 students.

.. Login to the OCP Environment as cluster administrator
+
----
$ oc login <<CLASSROOM-OCP-ENVIRONMENT>> -u opentlc-mgr
----

.. Find the currently available machinesets: oc get machinesets -n openshift-machine-api
+
----
$ oc get machinesets -n openshift-machine-api
----

.. Scale up a machineset, using this command:
+
----
oc scale machineset <MACHINESETNAME> --replicas=<COUNT> -n openshift-machine-api
----

. Configure Environment to Stay Up
.. During the course, you want the environment to stay up, without idling. You can set this in the CloudForms UI.
.. Select your service, then select *Admin* drop-down
.. Select *Unidle*

image::images/admin-unidle.png[]

== Prep Provisioning Environment

You will need an environment to run the Ansible provisioning playbooks. 

NOTE: If you are using a Mac, it is best to run the installation from a Linux environment. You can order a *Student VM* from OPENTLC. Once the environment is created then you'll need to ssh into Student VM and run the installation commands in this document.

=== Install Python Package Manager - pip

. Download and install pip
+
----
$ curl "https://bootstrap.pypa.io/get-pip.py" -o "get-pip.py"

$ python get-pip.py
----

. Verify The Installation
+
----
$ pip --version
----

=== Install Supporting Python Packages

The installation requires Python modules https://lxml.de/[lxml] and https://pypi.org/project/openshift/[openshift]

. Install lxml python module
+
----
$ sudo pip3 install lxml
----

. Install OpenShift python module
+
----
$ sudo pip3 install openshift
----

=== Install Ansible Tooling

The installation requires Ansible tooling

. Install Ansible
+
----
$ sudo dnf -y install ansible
----

== 3scale Control Plane

The 3scale installation process has three major steps:

.. Provision 1 3scale Control Plane
.. Provision 1 tenant per student
.. Provision 1 set of API gateways (staging and production) per tenant

=== Overview

This workload provisions a single centralized 3scale API Manager in a single OCP namespace.

This workload only needs to be executed once per OCP cluster.

It also allows for management (ie: creation / deletion) of a configurable number of API _tenants_ in the 3scale API Manager installation.

This role might be valuable in the following circumstances:

. *Instructor Led Training (ILTs), Hackathons and workshops*:
+
Given X number of students in an ILT requiring 3scale, provision a single central multi-tenant Red Hat 3scale API Manager where each student is assigned their own tenant.
+
The student is provided with administrative credentials to their assigned tenant.
+
This approach might be more desirable than the alternative where each student provisions their own 3scale API Manager.

=== Ansible Project Layout

. Notice the directory layout and files included in this project:
+
-----
$ tree

├── defaults
│   └── main.yml
├── meta
│   └── main.yml
├── README.adoc
├── tasks
│   ├── main.yml
│   ├── pre_workload.yml
│   ├── remove_workload.yml
│   ├── tenant_loop.yml
│   ├── tenant_mgmt.yml
│   ├── wait_for_deploy.yml
│   └── workload.yml
└── templates
    └── limitrange.yaml
-----

. Highlights of the most important files are as follows:

.. *defaults/main.yml* : ansible variables and their defaults
.. *tasks/pre_workload.ymml* : ansible tasks used to set clusterquota
.. *tasks/workload.yml* : ansible tasks executed when provisioning 3scale API Manager
.. *tasks/tenant_mgmt.yml* : ansible tasks executed when provisioning tenants

=== 3scale Deployment

. Log into the OCP environment as cluster manager
+
----
$ oc login <<CLASSROOM-OCP-ENVIRONMENT>> -u opentlc-mgr
----

=== Environment Variables

. Set up environment variables
+
----
$ echo "export OCP_AMP_ADMIN_ID=api0" >> ~/.bashrc  # OCP user that owns OCP namespace where mult-tenant 3scale resides
                                                    # A cluster quota is assigned to this user
                                                    # NOTE: this OCP user doesn't necessarily need to exist


$ echo "export API_MANAGER_NS=3scale-mt-\$OCP_AMP_ADMIN_ID" >> ~/.bashrc      # OCP namespace where 3scale API Manager resides
----

. Execute the following:
+
----
$ source ~/.bashrc
----

. SMTP Configurations to enable API Manager to send emails
.. The following SMTP configurations are set to null values, since the labs do not require 3scale Developer Portal sign-up flows.

.. Execute the following commands in the shell:
+
----
smtp_host=
smtp_port=
smtp_authentication=
smtp_userid=
smtp_passwd=
smtp_domain=
adminEmailUser=3scaleadmin
adminEmailDomain=redhat.com
----

. 3Scale Gateway Configurations
.. Execute the following commands in the shell:
+
----
RESUME_CONTROL_PLANE_GWS=true
SUBDOMAIN_BASE=`oc whoami --show-server | cut -d'.' -f 2,3,4,5,6 | cut -d':' -f 1`
use_rwo_for_cms=true
----

** `RESUME_CONTROL_PLANE_GWS`: 3scale API Manager includes a staging and production gateway by default. These two GWs typically are not used for applying API policies to requests  because the "data plane" (aka: gateways) tends to be deployed in a different environment. However, the staging gateway is needed by system-provider web application for API Gateway policies details. Subsequently, the default value is:  true

** `SUBDOMAIN_BASE`: OCP wildcard DNS after "apps";  ie; 2345.openshift.opentlc.com

** `use_rwo_for_cms`: 3scale control plane consists of a Content Management System (CMS) that typically is scaled out for improved performance in a production environment. 

. Red Hat Service Tokens

The installation requires Red Hat Service Tokens. You can retrieve these tokens from your Red Hat account (https://access.redhat.com/terms-based-registry).

. Set the following environment variables
+
----
$ rht_service_token_user=<change me>    # RHT Registry Service Account name as per:   https://access.redhat.com/terms-based-registry

$ rht_service_token_password=<changeme> # RHT Registry Service Account passwd as per: https://access.redhat.com/terms-based-registry/
----

== Emergency Response Demo

. Instructor layers 1 Emergency Response Demo per student
.. git clone https://github.com/btison/emergency-response-demo-install.git
.. git checkout service-mesh-advanced-ilt
.. cd ansible
.. cp inventories/inventory.template inventories/inventory
.. Add the value of your link:[mapbox token] to the _map_token_ variable in _inventories/inventory_ file. ie;
+
-----
[all:vars]
# MapBox API token, see https://docs.mapbox.com/help/how-mapbox-works/access-tokens/
map_token=pk.eyJ1KJoiamjyaWRlIiwiGSI6ImNqeGNjamxqYjAxcXczdXBrcW5mbmguamkifQ.iBEb0APX1Vmo_VtsDj-Y3f
-----

.. execute:
+
-----
start_id=1
end_id=20

for x in $(seq $start_id $end_id); do
   ansible-playbook -i inventories/inventory playbooks/install.yml \
      -e project_admin=user$x \
      -e set_realm_admin_as_default=True \
      -e install_monitoring=False \
      -e install_tools=False
done
-----

== *Red Hat Service Mesh*
. The ansible found in the course link:https://github.com/gpe-mw-training/ocp_service_mesh_advanced[Lab Assets] is recommended.

[IMPORTANT]
====
After running the Red Hat Service Mesh installation playbook, the `opentlc-mgr` cluster admin user will no longer be active. A new cluster admin user has been created as part of the installation playbook: `cluster-admin`, with the same password.
====
